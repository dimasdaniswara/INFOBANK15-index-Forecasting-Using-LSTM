{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yfRGAymdTmBA"
      },
      "source": [
        "#**Project: Predicting stock market index using LSTM**\n",
        "\n",
        "**Authors**: Hum Nath Bhandari, Binod Rimal, Nawa Raj Pokhrel, Ramchandra Rimal, Keshav Dahal, and Rajendra K.C. Khatri\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0m5kjGJtGNbQ"
      },
      "source": [
        "# **Part I:  Data Creation and Exploration**\n",
        "\n",
        "In this module we use various financial resources to create input data for our machine learning models. This includes the following data.\n",
        "- S\\&P 500 index data: Open price and Close price\n",
        "- Vix index\n",
        "- Interest Rate \n",
        "- Unemployment Rate\n",
        "- Consumer sentiment index\n",
        "- Dollar Index\n",
        "- MACD\n",
        "- RSI\n",
        "- ATR\n",
        "\n",
        "Our data will be the time series data with the following start and end date.\n",
        "\n",
        "- start date: 2006/01/02\n",
        "- end date:  2021/10/01\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9GvUIRynTIp",
        "outputId": "05945513-d81d-44bf-e4a8-2acfb2a8c3fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8-Ztn89P6M1A"
      },
      "outputs": [],
      "source": [
        "output_dir_path = r\"c:\\Users\\Dimas\\OneDrive - Institut Teknologi Bandung\\Documents\\Kuliah\\TA Yok Bisa\\coderef\"\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DqXKpN9QjM0H"
      },
      "source": [
        "### **Importing basic libraries and APIs** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WqxVeoCuX3mS"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd  ##import necessary libraries\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "\n",
        "# import tensorflow as tf\n",
        "# %load_ext tensorboard\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import datetime as dt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o-0dsDrzEaeM"
      },
      "source": [
        "## **Loading input data**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "w6Su9vxu28HO",
        "outputId": "c33e5e69-22f4-423a-a5ef-6905625f8d82"
      },
      "outputs": [],
      "source": [
        "# data = pd.read_csv('/content/drive/My Drive/input_data.csv')\n",
        "# data['Date']=pd.to_datetime(data['Date']).dt.date\n",
        "# data.set_index(\"Date\",inplace=True)\n",
        "# data.head()\n",
        "\n",
        "df = pd.read_excel(\"dataset-TA.xlsx\")\n",
        "df.set_index('Tanggal',inplace = True)\n",
        "data = df.copy()\n",
        "dates = df.index\n",
        "dates = pd.to_datetime(dates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "KL2KiiGddcAL",
        "outputId": "df9afc46-0dea-46a7-a758-2d654c85cf6c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Terakhir</th>\n",
              "      <th>Pembukaan</th>\n",
              "      <th>Close IDR</th>\n",
              "      <th>Consumer Confidence</th>\n",
              "      <th>Emas USD</th>\n",
              "      <th>BI Rate</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tanggal</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2022-12-26</th>\n",
              "      <td>1155.56</td>\n",
              "      <td>1148.94</td>\n",
              "      <td>15565.900391</td>\n",
              "      <td>113.1</td>\n",
              "      <td>1809.7</td>\n",
              "      <td>5.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-27</th>\n",
              "      <td>1150.40</td>\n",
              "      <td>1155.67</td>\n",
              "      <td>15607.000000</td>\n",
              "      <td>113.1</td>\n",
              "      <td>1823.1</td>\n",
              "      <td>5.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-28</th>\n",
              "      <td>1146.11</td>\n",
              "      <td>1150.40</td>\n",
              "      <td>15617.500000</td>\n",
              "      <td>113.1</td>\n",
              "      <td>1815.8</td>\n",
              "      <td>5.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-29</th>\n",
              "      <td>1146.98</td>\n",
              "      <td>1146.22</td>\n",
              "      <td>15789.000000</td>\n",
              "      <td>113.1</td>\n",
              "      <td>1826.0</td>\n",
              "      <td>5.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-30</th>\n",
              "      <td>1150.98</td>\n",
              "      <td>1147.04</td>\n",
              "      <td>15620.000000</td>\n",
              "      <td>113.1</td>\n",
              "      <td>1826.2</td>\n",
              "      <td>5.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Terakhir  Pembukaan     Close IDR  Consumer Confidence  Emas USD  \\\n",
              "Tanggal                                                                        \n",
              "2022-12-26   1155.56    1148.94  15565.900391                113.1    1809.7   \n",
              "2022-12-27   1150.40    1155.67  15607.000000                113.1    1823.1   \n",
              "2022-12-28   1146.11    1150.40  15617.500000                113.1    1815.8   \n",
              "2022-12-29   1146.98    1146.22  15789.000000                113.1    1826.0   \n",
              "2022-12-30   1150.98    1147.04  15620.000000                113.1    1826.2   \n",
              "\n",
              "            BI Rate  \n",
              "Tanggal              \n",
              "2022-12-26      5.5  \n",
              "2022-12-27      5.5  \n",
              "2022-12-28      5.5  \n",
              "2022-12-29      5.5  \n",
              "2022-12-30      5.5  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.dates as mdates"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e5EWNJs65Ecs"
      },
      "source": [
        "### **Data Exploration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "EIVKYEZm5Di2",
        "outputId": "08b2dbdc-039c-4374-e376-c054acd10ea3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# df = data.copy()\n",
        "df['MA_50'] = df['Terakhir'].rolling(50).mean()\n",
        "df['MA_200'] = df['Terakhir'].rolling(200).mean()\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "fig.set(facecolor = \"white\")\n",
        "plt.plot(dates, df['Terakhir'],'mediumblue',label=['S&P 500 Close Price'], linewidth = 2.2)\n",
        "plt.plot(dates, df['MA_50'],'red', label=['50-day MA'], linewidth = 2.2)\n",
        "plt.plot(dates, df['MA_200'],'darkgreen', label=['200-day MA'], linewidth = 2.2)\n",
        "plt.legend(['INFOBANK15 Close Price', '50-day Moving Average', '200-day Moving Average'], loc='upper left')\n",
        "# Format the x-axis tick labels to show only the years\n",
        "date_format = mdates.DateFormatter('%Y')\n",
        "plt.gca().xaxis.set_major_formatter(date_format)\n",
        "plt.gca().xaxis_date()\n",
        "plt.title('')\n",
        "plt.xlabel('Time (years)')\n",
        "plt.ylabel('Close price')\n",
        "fig.savefig(output_dir_path+ \"original_data_plus_moving_averages.png\",dpi=600)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Vap-SkxQjy"
      },
      "source": [
        "####  **Correlation heatmap**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "1Fv9RQuWqbzk",
        "outputId": "2621f848-91fa-4dcf-efc7-c9dd16c7f993"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "fig = plt.figure(figsize= (10,5))\n",
        "sns.heatmap(data.corr(), annot=True)\n",
        "sns.set_style(\"whitegrid\")\n",
        "#fig.savefig(output_dir_path+\"correlation_heatmap.png\",dpi=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "qACPoxnPiNSC",
        "outputId": "aeeeced5-c981-47bd-bcd2-c4088bb30c19"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data = data.iloc[:, 1:]\n",
        "fig = plt.figure(figsize= (10,5))\n",
        "sns.heatmap(data.corr(), annot=True)\n",
        "sns.set_style(\"whitegrid\")\n",
        "#fig.savefig(output_dir_path+\"correlation_heatmap2.png\",dpi=600)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "pcaukJvvNTPN",
        "outputId": "85f5c976-db10-436b-b898-2b641166ab1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 1440x864 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x720 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig = plt.figure(figsize = (20, 12))\n",
        "#fig.axes.get_yaxis().set_visible(False)\n",
        "data.plot(subplots = True, figsize = (12,10),grid=False)\n",
        "sns.set_style(\"whitegrid\")\n",
        "#fig.savefig(output_dir_path+\"timeseries.png\",dpi=600)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gMsCJU6A38Zk"
      },
      "source": [
        "### **Denoising Closing Price**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "6TYqklCZ32TS",
        "outputId": "c6e5cd99-07c7-42ce-c2f8-21b43cf2afcf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pembukaan</th>\n",
              "      <th>Close IDR</th>\n",
              "      <th>Consumer Confidence</th>\n",
              "      <th>Emas USD</th>\n",
              "      <th>BI Rate</th>\n",
              "      <th>Close</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tanggal</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-12-23</th>\n",
              "      <td>573.42</td>\n",
              "      <td>13449.0</td>\n",
              "      <td>115.9</td>\n",
              "      <td>1133.6</td>\n",
              "      <td>4.75</td>\n",
              "      <td>607.574338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-27</th>\n",
              "      <td>573.62</td>\n",
              "      <td>13442.0</td>\n",
              "      <td>115.9</td>\n",
              "      <td>1138.8</td>\n",
              "      <td>4.75</td>\n",
              "      <td>607.574338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-28</th>\n",
              "      <td>587.15</td>\n",
              "      <td>13429.0</td>\n",
              "      <td>115.9</td>\n",
              "      <td>1140.9</td>\n",
              "      <td>4.75</td>\n",
              "      <td>607.574338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-29</th>\n",
              "      <td>597.09</td>\n",
              "      <td>13490.0</td>\n",
              "      <td>115.9</td>\n",
              "      <td>1158.1</td>\n",
              "      <td>4.75</td>\n",
              "      <td>607.574338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-30</th>\n",
              "      <td>612.84</td>\n",
              "      <td>13466.0</td>\n",
              "      <td>115.9</td>\n",
              "      <td>1151.7</td>\n",
              "      <td>4.75</td>\n",
              "      <td>614.092713</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Pembukaan  Close IDR  Consumer Confidence  Emas USD  BI Rate  \\\n",
              "Tanggal                                                                    \n",
              "2016-12-23     573.42    13449.0                115.9    1133.6     4.75   \n",
              "2016-12-27     573.62    13442.0                115.9    1138.8     4.75   \n",
              "2016-12-28     587.15    13429.0                115.9    1140.9     4.75   \n",
              "2016-12-29     597.09    13490.0                115.9    1158.1     4.75   \n",
              "2016-12-30     612.84    13466.0                115.9    1151.7     4.75   \n",
              "\n",
              "                 Close  \n",
              "Tanggal                 \n",
              "2016-12-23  607.574338  \n",
              "2016-12-27  607.574338  \n",
              "2016-12-28  607.574338  \n",
              "2016-12-29  607.574338  \n",
              "2016-12-30  614.092713  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from skimage.restoration import (denoise_wavelet, estimate_sigma)\n",
        "\n",
        "data['Close']= denoise_wavelet(data.iloc[:, 0], wavelet='haar',\n",
        "                                      #method='BayesShrink', \n",
        "                                      method='VisuShrink',  \n",
        "                                      mode='soft', rescale_sigma = True)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.drop(\"Pembukaan\",axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h7CN1Pi07Nzi"
      },
      "source": [
        "### **Machine learning libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Fi67Y5BNKB1k"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "import time\n",
        "import math\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ifiXXSC7KluW"
      },
      "source": [
        "### **Supporting functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ikplCNqpJ8CN"
      },
      "outputs": [],
      "source": [
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "    return (np.mean(np.abs((y_true - y_pred)/(y_true))*100)) #some issues with zero denominator\n",
        "\n",
        "def calculate_scores(y_true, y_pred):\n",
        "  rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
        "  #R2_score = r2_score(y_true, y_pred)\n",
        "  R = np.corrcoef(y_true, y_pred)\n",
        "  #mae = mean_absolute_error(y_true, y_pred)\n",
        "  mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "  #dic = {'rmse':rmse, 'R2_score': R2_score, 'R':R[0,1], 'mae': mae, 'mape': mape}\n",
        "  dic = {'rmse':rmse, 'R': R[0,1], 'mape': mape}\n",
        "  return (dic)\n",
        "\n",
        "\n",
        "\n",
        "def DatasetCreation(dataset, time_step = 1):  ##defining a function that gives a dataset and a time step, which then returns the input and output data\n",
        "   DataX, DataY = [], []\n",
        "   for i in range(len(dataset)- time_step -1):\n",
        "         a = dataset[i:(i+ time_step), ]\n",
        "         DataX.append(a)\n",
        "         DataY.append(dataset[i + time_step, 0]) #ydata consists close price\n",
        "   return np.array(DataX), np.array(DataY)\n",
        "\n",
        "\n",
        "def data_split(data, split = 0.2):\n",
        "  #======= creating training and test data===#\n",
        "  l1   = int(len(data) * (1- split))\n",
        "  l2    = len(data) - l1\n",
        "  data1  = data.iloc[0:l1,:]\n",
        "  data2   = data.iloc[l1:len(data),:]\n",
        "  return data1, data2\n",
        "\n",
        "def min_max_transform(data, feature_range=(0, 1)):\n",
        "   scaler = MinMaxScaler(feature_range) \n",
        "   return scaler.fit_transform(data)\n",
        "\n",
        "def min_max_inverse_transform(data_scaled, min_original, max_original):\n",
        "    return min_original +  data_scaled*(max_original - min_original)\n",
        "\n",
        "\n",
        "\n",
        "def write_dic_to_file(dic_name, file_name):\n",
        "  file = open(file_name, 'w')\n",
        "  file.write(str(dic_name))\n",
        "  file.close()\n",
        "\n",
        "import ast\n",
        "def read_dic_from_file(file_name):\n",
        "  file = open(file_name, \"r\")\n",
        "  contents = file.read()\n",
        "  dictionary = ast.literal_eval(contents)\n",
        "  file.close()\n",
        "  return dictionary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Terakhir</th>\n",
              "      <th>Pembukaan</th>\n",
              "      <th>Close IDR</th>\n",
              "      <th>Consumer Confidence</th>\n",
              "      <th>Emas USD</th>\n",
              "      <th>BI Rate</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tanggal</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-12-23</th>\n",
              "      <td>576.13</td>\n",
              "      <td>573.42</td>\n",
              "      <td>13449.000000</td>\n",
              "      <td>115.9</td>\n",
              "      <td>1133.6</td>\n",
              "      <td>4.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-27</th>\n",
              "      <td>582.88</td>\n",
              "      <td>573.62</td>\n",
              "      <td>13442.000000</td>\n",
              "      <td>115.9</td>\n",
              "      <td>1138.8</td>\n",
              "      <td>4.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-28</th>\n",
              "      <td>594.30</td>\n",
              "      <td>587.15</td>\n",
              "      <td>13429.000000</td>\n",
              "      <td>115.9</td>\n",
              "      <td>1140.9</td>\n",
              "      <td>4.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-29</th>\n",
              "      <td>611.62</td>\n",
              "      <td>597.09</td>\n",
              "      <td>13490.000000</td>\n",
              "      <td>115.9</td>\n",
              "      <td>1158.1</td>\n",
              "      <td>4.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-30</th>\n",
              "      <td>613.80</td>\n",
              "      <td>612.84</td>\n",
              "      <td>13466.000000</td>\n",
              "      <td>115.9</td>\n",
              "      <td>1151.7</td>\n",
              "      <td>4.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-26</th>\n",
              "      <td>1155.56</td>\n",
              "      <td>1148.94</td>\n",
              "      <td>15565.900391</td>\n",
              "      <td>113.1</td>\n",
              "      <td>1809.7</td>\n",
              "      <td>5.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-27</th>\n",
              "      <td>1150.40</td>\n",
              "      <td>1155.67</td>\n",
              "      <td>15607.000000</td>\n",
              "      <td>113.1</td>\n",
              "      <td>1823.1</td>\n",
              "      <td>5.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-28</th>\n",
              "      <td>1146.11</td>\n",
              "      <td>1150.40</td>\n",
              "      <td>15617.500000</td>\n",
              "      <td>113.1</td>\n",
              "      <td>1815.8</td>\n",
              "      <td>5.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-29</th>\n",
              "      <td>1146.98</td>\n",
              "      <td>1146.22</td>\n",
              "      <td>15789.000000</td>\n",
              "      <td>113.1</td>\n",
              "      <td>1826.0</td>\n",
              "      <td>5.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-30</th>\n",
              "      <td>1150.98</td>\n",
              "      <td>1147.04</td>\n",
              "      <td>15620.000000</td>\n",
              "      <td>113.1</td>\n",
              "      <td>1826.2</td>\n",
              "      <td>5.50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1241 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Terakhir  Pembukaan     Close IDR  Consumer Confidence  Emas USD  \\\n",
              "Tanggal                                                                        \n",
              "2016-12-23    576.13     573.42  13449.000000                115.9    1133.6   \n",
              "2016-12-27    582.88     573.62  13442.000000                115.9    1138.8   \n",
              "2016-12-28    594.30     587.15  13429.000000                115.9    1140.9   \n",
              "2016-12-29    611.62     597.09  13490.000000                115.9    1158.1   \n",
              "2016-12-30    613.80     612.84  13466.000000                115.9    1151.7   \n",
              "...              ...        ...           ...                  ...       ...   \n",
              "2022-12-26   1155.56    1148.94  15565.900391                113.1    1809.7   \n",
              "2022-12-27   1150.40    1155.67  15607.000000                113.1    1823.1   \n",
              "2022-12-28   1146.11    1150.40  15617.500000                113.1    1815.8   \n",
              "2022-12-29   1146.98    1146.22  15789.000000                113.1    1826.0   \n",
              "2022-12-30   1150.98    1147.04  15620.000000                113.1    1826.2   \n",
              "\n",
              "            BI Rate  \n",
              "Tanggal              \n",
              "2016-12-23     4.75  \n",
              "2016-12-27     4.75  \n",
              "2016-12-28     4.75  \n",
              "2016-12-29     4.75  \n",
              "2016-12-30     4.75  \n",
              "...             ...  \n",
              "2022-12-26     5.50  \n",
              "2022-12-27     5.50  \n",
              "2022-12-28     5.50  \n",
              "2022-12-29     5.50  \n",
              "2022-12-30     5.50  \n",
              "\n",
              "[1241 rows x 6 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYBcGcR7LSZW"
      },
      "source": [
        "## **Supporting functions for results visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7mNqwC_BQ-o7"
      },
      "outputs": [],
      "source": [
        "def test_scores_plot(model_output):\n",
        "  neurons = model_output['avg_scores']['neurons']\n",
        "  rmse = model_output['avg_scores']['rmse']\n",
        "  #mae =  model_output['avg_scores']['mae']\n",
        "  mape =  model_output['avg_scores']['mape']\n",
        "  #R2 =   model_output['avg_scores']['R2']\n",
        "  R =    model_output['avg_scores']['R']\n",
        "  #time =  model_output['avg_scores']['elapsed_time']\n",
        "\n",
        "  fig = plt.figure(figsize = (18, 4))\n",
        "  plt.subplot(131)\n",
        "  plt.plot(neurons, rmse, '--o', linewidth = 2, color = 'indigo')\n",
        "  plt.title(\"(a)\")\n",
        "  plt.xlabel(\"Neurons\")\n",
        "  plt.ylabel(\"Avg. RMSE\")\n",
        "  sns.set_style(\"whitegrid\")\n",
        "\n",
        "  \n",
        "  plt.subplot(132)\n",
        "  plt.plot(neurons, mape, '--o', linewidth = 2, color = 'darkgreen')\n",
        "  plt.title(\"(b)\")\n",
        "  plt.xlabel(\"Neurons\")\n",
        "  plt.ylabel(\"Avg. MAPE\")\n",
        "\n",
        "\n",
        "  plt.subplot(133)\n",
        "  plt.plot(neurons, R, '--o', linewidth = 2, color = 'darkred')\n",
        "  plt.title(\"(c)\")\n",
        "  plt.xlabel(\"Neurons\")\n",
        "  plt.ylabel(\"Avg. R \")\n",
        "\n",
        "  fig.savefig(output_dir_path+\"multiple_avg_scores_plots.png\",dpi=600)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def true_pred_plot(model_output):\n",
        "  \n",
        "  y_train = model_output['datasets']['y_train']\n",
        "  y_test =  model_output['datasets']['y_test']\n",
        "\n",
        "  train_pred = model_output['best_model']['train_predictions']\n",
        "  test_pred = model_output['best_model']['test_predictions']\n",
        "\n",
        "  ##====== Visualizing true vs predicted plots ========#\n",
        "  fig = plt.figure(figsize= (14,5))\n",
        "  plt.subplot(121)\n",
        "  #sns.relplot(x = y_train_original, y = train_pred_original)\n",
        "  plt.scatter(y_train, train_pred, marker= \"+\", color = 'mediumblue')\n",
        "  identity_line = np.linspace(max(min(y_train), min(train_pred)), min(max(y_train), max(train_pred)))\n",
        "  plt.plot(identity_line, identity_line, color=\"red\", linestyle=\"dashed\", linewidth= 2.5)\n",
        "\n",
        "  plt.xlabel(\"True\")\n",
        "  plt.ylabel(\"Predicted\")\n",
        "  plt.title(\"(a)\")\n",
        "\n",
        "  plt.subplot(122)\n",
        "  #sns.relplot(x = y_test_original, y = test_pred_original)\n",
        "  plt.scatter(y_test, test_pred, marker = \"+\", color = 'mediumblue')\n",
        "  identity_line = np.linspace(max(min(y_test), min(test_pred)), min(max(y_test), max(test_pred)))\n",
        "  plt.plot(identity_line, identity_line, color=\"red\", linestyle=\"dashed\", linewidth= 2.5)\n",
        "  plt.xlabel(\"True\")\n",
        "  plt.ylabel(\"Predicted\")\n",
        "  plt.title(\"(b)\")\n",
        "  fig.savefig(output_dir_path+\"True_vs_predicted_plot.png\", dpi=600)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def prediction_plot(model_output):\n",
        "  time_step =  model_output['hyper_parameters']['time_step']\n",
        "  best_replicate = model_output['best_model']['replicate']\n",
        "\n",
        "  data = model_output['datasets']['data']\n",
        "\n",
        "  train_predict_plot_data = np.empty_like(data.values[:,0])# extracting closing price\n",
        "  train_predict_plot_data[:] = np.nan\n",
        "\n",
        "  test_predict_plot_data = np.empty_like(data.values[:,0])\n",
        "  test_predict_plot_data[:] = np.nan\n",
        "\n",
        "  fig1 = plt.figure(figsize = (18,12))\n",
        "\n",
        "  plt.subplot(231)\n",
        "\n",
        "  train_pred = model_output['train_predictions'][0][best_replicate]\n",
        "  test_pred = model_output['test_predictions'][0][best_replicate]\n",
        "\n",
        "\n",
        "  train_predict_plot_data[time_step:len(train_pred)+ time_step] =  train_pred\n",
        "  test_predict_plot_data[len(train_pred)+(time_step*2)+1:len(data.values)-1] = test_pred\n",
        "  \n",
        "  plt.plot(dates,data.values[:,0],'k',linewidth = 1.5)\n",
        "  plt.plot(train_predict_plot_data,'mediumblue',linewidth = 1.5)\n",
        "  plt.plot(test_predict_plot_data,'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Close price')\n",
        "  plt.title(\"(a)\")\n",
        "  plt.legend(['True value', 'Predicted value in train set', 'Predicted value in test set'], loc = 'upper left')\n",
        "\n",
        "  plt.subplot(232)\n",
        "\n",
        "  train_pred = model_output['train_predictions'][1][best_replicate]\n",
        "  test_pred = model_output['test_predictions'][1][best_replicate]\n",
        "\n",
        "  train_predict_plot_data[time_step:len(train_pred)+ time_step] =  train_pred \n",
        "  test_predict_plot_data[len(train_pred)+(time_step*2)+1:len(data.values)-1] =  test_pred\n",
        "\n",
        "  plt.plot(dates,data.values[:,0],'k',linewidth = 1.5)\n",
        "  plt.plot(train_predict_plot_data,'mediumblue',linewidth = 1.5)\n",
        "  plt.plot(test_predict_plot_data,'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Close price')\n",
        "  plt.title(\"(b)\")\n",
        "  plt.legend(['True value', 'Predicted value in train set', 'Predicted value in test set'], loc = 'upper left')\n",
        "\n",
        "  plt.subplot(233)\n",
        "\n",
        "  train_pred = model_output['train_predictions'][2][best_replicate]\n",
        "  test_pred = model_output['test_predictions'][2][best_replicate]\n",
        "\n",
        "  train_predict_plot_data[time_step:len(train_pred)+ time_step] = train_pred\n",
        "  test_predict_plot_data[len(train_pred)+(time_step*2)+1:len(data.values)-1] =  test_pred\n",
        "\n",
        "  plt.plot(dates,data.values[:,0],'k',linewidth = 1.5)\n",
        "  plt.plot(train_predict_plot_data,'mediumblue',linewidth = 1.5)\n",
        "  plt.plot(test_predict_plot_data,'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Close price')\n",
        "  plt.title(\"(c)\")\n",
        "  plt.legend(['True value', 'Predicted value in train set', 'Predicted value in test set'], loc = 'upper left')\n",
        "\n",
        "\n",
        "  plt.subplot(234)\n",
        "\n",
        "  train_pred = model_output['train_predictions'][3][best_replicate]\n",
        "  test_pred = model_output['test_predictions'][3][best_replicate]\n",
        "\n",
        "  train_predict_plot_data[time_step:len(train_pred)+ time_step] = train_pred\n",
        "  test_predict_plot_data[len(train_pred)+(time_step*2)+1:len(data.values)-1] = test_pred\n",
        "\n",
        "  plt.plot(dates,data.values[:,0],'k',linewidth = 1.5)\n",
        "  plt.plot(train_predict_plot_data,'mediumblue',linewidth = 1.5)\n",
        "  plt.plot(test_predict_plot_data,'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Close price')\n",
        "  plt.title(\"(d)\")\n",
        "  plt.legend(['True value', 'Predicted value in train set', 'Predicted value in test set'], loc = 'upper left')\n",
        "  \n",
        "\n",
        "  plt.subplot(235)\n",
        "\n",
        "  train_pred = model_output['train_predictions'][4][best_replicate]\n",
        "  test_pred = model_output['test_predictions'][4][best_replicate]\n",
        "\n",
        "  train_predict_plot_data[time_step:len(train_pred)+ time_step] = train_pred\n",
        "  test_predict_plot_data[len(train_pred)+(time_step*2)+1:len(data.values)-1] = test_pred\n",
        "\n",
        "  plt.plot(dates,data.values[:,0],'k',linewidth = 1.5)\n",
        "  plt.plot(train_predict_plot_data,'mediumblue',linewidth = 1.5)\n",
        "  plt.plot(test_predict_plot_data,'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Close price')\n",
        "  plt.title(\"(e)\")\n",
        "  plt.legend(['True value', 'Predicted value in train set', 'Predicted value in test set'], loc = 'upper left')\n",
        "\n",
        "  plt.subplot(236)\n",
        "\n",
        "  train_pred = model_output['train_predictions'][5][best_replicate]\n",
        "  test_pred = model_output['test_predictions'][5][best_replicate]\n",
        "\n",
        "  train_predict_plot_data[time_step:len(train_pred)+ time_step] = train_pred\n",
        "  test_predict_plot_data[len(train_pred)+(time_step*2)+1:len(data.values)-1] = test_pred\n",
        "\n",
        "  plt.plot(dates,data.values[:,0],'k',linewidth = 1.5)\n",
        "  plt.plot(train_predict_plot_data,'mediumblue',linewidth = 1.5)\n",
        "  plt.plot(test_predict_plot_data,'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Close price')\n",
        "  plt.title(\"(f)\")\n",
        "  plt.legend(['True value', 'Predicted value in train set', 'Predicted value in test set'], loc = 'upper left')\n",
        "\n",
        "\n",
        "  fig1.savefig(output_dir_path+\"predictions_plots_fullset.png\",dpi=600)\n",
        "  plt.show()\n",
        "\n",
        "  fig2 = plt.figure(figsize = (18,12))\n",
        "\n",
        "  plt.subplot(231)\n",
        "  plt.plot(dates,data.values[len(train_pred)+(time_step*2)+1:-1, 0],'mediumblue',linewidth = 1.5)\n",
        "  plt.plot(model_output['test_predictions'][0][best_replicate], 'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Close price')\n",
        "  plt.title(\"(a)\")\n",
        "  plt.legend(['True value', 'Predicted value'], loc='upper left')\n",
        "\n",
        "\n",
        "  plt.subplot(232)\n",
        "\n",
        "  plt.plot(dates,data.values[len(train_pred)+(time_step*2)+1:-1, 0],'mediumblue',linewidth = 1.5)\n",
        "  plt.plot(model_output['test_predictions'][1][best_replicate], 'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Close price')\n",
        "  plt.title(\"(b)\")\n",
        "  plt.legend(['True value', 'Predicted value'], loc='upper left')\n",
        "\n",
        "\n",
        "  plt.subplot(233)\n",
        "\n",
        "  plt.plot(dates,data.values[len(train_pred)+(time_step*2)+1:-1, 0],'mediumblue',linewidth = 1.5)\n",
        "  plt.plot(model_output['test_predictions'][2][best_replicate], 'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Close price')\n",
        "  plt.title(\"(c)\")\n",
        "  plt.legend(['True value', 'Predicted value'], loc='upper left')\n",
        "\n",
        "\n",
        "\n",
        "  plt.subplot(234)\n",
        "\n",
        "  plt.plot(dates,data.values[len(train_pred)+(time_step*2)+1:-1, 0],'mediumblue',linewidth = 1.5)\n",
        "  plt.plot(model_output['test_predictions'][3][best_replicate], 'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Close price')\n",
        "  plt.title(\"(d)\")\n",
        "  plt.legend(['True value', 'Predicted value'], loc='upper left')\n",
        "  \n",
        "  plt.subplot(235)\n",
        "\n",
        "  plt.plot(dates,data.values[len(train_pred)+(time_step*2)+1:-1, 0],'mediumblue',linewidth = 1.5)\n",
        "  plt.plot(model_output['test_predictions'][4][best_replicate], 'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Close price')\n",
        "  plt.title(\"(e)\")\n",
        "  plt.legend(['True value', 'Predicted value'], loc='upper left')\n",
        "\n",
        "  \n",
        "  plt.subplot(236)\n",
        "\n",
        "  plt.plot(dates,data.values[len(train_pred)+(time_step*2)+1:-1, 0],'mediumblue',linewidth = 1.5)\n",
        "  plt.plot(model_output['test_predictions'][5][best_replicate], 'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Close price')\n",
        "  plt.title(\"(f)\")\n",
        "  plt.legend(['True value', 'Predicted value'], loc='upper left')\n",
        "\n",
        "\n",
        "\n",
        "  fig2.savefig(output_dir_path+\"predictions_plots_testset.png\",dpi=600)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def best_model_prediction_plot(model_output):\n",
        "\n",
        "  time_step =  model_output['hyper_parameters']['time_step']\n",
        "\n",
        "  data = model_output['datasets']['data']\n",
        "\n",
        "  train_predict_plot_data = np.empty_like(data.values[:,0])# extracting closing price\n",
        "  train_predict_plot_data[:] = np.nan\n",
        "\n",
        "  test_predict_plot_data = np.empty_like(data.values[:,0])\n",
        "  test_predict_plot_data[:] = np.nan\n",
        "\n",
        "  \n",
        "  fig = plt.figure(figsize = (14,5))\n",
        "\n",
        "  plt.subplot(121)\n",
        "\n",
        "  train_pred = model_output['best_model']['train_predictions']\n",
        "  test_pred = model_output['best_model']['test_predictions']\n",
        "\n",
        "  train_predict_plot_data[time_step:len(train_pred)+ time_step] =  train_pred\n",
        "  test_predict_plot_data[len(train_pred)+(time_step*2)+1:len(data.values)-1] = test_pred\n",
        "  \n",
        "  plt.plot(dates,data.values[:,0],'k', linewidth = 1.5)\n",
        "  plt.plot(train_predict_plot_data,'mediumblue',linewidth = 1.5)\n",
        "  plt.plot(test_predict_plot_data,'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Close price')\n",
        "  plt.title(\"(a)\")\n",
        "  plt.legend(['True value', 'Predicted value in train set', 'Predicted value in test set'], loc = 'upper left')\n",
        "\n",
        "\n",
        "  plt.subplot(122)\n",
        "  plt.plot(dates,data.values[len(train_pred)+(time_step*2)+1:-1, 0],'k',linewidth = 1.5)\n",
        "  plt.plot(test_pred,'darkgreen',linewidth = 1.5)\n",
        "  plt.xlabel('')\n",
        "  plt.ylabel('Close price')\n",
        "  plt.title(\"(b)\")\n",
        "  plt.legend(['True value', 'Predicted value'], loc='upper left')\n",
        "\n",
        "  fig.savefig(output_dir_path+\"best_model_predictions_plots.png\",dpi=600)\n",
        "\n",
        "\n",
        "def rmse_boxplots(model_output):\n",
        "  fig = plt.figure(figsize = (6,5))\n",
        "  plt.boxplot(model_output['scores']['rmse'], patch_artist=True)\n",
        "  plt.xticks([1,2,3,4,5,6], ['10', '30', '50', '100', '150', '200'])\n",
        "  plt.xlabel('Number of neurons')\n",
        "  plt.ylabel('RMSE')\n",
        "  fig.savefig(output_dir_path+\"rmse_boxplots.png\",dpi=600)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def rmse_violinplotplots(model_output):\n",
        "  fig = plt.figure(figsize = (6,5))\n",
        "  plt.violinplot(model_output['scores']['rmse'])\n",
        "  plt.xticks([1,2,3,4,5,6], ['10', '30', '50', '100', '150', '200'])\n",
        "  plt.xlabel('Number of neurons')\n",
        "  plt.ylabel('RMSE')\n",
        "  fig.savefig(output_dir_path+\"rmse_violinplots.png\",dpi=600)\n",
        "  plt.show()\n",
        "\n",
        "def all_scores_boxplots(model_output):\n",
        "\n",
        "  fig = plt.figure(figsize = (18,5))\n",
        "  plt.subplot(131)\n",
        "  p1 = plt.boxplot(model_output['scores']['rmse'],patch_artist=True)\n",
        "  for i, box in enumerate(p1['boxes']):\n",
        "    # change outline color\n",
        "    box.set(color= 'blue', linewidth = 1.2)\n",
        "    # change fill color\n",
        "    box.set(facecolor = 'mediumblue')\n",
        "  plt.xticks([1,2,3,4,5,6], ['10', '30', '50', '100', '150', '200'])\n",
        "  plt.title(\"(a)\")\n",
        "  plt.xlabel('Number of neurons')\n",
        "  plt.ylabel('RMSE')\n",
        "\n",
        "  plt.subplot(132)\n",
        "  p2 = plt.boxplot(model_output['scores']['mape'],patch_artist=True)\n",
        "  for i, box in enumerate(p2['boxes']):\n",
        "    #change outline color\n",
        "    box.set(color= 'blue', linewidth = 1.2)\n",
        "    # change fill color\n",
        "    box.set(facecolor = 'indigo')\n",
        "\n",
        "  plt.xticks([1,2,3,4,5,6], ['10', '30', '50', '100', '150', '200'])\n",
        "  plt.title(\"(b)\")\n",
        "  plt.xlabel('Number of neurons')\n",
        "  plt.ylabel('MAPE')\n",
        "\n",
        "  plt.subplot(133)\n",
        "  p3 = plt.boxplot(model_output['scores']['R'],patch_artist=True)\n",
        "  for i, box in enumerate(p3['boxes']):\n",
        "    #change outline color\n",
        "    box.set(color= 'blue', linewidth = 1.2)\n",
        "    # change fill color\n",
        "    box.set(facecolor = 'darkgreen')\n",
        "  plt.xticks([1,2,3,4,5,6], ['10', '30', '50', '100', '150', '200'])\n",
        "  plt.title(\"(c)\")\n",
        "  plt.xlabel('Number of neurons')\n",
        "  plt.ylabel('R')\n",
        "\n",
        "  fig.savefig(output_dir_path+\"all_scores_boxplots.png\",dpi=600)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def create_visualization(model_output):\n",
        "  true_pred_plot(model_output)\n",
        "  test_scores_plot(model_output)\n",
        "  prediction_plot(model_output)\n",
        "  best_model_prediction_plot(model_output)\n",
        "  rmse_boxplots(model_output)\n",
        "  all_scores_boxplots(model_output)\n",
        "  \n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3f0bSL1I0SD"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ED4G64o5Hihx"
      },
      "source": [
        "## **Function for Building Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "a8PHScCoHoUn"
      },
      "outputs": [],
      "source": [
        "def build_model(layers, time_step, num_features, optimizer,learning_rate, verbose = 1):\n",
        "  \n",
        "  model = Sequential()\n",
        "    \n",
        "  for i in range(len(layers)):\n",
        "    if len(layers)==1:\n",
        "      model.add(LSTM(np.int(layers[i]), input_shape = (time_step, num_features)))\n",
        "    else:\n",
        "      if i < len(layers)-1:\n",
        "        if i == 0:\n",
        "          model.add(LSTM(np.int(layers[i]), input_shape=(time_step, num_features), return_sequences= True))\n",
        "          #model.add(Dropout(0.10))\n",
        "        else:\n",
        "          model.add(LSTM(np.int(layers[i]), return_sequences=True))\n",
        "          #model.add(Dropout(0.10))\n",
        "      else:\n",
        "        model.add(LSTM(np.int(layers[i])))\n",
        "        #model.add(Dropout(0.10))  \n",
        "  model.add(Dense(1, activation = 'linear'))\n",
        "  \n",
        "  if optimizer == 'Adam':\n",
        "    opt = optimizers.Adam(lr = learning_rate)\n",
        "  elif optimizer == 'Adagrad': \n",
        "    opt = optimizers.Adagrad(lr = learning_rate)\n",
        "  elif optimizer == 'Nadam':\n",
        "    opt = optimizers.Nadam(lr = learning_rate)\n",
        "  elif optimizer == 'Adadelta':\n",
        "    opt = optimizers.Adadelta(lr= learning_rate)\n",
        "  elif optimizer == 'RMSprop':\n",
        "    opt = optimizers.RMSprop(lr= learning_rate)\n",
        "  else:\n",
        "    print(\"No optimizer found in the list(['Adam', 'Adagrad','Nadam', 'Adadelta', 'RMSprop'])! Please apply your optimizer manually...\")\n",
        "\n",
        "  model.compile(loss='mean_squared_error', optimizer= opt)\n",
        "   \n",
        "  if verbose == 1:\n",
        "    print(model.summary())\n",
        "  return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mbqNfEZEfGKo"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHuOeDc1I3Iu",
        "outputId": "cafa453d-b574-4860-9a3f-ead380d5e3fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 250)               261000    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 261,251\n",
            "Trainable params: 261,251\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.sequential.Sequential at 0x25f7d7444c8>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizers_names = ['Adam', 'Adagrad', 'Nadam']\n",
        "\n",
        "build_model([250], 5, 10, optimizers_names[2], 0.001, 1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bdv4kf0CIcdR"
      },
      "source": [
        "## **Function for Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R_uHVktrIj6K"
      },
      "outputs": [],
      "source": [
        "def hyper_parameter_tuning(layers, data, time_step, split, optimizers_names, learning_rates, batch_sizes, epochs, num_replicates = 2):\n",
        "  #======= creating training and test data===#\n",
        "  train_data, val_data = data_split(data, split)\n",
        "\n",
        "  num_features = train_data.shape[1]\n",
        "\n",
        "  min_train, max_train  = train_data[\"Terakhir\"].min(), train_data[\"Terakhir\"].max()\n",
        "  min_val, max_val   =    val_data[\"Terakhir\"].min(), val_data[\"Terakhir\"].max()\n",
        "\n",
        "  train_data_scaled  =  min_max_transform(train_data)\n",
        "  val_data_scaled    = min_max_transform(val_data)\n",
        "\n",
        "  X_train, y_train =   DatasetCreation(train_data_scaled, time_step)\n",
        "  X_val, y_val     =   DatasetCreation(val_data_scaled, time_step)\n",
        "   \n",
        "  #========dealing with time series=========#\n",
        "  \n",
        "  best_avg_rmse = 99999999999\n",
        "  \n",
        "  collect_rmse = []\n",
        "  \n",
        "  all_avg_rmse = np.zeros((len(optimizers_names), len(learning_rates), len(batch_sizes)))\n",
        "\n",
        "  best_hyper_parameters = {\"model\": layers, \"optimizer\": None, \"learning_rate\": None, \"batch_size\": None,\"best_avg_rmse\": None}\n",
        "  \n",
        "  for opt in range(len(optimizers_names)):\n",
        "    \n",
        "    for lr in range(len(learning_rates)):\n",
        "      \n",
        "      for batch_size in range(len(batch_sizes)):\n",
        "        \n",
        "        for i in range(num_replicates):\n",
        "\n",
        "          print(\"Running for \" + optimizers_names[opt] + \" optimizer \" + str(learning_rates[lr]) +  \" learning_rate \" +  str(batch_sizes[batch_size]) + \" batch_size and \" + str(i) +  \" replicate \" +  \"\\n\")\n",
        "                \n",
        "          model = build_model(layers, time_step, num_features, optimizers_names[opt], learning_rate = learning_rates[lr], verbose = 0)\n",
        "          \n",
        "          callback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience= 5,verbose = 1)\n",
        "\n",
        "          # history = model.fit(X_train, y_train, batch_size = batch_sizes[batch_size], epochs= epochs, validation_data = (X_val, y_val), callbacks=[callback], verbose = 1)\n",
        "          history = model.fit(X_train, y_train, batch_size = batch_sizes[batch_size], epochs= epochs, validation_data = (X_val, y_val), callbacks=[callback])\n",
        "                  \n",
        "          \n",
        "\n",
        "          #==============Making predictions in original scale ==========\n",
        "          \n",
        "          val_pred    =  min_max_inverse_transform(model.predict(X_val).ravel(), min_val, max_val)\n",
        "\n",
        "          collect_rmse.append(math.sqrt(mean_squared_error(min_max_inverse_transform(y_val, min_val, max_val),val_pred)))\n",
        "                  \n",
        "        avg_rmse = np.mean(np.array(collect_rmse))\n",
        "        all_avg_rmse[opt][lr][batch_size] = avg_rmse\n",
        "\n",
        "        if avg_rmse < best_avg_rmse:\n",
        "          best_avg_rmse = avg_rmse\n",
        "          best_hyper_parameters = {\"model\": layers,  \"optimizer\": optimizers_names[opt], \"learning_rate\": learning_rates[lr], \"batch_size\": batch_sizes[batch_size], \"best_avg_rmse\": best_avg_rmse} \n",
        "\n",
        "\n",
        "  output_dictionary = {\n",
        "      \"best_hyper_parameters\":  best_hyper_parameters,\n",
        "      \"all_avg_rmse\": all_avg_rmse\n",
        "       } \n",
        "\n",
        "  #writing output dictionary in the file\n",
        "\n",
        "  file_name = output_dir_path+ \"sl-lstm-\" + str(layers[0])+ \"-neurons-validation_results\"+ str(time.time())+ \".txt\"\n",
        "  write_dic_to_file(output_dictionary, file_name)\n",
        "\n",
        "  print(\"Best_hyper_parameters: \\n\", output_dictionary['best_hyper_parameters']) \n",
        "  print(\"all_avg_rmse: \\n\", output_dictionary['all_avg_rmse'])\n",
        "\n",
        "  return output_dictionary['best_hyper_parameters']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htx8uH9O2epV"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aQYNu38-1Xo4"
      },
      "source": [
        "### **Case I: Tuning parameter of 10N-singlelayer-LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "wsNMrDUN1YaR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0278 - val_loss: 0.0068\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0049\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0040\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0040\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0049\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0061\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0054\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0020 - val_loss: 0.0074\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0020 - val_loss: 0.0048\n",
            "Epoch 00009: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 11s 13ms/sample - loss: 0.0403 - val_loss: 0.0044\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0020 - val_loss: 0.0062\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0041\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0041\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0046\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0038\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0055\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0043\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0024 - val_loss: 0.0042\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - ETA: 0s - loss: 0.001 - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0054\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0065\n",
            "Epoch 00011: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 10s 13ms/sample - loss: 0.0146 - val_loss: 0.0073\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0048\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0028 - val_loss: 0.0057\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0085\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0042\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0061\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0028 - val_loss: 0.0037\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0019 - val_loss: 0.0040\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0049\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0036\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0021 - val_loss: 0.0043\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0058\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0020 - val_loss: 0.0070\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0042\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0022 - val_loss: 0.0102\n",
            "Epoch 00015: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 10s 12ms/sample - loss: 0.0370 - val_loss: 0.0066\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0016 - val_loss: 0.0051\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0036\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0010 - val_loss: 0.0067\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0052\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0052\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0016 - val_loss: 0.0039\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0042\n",
            "Epoch 00008: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 10s 13ms/sample - loss: 0.0288 - val_loss: 0.0050\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0044\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0015 - val_loss: 0.0082\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0020 - val_loss: 0.0092\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0042\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0012 - val_loss: 0.0047\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0048\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0013 - val_loss: 0.0033\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0040\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0013 - val_loss: 0.0053\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0014 - val_loss: 0.0036\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0014 - val_loss: 0.0049\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0037\n",
            "Epoch 00013: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 10s 13ms/sample - loss: 0.0433 - val_loss: 0.0071\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0049\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0015 - val_loss: 0.0040\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0111\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0014 - val_loss: 0.0037\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0041\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0011 - val_loss: 0.0040\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0016 - val_loss: 0.0038\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0041\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0018 - val_loss: 0.0036\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0048\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0012 - val_loss: 0.0045\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0045\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0014 - val_loss: 0.0045\n",
            "Epoch 00015: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 10s 13ms/sample - loss: 0.0719 - val_loss: 0.0087\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 670us/sample - loss: 0.0017 - val_loss: 0.0055\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 811us/sample - loss: 0.0012 - val_loss: 0.0052\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 936us/sample - loss: 0.0013 - val_loss: 0.0050\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0044\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0041\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 777us/sample - loss: 0.0011 - val_loss: 0.0045\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 687us/sample - loss: 0.0013 - val_loss: 0.0055\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 900us/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 777us/sample - loss: 0.0012 - val_loss: 0.0040\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0010 - val_loss: 0.0039\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.2710e-04 - val_loss: 0.0036\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 993us/sample - loss: 0.0010 - val_loss: 0.0036\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 923us/sample - loss: 0.0010 - val_loss: 0.0039\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0037\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 824us/sample - loss: 0.0010 - val_loss: 0.0039\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 817us/sample - loss: 9.6051e-04 - val_loss: 0.0038\n",
            "Epoch 00018: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0428 - val_loss: 0.0066\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 792us/sample - loss: 0.0019 - val_loss: 0.0060\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0057\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0048\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 888us/sample - loss: 0.0011 - val_loss: 0.0045\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0045\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0037\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 898us/sample - loss: 0.0010 - val_loss: 0.0045\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 811us/sample - loss: 0.0013 - val_loss: 0.0034\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 884us/sample - loss: 9.8307e-04 - val_loss: 0.0044\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0037\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 777us/sample - loss: 0.0013 - val_loss: 0.0054\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 808us/sample - loss: 0.0014 - val_loss: 0.0043\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 752us/sample - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 00014: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.1703 - val_loss: 0.0927\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 660us/sample - loss: 0.0305 - val_loss: 0.0233\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 751us/sample - loss: 0.0034 - val_loss: 0.0079\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0067\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 805us/sample - loss: 0.0014 - val_loss: 0.0062\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 803us/sample - loss: 0.0013 - val_loss: 0.0066\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0058\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 853us/sample - loss: 0.0011 - val_loss: 0.0057\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 748us/sample - loss: 0.0012 - val_loss: 0.0050\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0050\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0052\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0047\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.6872e-04 - val_loss: 0.0051\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 922us/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0010 - val_loss: 0.0041\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 946us/sample - loss: 0.0010 - val_loss: 0.0053\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 843us/sample - loss: 0.0010 - val_loss: 0.0039\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.8181e-04 - val_loss: 0.0037\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0039\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.7443e-04 - val_loss: 0.0037\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.3280e-04 - val_loss: 0.0046\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.8008e-04 - val_loss: 0.0035\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0036\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0042\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.8357e-04 - val_loss: 0.0034\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.0386e-04 - val_loss: 0.0039\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0010 - val_loss: 0.0039\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.3065e-04 - val_loss: 0.0037\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.8176e-04 - val_loss: 0.0036\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0010 - val_loss: 0.0035\n",
            "Epoch 00031: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 14s 17ms/sample - loss: 0.0114 - val_loss: 0.0095\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0019 - val_loss: 0.0065\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0098\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0059\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0057\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0046\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0067\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0012 - val_loss: 0.0042\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0046\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.2080e-04 - val_loss: 0.0035\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0010 - val_loss: 0.0036\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0041\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0035\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0010 - val_loss: 0.0038\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0012 - val_loss: 0.0036\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.0850e-04 - val_loss: 0.0055\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0032\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0010 - val_loss: 0.0035\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0034\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.4481e-04 - val_loss: 0.0034\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0010 - val_loss: 0.0032\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.0495e-04 - val_loss: 0.0040\n",
            "Epoch 00024: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 12s 15ms/sample - loss: 0.0181 - val_loss: 0.0117\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0072\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0080\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0020 - val_loss: 0.0078\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0065\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0057\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0050\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0012 - val_loss: 0.0041\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0054\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0036\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0038\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0034\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0034\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0036\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0036\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0041\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0056\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0010 - val_loss: 0.0036\n",
            "Epoch 00019: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 13s 16ms/sample - loss: 0.0078 - val_loss: 0.0086\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0019 - val_loss: 0.0074\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0063\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0014 - val_loss: 0.0046\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0013 - val_loss: 0.0037\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0039\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0012 - val_loss: 0.0033\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0012 - val_loss: 0.0037\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0039 0s\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 9.1120e-04 - val_loss: 0.0032\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0032\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0010 - val_loss: 0.0051\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0012 - val_loss: 0.0046\n",
            "Epoch 00015: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 12s 16ms/sample - loss: 0.0234 - val_loss: 0.0065\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0076\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0072\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0067\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0056\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0014 - val_loss: 0.0062\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0015 - val_loss: 0.0055\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0052\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0012 - val_loss: 0.0059\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0012 - val_loss: 0.0051\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0010 - val_loss: 0.0041\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0010 - val_loss: 0.0044\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 9.0191e-04 - val_loss: 0.0043\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0010 - val_loss: 0.0036\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0010 - val_loss: 0.0045\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 8.9513e-04 - val_loss: 0.0034\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.1298e-04 - val_loss: 0.0033\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0011 - val_loss: 0.0036\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.2122e-04 - val_loss: 0.0045\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.5764e-04 - val_loss: 0.0038\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0010 - val_loss: 0.0035\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 8.8548e-04 - val_loss: 0.0037\n",
            "Epoch 00023: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 13s 17ms/sample - loss: 0.0599 - val_loss: 0.0146\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0021 - val_loss: 0.0104\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0019 - val_loss: 0.0113\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0020 - val_loss: 0.0096\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0019 - val_loss: 0.0078\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0017 - val_loss: 0.0071\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0067\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0015 - val_loss: 0.0064\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0014 - val_loss: 0.0066\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0014 - val_loss: 0.0058\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0013 - val_loss: 0.0060\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0054\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0066\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0013 - val_loss: 0.0052\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0047\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.9768e-04 - val_loss: 0.0047\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0011 - val_loss: 0.0045\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 9.3727e-04 - val_loss: 0.0046\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 9.9921e-04 - val_loss: 0.0044\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.8123e-04 - val_loss: 0.0040\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.1274e-04 - val_loss: 0.0043\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.1538e-04 - val_loss: 0.0044\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.5391e-04 - val_loss: 0.0040\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 9.6643e-04 - val_loss: 0.0042\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 8.8927e-04 - val_loss: 0.0039\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 8.8207e-04 - val_loss: 0.0045\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 9.0193e-04 - val_loss: 0.0044\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0010 - val_loss: 0.0040\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 8.6569e-04 - val_loss: 0.0041\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 9.1666e-04 - val_loss: 0.0039\n",
            "Epoch 00030: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 14s 17ms/sample - loss: 0.0165 - val_loss: 0.0067\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0017 - val_loss: 0.0065\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0018 - val_loss: 0.0067\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0014 - val_loss: 0.0070\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0015 - val_loss: 0.0059\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0013 - val_loss: 0.0059\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0012 - val_loss: 0.0061\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0063\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0013 - val_loss: 0.0056\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0046\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0059\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0059 ETA: 0s - los\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.8914e-04 - val_loss: 0.0041\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.7470e-04 - val_loss: 0.0044\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0037\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.5662e-04 - val_loss: 0.0038oss: 9.9266\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.7639e-04 - val_loss: 0.0037\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 8.9996e-04 - val_loss: 0.0046\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.8135e-04 - val_loss: 0.0037\n",
            "Epoch 00021: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 13s 16ms/sample - loss: 0.0396 - val_loss: 0.0207\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 811us/sample - loss: 0.0033 - val_loss: 0.0091\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 864us/sample - loss: 0.0017 - val_loss: 0.0074\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - ETA: 0s - loss: 0.001 - 1s 852us/sample - loss: 0.0014 - val_loss: 0.0075\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0071\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 822us/sample - loss: 0.0013 - val_loss: 0.0074\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0068\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 963us/sample - loss: 0.0012 - val_loss: 0.0065\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0080\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 813us/sample - loss: 0.0013 - val_loss: 0.0062\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0065\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 921us/sample - loss: 0.0011 - val_loss: 0.0060\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 809us/sample - loss: 0.0012 - val_loss: 0.0067\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0010 - val_loss: 0.0065\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 954us/sample - loss: 9.6671e-04 - val_loss: 0.0052 - ETA: 0s - loss: 9.52\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 1s 991us/sample - loss: 9.7881e-04 - val_loss: 0.0051\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.7079e-04 - val_loss: 0.0050\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 825us/sample - loss: 9.9205e-04 - val_loss: 0.0044\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 1s 823us/sample - loss: 8.9240e-04 - val_loss: 0.0047\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.4419e-04 - val_loss: 0.0044-0 - ETA: 0s - loss\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 1s 852us/sample - loss: 9.7973e-04 - val_loss: 0.0043\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 1s 928us/sample - loss: 0.0011 - val_loss: 0.0050\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0040\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 1s 806us/sample - loss: 8.5814e-04 - val_loss: 0.0043\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.8486e-04 - val_loss: 0.0039\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 1s 919us/sample - loss: 9.2815e-04 - val_loss: 0.0043\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 1s 963us/sample - loss: 9.5043e-04 - val_loss: 0.0042\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 1s 894us/sample - loss: 8.2455e-04 - val_loss: 0.0039\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 1s 916us/sample - loss: 7.9344e-04 - val_loss: 0.0038\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 7.9920e-04 - val_loss: 0.0043\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 1s 875us/sample - loss: 0.0010 - val_loss: 0.0040\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 1s 902us/sample - loss: 9.2920e-04 - val_loss: 0.0036\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.1912e-04 - val_loss: 0.0038\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 1s 838us/sample - loss: 7.6270e-04 - val_loss: 0.0039\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 7.6374e-04 - val_loss: 0.0035\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 1s 839us/sample - loss: 7.9149e-04 - val_loss: 0.0036\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 1s 836us/sample - loss: 8.4320e-04 - val_loss: 0.0038\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 7.8750e-04 - val_loss: 0.0036\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 1s 808us/sample - loss: 7.6296e-04 - val_loss: 0.0046\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 1s 823us/sample - loss: 0.0010 - val_loss: 0.0040\n",
            "Epoch 00040: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 13s 17ms/sample - loss: 0.0305 - val_loss: 0.0122\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 841us/sample - loss: 0.0029 - val_loss: 0.0071\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 850us/sample - loss: 0.0018 - val_loss: 0.0061\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0016 - val_loss: 0.0059\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 903us/sample - loss: 0.0015 - val_loss: 0.0057\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 762us/sample - loss: 0.0013 - val_loss: 0.0062\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0055\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 836us/sample - loss: 0.0012 - val_loss: 0.0055\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 829us/sample - loss: 0.0012 - val_loss: 0.0050\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0061\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 814us/sample - loss: 0.0013 - val_loss: 0.0049\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0047\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 998us/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - ETA: 0s - loss: 9.3460e-0 - 1s 898us/sample - loss: 9.3124e-04 - val_loss: 0.0045\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.4881e-04 - val_loss: 0.0040\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 1s 829us/sample - loss: 0.0010 - val_loss: 0.0041\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 883us/sample - loss: 9.9320e-04 - val_loss: 0.0040\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0045\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 1s 928us/sample - loss: 0.0012 - val_loss: 0.0042\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 852us/sample - loss: 8.4391e-04 - val_loss: 0.0035\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.4392e-04 - val_loss: 0.0035-\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 1s 864us/sample - loss: 0.0014 - val_loss: 0.0048\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.7748e-04 - val_loss: 0.0034\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.5208e-04 - val_loss: 0.0039\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 1s 902us/sample - loss: 9.3099e-04 - val_loss: 0.0035\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.4695e-04 - val_loss: 0.0033\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 1s 891us/sample - loss: 9.0344e-04 - val_loss: 0.0034\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 1s 841us/sample - loss: 8.1475e-04 - val_loss: 0.0033\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 7.8808e-04 - val_loss: 0.0035\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 1s 966us/sample - loss: 0.0010 - val_loss: 0.0037\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 1s 869us/sample - loss: 9.5306e-04 - val_loss: 0.0036\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.9068e-04 - val_loss: 0.0032\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.7521e-04 - val_loss: 0.0034\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 1s 883us/sample - loss: 9.2218e-04 - val_loss: 0.0035\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.2486e-04 - val_loss: 0.0034\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 1s 991us/sample - loss: 8.0753e-04 - val_loss: 0.0035\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.8230e-04 - val_loss: 0.0032\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 1s 996us/sample - loss: 8.4519e-04 - val_loss: 0.0034\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 1s 928us/sample - loss: 7.9811e-04 - val_loss: 0.0036\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.2433e-04 - val_loss: 0.0034\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 1s 912us/sample - loss: 8.8532e-04 - val_loss: 0.0035\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.1891e-04 - val_loss: 0.0033\n",
            "Epoch 00042: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 13s 17ms/sample - loss: 0.0437 - val_loss: 0.0207\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 739us/sample - loss: 0.0044 - val_loss: 0.0123\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0019 - val_loss: 0.0103\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0091\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 806us/sample - loss: 0.0020 - val_loss: 0.0097\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 817us/sample - loss: 0.0017 - val_loss: 0.0087\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 958us/sample - loss: 0.0015 - val_loss: 0.0075\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0076\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0069\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 971us/sample - loss: 0.0014 - val_loss: 0.0063\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0062\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 913us/sample - loss: 0.0014 - val_loss: 0.0060\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 843us/sample - loss: 0.0013 - val_loss: 0.0056\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0055\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 916us/sample - loss: 0.0014 - val_loss: 0.0053\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 1s 841us/sample - loss: 0.0011 - val_loss: 0.0055\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0049\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 865us/sample - loss: 0.0011 - val_loss: 0.0046\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0054\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 881us/sample - loss: 0.0012 - val_loss: 0.0050\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.9429e-04 - val_loss: 0.0046\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 1s 916us/sample - loss: 0.0010 - val_loss: 0.0045\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.7391e-04 - val_loss: 0.0040\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 1s 975us/sample - loss: 9.8408e-04 - val_loss: 0.0040\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 1s 814us/sample - loss: 8.9087e-04 - val_loss: 0.0040\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.8984e-04 - val_loss: 0.0041\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 1s 786us/sample - loss: 0.0010 - val_loss: 0.0039\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 1s 823us/sample - loss: 9.5107e-04 - val_loss: 0.0041\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.0496e-04 - val_loss: 0.0037\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 1s 761us/sample - loss: 8.5346e-04 - val_loss: 0.0036\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.8129e-04 - val_loss: 0.0038\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 1s 889us/sample - loss: 8.5529e-04 - val_loss: 0.0035\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 1s 825us/sample - loss: 8.7777e-04 - val_loss: 0.0035\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.4765e-04 - val_loss: 0.0034\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 1s 926us/sample - loss: 7.7379e-04 - val_loss: 0.0042\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.6641e-04 - val_loss: 0.0035\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 1s 748us/sample - loss: 9.4976e-04 - val_loss: 0.0037\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.6140e-04 - val_loss: 0.0035\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 1s 800us/sample - loss: 7.7989e-04 - val_loss: 0.0037\n",
            "Epoch 00039: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 15s 20ms/sample - loss: 0.0909 - val_loss: 0.0418\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0070 - val_loss: 0.0176\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0034 - val_loss: 0.0142\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0029 - val_loss: 0.0128\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0026 - val_loss: 0.0114\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0024 - val_loss: 0.0110\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0023 - val_loss: 0.0109\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0022 - val_loss: 0.0101\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0021 - val_loss: 0.0100\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0020 - val_loss: 0.0100\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0020 - val_loss: 0.0094\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0018 - val_loss: 0.0098\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0018 - val_loss: 0.0089\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0090\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0018 - val_loss: 0.0087\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0016 - val_loss: 0.0095\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0016 - val_loss: 0.0081\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0016 - val_loss: 0.0074\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0015 - val_loss: 0.0079\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0078\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0014 - val_loss: 0.0073\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0078\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0014 - val_loss: 0.0072\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0014 - val_loss: 0.0065\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0076\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0013 - val_loss: 0.0068\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0013 - val_loss: 0.0063\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0064\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0013 - val_loss: 0.0065\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0012 - val_loss: 0.0060\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0061\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0012 - val_loss: 0.0065\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0053\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0066\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0056\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0054\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0010 - val_loss: 0.0049\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0054\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0010 - val_loss: 0.0059\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0063\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0010 - val_loss: 0.0053\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.6233e-04 - val_loss: 0.0052\n",
            "Epoch 00042: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 16s 20ms/sample - loss: 0.0215 - val_loss: 0.0104\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0021 - val_loss: 0.0075\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0019 - val_loss: 0.0079\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0071\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0016 - val_loss: 0.0070\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0015 - val_loss: 0.0069\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0067\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0014 - val_loss: 0.0065\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0067\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0063\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0013 - val_loss: 0.0060\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0058\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - ETA: 0s - loss: 0.001 - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0057\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0060\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0053\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0053\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0052\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0049\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0046\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0048\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.8463e-04 - val_loss: 0.0045\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.8002e-04 - val_loss: 0.0049\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 9.8576e-04 - val_loss: 0.0047-\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.7187e-04 - val_loss: 0.0042\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 9.4766e-04 - val_loss: 0.0043\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.3865e-04 - val_loss: 0.0042\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.1725e-04 - val_loss: 0.0039\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.5161e-04 - val_loss: 0.0044\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 9.5941e-04 - val_loss: 0.0040\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.9208e-04 - val_loss: 0.0041\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 8.7767e-04 - val_loss: 0.0040\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.6096e-04 - val_loss: 0.0040\n",
            "Epoch 00032: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 16s 20ms/sample - loss: 0.0480 - val_loss: 0.0198\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0053 - val_loss: 0.0116\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0024 - val_loss: 0.0108\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0023 - val_loss: 0.0100\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0021 - val_loss: 0.0090\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0020 - val_loss: 0.0087A: \n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0019 - val_loss: 0.0087\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0019 - val_loss: 0.0091\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0018 - val_loss: 0.0086\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0018 - val_loss: 0.0085\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0018 - val_loss: 0.008901\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0083\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0017 - val_loss: 0.0086\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0016 - val_loss: 0.0080\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0016 - val_loss: 0.0078\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0016 - val_loss: 0.0082\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0015 - val_loss: 0.0073\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0077\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0073\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0070\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0014 - val_loss: 0.0068\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0064\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0067\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0013 - val_loss: 0.0069\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0059\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0065\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0054\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0012 - val_loss: 0.0055\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0055\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0053\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0051\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0048\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0047\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0049\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 9.8490e-04 - val_loss: 0.0055\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 9.6807e-04 - val_loss: 0.0043\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.4938e-04 - val_loss: 0.0041\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 9.2639e-04 - val_loss: 0.0045\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.7344e-04 - val_loss: 0.0042\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 8.8781e-04 - val_loss: 0.0040\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.1669e-04 - val_loss: 0.0041TA: 0s - \n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.6241e-04 - val_loss: 0.0043\n",
            "Epoch 43/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.9022e-04 - val_loss: 0.0037\n",
            "Epoch 44/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.4048e-04 - val_loss: 0.0042\n",
            "Epoch 45/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 8.8415e-04 - val_loss: 0.0041\n",
            "Epoch 46/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.2863e-04 - val_loss: 0.0038\n",
            "Epoch 47/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 8.2188e-04 - val_loss: 0.0037\n",
            "Epoch 48/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 7.8869e-04 - val_loss: 0.0039\n",
            "Epoch 00048: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 16s 20ms/sample - loss: 0.3030 - val_loss: 0.0392\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0169 - val_loss: 0.0227\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0093 - val_loss: 0.0180\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0048 - val_loss: 0.0148\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0032 - val_loss: 0.0142\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0028 - val_loss: 0.0144\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0026 - val_loss: 0.0124\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0024 - val_loss: 0.0121\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0023 - val_loss: 0.0115\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0022 - val_loss: 0.0111\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0021 - val_loss: 0.0098\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0020 - val_loss: 0.0093\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0020 - val_loss: 0.0101\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0019 - val_loss: 0.0096\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0018 - val_loss: 0.0090\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0018 - val_loss: 0.0091\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0018 - val_loss: 0.0086\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0086\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0089\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - ETA: 0s - loss: 0.0017 - ETA: - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0091\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0084\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0084\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0016 - val_loss: 0.0084\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0016 - val_loss: 0.0085\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0085\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0086\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0085\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0085\n",
            "Epoch 00028: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 16s 20ms/sample - loss: 0.0609 - val_loss: 0.0624\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0146 - val_loss: 0.0214\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0053 - val_loss: 0.0153\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0039 - val_loss: 0.0125\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0032 - val_loss: 0.0115\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0028 - val_loss: 0.0096\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0026 - val_loss: 0.0096\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0024 - val_loss: 0.0092\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0022 - val_loss: 0.0093\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0022 - val_loss: 0.0094 ETA: 0\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0022 - val_loss: 0.0093\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0020 - val_loss: 0.0096\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0019 - val_loss: 0.0092\n",
            "Epoch 00013: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 0.0457 - val_loss: 0.0422\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0054 - val_loss: 0.0199\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0031 - val_loss: 0.0154\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0025 - val_loss: 0.0144\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0022 - val_loss: 0.0127\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0020 - val_loss: 0.0123\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0019 - val_loss: 0.0119\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0126\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0117\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0114\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0110\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0116\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0110\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0112\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0016 - val_loss: 0.0108\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0100\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0105\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0097\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0093\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0094\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0092\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0094\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0086\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0091\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0088\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0083\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0081\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0077\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0014 - val_loss: 0.0087\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0079: 0s - loss: 0.00\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0077\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0082\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0086\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0076\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0078\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0075\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0071\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0072\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0075\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0071\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0068\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0069\n",
            "Epoch 43/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0067\n",
            "Epoch 44/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0066\n",
            "Epoch 45/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0065\n",
            "Epoch 46/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.8006e-04 - val_loss: 0.0062\n",
            "Epoch 47/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0010 - val_loss: 0.0063\n",
            "Epoch 48/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.5708e-04 - val_loss: 0.0062\n",
            "Epoch 49/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.9787e-04 - val_loss: 0.0062\n",
            "Epoch 50/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.4617e-04 - val_loss: 0.0062\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 16s 21ms/sample - loss: 0.4459 - val_loss: 0.1457\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 933us/sample - loss: 0.0439 - val_loss: 0.0542\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0138 - val_loss: 0.0401\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0084 - val_loss: 0.0271\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0051 - val_loss: 0.0198\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0037 - val_loss: 0.0164\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0031 - val_loss: 0.0147\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 941us/sample - loss: 0.0029 - val_loss: 0.0138\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0027 - val_loss: 0.0132\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0026 - val_loss: 0.0127\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0025 - val_loss: 0.0125\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0024 - val_loss: 0.0121\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0023 - val_loss: 0.0117\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 955us/sample - loss: 0.0023 - val_loss: 0.0115\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0022 - val_loss: 0.0114\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 1s 847us/sample - loss: 0.0022 - val_loss: 0.0112\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0021 - val_loss: 0.0110\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0021 - val_loss: 0.0109\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0021 - val_loss: 0.0108\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 918us/sample - loss: 0.0020 - val_loss: 0.0106\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0020 - val_loss: 0.0105\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0020 - val_loss: 0.0104\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0019 - val_loss: 0.0104\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 1s 837us/sample - loss: 0.0019 - val_loss: 0.0102\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0019 - val_loss: 0.0103\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0102\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0100\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0100\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0099\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0100\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0099\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 1s 991us/sample - loss: 0.0017 - val_loss: 0.0099\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0099\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0098\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0098\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0098\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 1s 991us/sample - loss: 0.0016 - val_loss: 0.0097\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0016 - val_loss: 0.0097\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 1s 933us/sample - loss: 0.0016 - val_loss: 0.0098\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0016 - val_loss: 0.0098\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0016 - val_loss: 0.0097\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0016 - val_loss: 0.0095\n",
            "Epoch 43/50\n",
            "787/787 [==============================] - 1s 950us/sample - loss: 0.0016 - val_loss: 0.0095\n",
            "Epoch 44/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0016 - val_loss: 0.0095\n",
            "Epoch 45/50\n",
            "787/787 [==============================] - 1s 940us/sample - loss: 0.0016 - val_loss: 0.0095\n",
            "Epoch 46/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0094\n",
            "Epoch 47/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0094\n",
            "Epoch 48/50\n",
            "787/787 [==============================] - 1s 980us/sample - loss: 0.0015 - val_loss: 0.0093\n",
            "Epoch 49/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0092\n",
            "Epoch 50/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0094\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 16s 20ms/sample - loss: 0.0390 - val_loss: 0.0149\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 940us/sample - loss: 0.0073 - val_loss: 0.0091\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0029 - val_loss: 0.0073\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0020 - val_loss: 0.0076\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 973us/sample - loss: 0.0019 - val_loss: 0.0074\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0019 - val_loss: 0.0075\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0074\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0075\n",
            "Epoch 00008: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0631 - val_loss: 0.0541\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 877us/sample - loss: 0.0180 - val_loss: 0.0318\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0087 - val_loss: 0.0241\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0046 - val_loss: 0.0217\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0033 - val_loss: 0.0195\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 909us/sample - loss: 0.0028 - val_loss: 0.0175\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0026 - val_loss: 0.0168\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0024 - val_loss: 0.0142\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0022 - val_loss: 0.0131\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0021 - val_loss: 0.0119\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0020 - val_loss: 0.0115\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0020 - val_loss: 0.0107\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0019 - val_loss: 0.0110\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0019 - val_loss: 0.0102\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0019 - val_loss: 0.0092\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0096\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0097\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0096\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0092\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0089\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0088\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0089\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0087\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0086\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0085\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0083\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0083\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0016 - val_loss: 0.0083\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 1s 964us/sample - loss: 0.0016 - val_loss: 0.0082\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0016 - val_loss: 0.0084\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0016 - val_loss: 0.0080\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 1s 992us/sample - loss: 0.0016 - val_loss: 0.0080\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0077\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 1s 949us/sample - loss: 0.0016 - val_loss: 0.0078\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0078\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0076\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0075\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0074\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0073\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0073\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0072\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0014 - val_loss: 0.0070\n",
            "Epoch 43/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0072\n",
            "Epoch 44/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0071\n",
            "Epoch 45/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0069\n",
            "Epoch 46/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0068\n",
            "Epoch 47/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0068\n",
            "Epoch 48/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0067\n",
            "Epoch 49/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0065\n",
            "Epoch 50/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0065\n",
            "Best_hyper_parameters: \n",
            " {'model': [10], 'optimizer': 'Adam', 'learning_rate': 0.01, 'batch_size': 16, 'best_avg_rmse': 14.502820475707669}\n",
            "all_avg_rmse: \n",
            " [[[18.40022105 16.2320049  15.38553572]\n",
            "  [15.04101055 14.73743195 14.50282048]\n",
            "  [14.50293415 15.13392018 15.60870625]]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model': [10],\n",
              " 'optimizer': 'Adam',\n",
              " 'learning_rate': 0.01,\n",
              " 'batch_size': 16,\n",
              " 'best_avg_rmse': 14.502820475707669}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "layers = [10]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam']\n",
        "#optimizers_names = ['Adam']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs = 50\n",
        "num_replicates = 3\n",
        "test_split = 0.2 \n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "N10_best_hyper_parameters = hyper_parameter_tuning(layers, train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "N10_best_hyper_parameters\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fArhgB-XNd1U"
      },
      "source": [
        "### **Case II: Tuning parameter of 30N-singlelayer-LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "p6tMjJzrNb0O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 0.0767 - val_loss: 0.0034\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0037\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0054\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0020 - val_loss: 0.0060\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0016 - val_loss: 0.0039\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0045\n",
            "Epoch 00006: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 20s 26ms/sample - loss: 0.0459 - val_loss: 0.0044\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0023 - val_loss: 0.0049\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0012 - val_loss: 0.0042\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0042\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0062\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0036\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0023 - val_loss: 0.0063\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0015 - val_loss: 0.0039\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0017 - val_loss: 0.0036\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0042\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0018 - val_loss: 0.0069\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0017 - val_loss: 0.0038\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0031 - val_loss: 0.0070\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0064 - val_loss: 0.0054\n",
            "Epoch 00014: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 21s 26ms/sample - loss: 0.0494 - val_loss: 0.0059\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0017 - val_loss: 0.0068\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0050\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0079\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0016 - val_loss: 0.0040\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0047\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0019 - val_loss: 0.0063\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0013 - val_loss: 0.0074\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0048\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0015 - val_loss: 0.0050\n",
            "Epoch 00010: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0685 - val_loss: 0.0083\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0014 - val_loss: 0.0049\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0013 - val_loss: 0.0042\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0044\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0013 - val_loss: 0.0043\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0041\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0049\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0042\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0011 - val_loss: 0.0055\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0046\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0013 - val_loss: 0.0051\n",
            "Epoch 00011: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 22s 27ms/sample - loss: 0.1635 - val_loss: 0.0050\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0065\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0012 - val_loss: 0.0072\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0019 - val_loss: 0.0046\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0039\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0055\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0055\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0074\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0049\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0058\n",
            "Epoch 00010: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0891 - val_loss: 0.0092\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0015 - val_loss: 0.0047\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0041\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0048\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0042\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0036\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0012 - val_loss: 0.0040\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0040\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0013 - val_loss: 0.0038\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0011 - val_loss: 0.0037\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0012 - val_loss: 0.0038\n",
            "Epoch 00012: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.4966 - val_loss: 0.0072\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 881us/sample - loss: 0.0018 - val_loss: 0.0043\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 970us/sample - loss: 0.0010 - val_loss: 0.0038\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.8783e-04 - val_loss: 0.0035\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 828us/sample - loss: 9.3815e-04 - val_loss: 0.0043\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 968us/sample - loss: 0.0012 - val_loss: 0.0067\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0046\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 960us/sample - loss: 0.0013 - val_loss: 0.0053\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 848us/sample - loss: 0.0011 - val_loss: 0.0048\n",
            "Epoch 00009: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 20s 26ms/sample - loss: 0.1389 - val_loss: 0.0069\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0045\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 867us/sample - loss: 0.0010 - val_loss: 0.0044\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 883us/sample - loss: 0.0012 - val_loss: 0.0046\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0038\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 958us/sample - loss: 0.0012 - val_loss: 0.0042\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 898us/sample - loss: 9.5332e-04 - val_loss: 0.0046\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 791us/sample - loss: 8.9127e-04 - val_loss: 0.0038\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.8975e-04 - val_loss: 0.0036\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 923us/sample - loss: 9.5556e-04 - val_loss: 0.0046\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 883us/sample - loss: 0.0010 - val_loss: 0.0041\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 843us/sample - loss: 8.2159e-04 - val_loss: 0.0039\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.5161e-04 - val_loss: 0.0038\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.8737e-04 - val_loss: 0.0037\n",
            "Epoch 00014: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 0.1106 - val_loss: 0.0142\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 860us/sample - loss: 0.0026 - val_loss: 0.0066\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0057\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 958us/sample - loss: 0.0016 - val_loss: 0.0050\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 881us/sample - loss: 0.0011 - val_loss: 0.0046\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 862us/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 983us/sample - loss: 0.0014 - val_loss: 0.0042\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 945us/sample - loss: 0.0011 - val_loss: 0.0045\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0010 - val_loss: 0.0041010   \n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 897us/sample - loss: 0.0012 - val_loss: 0.0045\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 878us/sample - loss: 0.0013 - val_loss: 0.0043\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0010 - val_loss: 0.0046\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 907us/sample - loss: 9.0145e-04 - val_loss: 0.0040\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 894us/sample - loss: 0.0010 - val_loss: 0.0047\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 1s 886us/sample - loss: 9.4315e-04 - val_loss: 0.0053\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0040\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 955us/sample - loss: 0.0016 - val_loss: 0.0049\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0042\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.4994e-04 - val_loss: 0.0038\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 1s 867us/sample - loss: 9.4513e-04 - val_loss: 0.0043\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.7915e-04 - val_loss: 0.0047\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.3588e-04 - val_loss: 0.0040\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.4150e-04 - val_loss: 0.0037\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.4138e-04 - val_loss: 0.0039\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.3910e-04 - val_loss: 0.0036\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0042\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0011 - val_loss: 0.0058\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0010 - val_loss: 0.0043010\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.5202e-04 - val_loss: 0.0038\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 9.5055e-04 - val_loss: 0.0047\n",
            "Epoch 00031: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 26s 33ms/sample - loss: 0.0065 - val_loss: 0.0085\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0020 - val_loss: 0.0049\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0049\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0040\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0042\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0037\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0012 - val_loss: 0.0041\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0041\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0032\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0045\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0031\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0032\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0042\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0010 - val_loss: 0.0037\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0037\n",
            "Epoch 00016: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 22s 29ms/sample - loss: 0.0132 - val_loss: 0.0101\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0018 - val_loss: 0.0081\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0051\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0052\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0016 - val_loss: 0.0046\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0040\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0040\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0033\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0036\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.6965e-04 - val_loss: 0.0039\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0032- loss: 0.0010   - ETA: 0s - \n",
            "Epoch 14/50\n",
            "787/787 [==============================] - ETA: 0s - loss: 0.001 - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0040\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0038\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.9857e-04 - val_loss: 0.0033\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0043\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 4s 4ms/sample - loss: 9.5185e-04 - val_loss: 0.0033\n",
            "Epoch 00018: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 23s 29ms/sample - loss: 0.0060 - val_loss: 0.0083\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0023 - val_loss: 0.0066\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0044\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - ETA: 0s - loss: 0.001 - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0052\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0044\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0041\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0039\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0032\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0049\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0034\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0049\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0046\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0039\n",
            "Epoch 00013: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 23s 29ms/sample - loss: 0.0134 - val_loss: 0.0067\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0017 - val_loss: 0.0074\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0072\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0062\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0070\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0058 ET\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0050\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0046\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0042\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0040\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0011 - val_loss: 0.0035\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0041\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.1774e-04 - val_loss: 0.0034\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0042 ETA: 0s - loss: 0\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.2779e-04 - val_loss: 0.0037\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.0133e-04 - val_loss: 0.0034\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 8.3443e-04 - val_loss: 0.0040\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.6652e-04 - val_loss: 0.0033\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 9.6370e-04 - val_loss: 0.0042\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.6254e-04 - val_loss: 0.0037\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.7130e-04 - val_loss: 0.0035\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0052\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.4460e-04 - val_loss: 0.0034\n",
            "Epoch 00024: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 24s 31ms/sample - loss: 0.0205 - val_loss: 0.0075\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0065\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0062\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0056\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0053\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - ETA: 0s - loss: 0.0014-  - ETA: 0s - loss: 0.00 - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0057\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0048\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0044\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.7206e-04 - val_loss: 0.0044\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.00342\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.8973e-04 - val_loss: 0.0041\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.1533e-04 - val_loss: 0.0034\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0034 - ETA: 0s \n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 8.7592e-04 - val_loss: 0.0034\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0038\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.4074e-04 - val_loss: 0.0037\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 9.6694e-04 - val_loss: 0.0035\n",
            "Epoch 00020: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 23s 30ms/sample - loss: 0.0075 - val_loss: 0.0069\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0018 - val_loss: 0.0064\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0066\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0066\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0063\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0014 - val_loss: 0.0049\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0039\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0010 - val_loss: 0.0043\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0036\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0040\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0010 - val_loss: 0.0053TA: 0s - loss: 0. - ETA: 0s - loss:\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0044\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.5619e-04 - val_loss: 0.0038\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 9.4033e-04 - val_loss: 0.0035\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.6258e-04 - val_loss: 0.0032\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.5088e-04 - val_loss: 0.0032\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 8.7821e-04 - val_loss: 0.0033\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 9.4552e-04 - val_loss: 0.0042\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.0847e-04 - val_loss: 0.0033\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0010 - val_loss: 0.0036\n",
            "Epoch 00022: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 22s 28ms/sample - loss: 0.0221 - val_loss: 0.0131\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 869us/sample - loss: 0.0019 - val_loss: 0.0088\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0075\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0016 - val_loss: 0.0072\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 968us/sample - loss: 0.0016 - val_loss: 0.0073\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0058\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0070\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0062\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0059\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0054\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 983us/sample - loss: 0.0014 - val_loss: 0.0047\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0059\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0010 - val_loss: 0.0059\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 925us/sample - loss: 0.0012 - val_loss: 0.0044\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.6150e-04 - val_loss: 0.0041\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.6690e-04 - val_loss: 0.0039\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.6916e-04 - val_loss: 0.0042\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.4299e-04 - val_loss: 0.0041\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - ETA: 0s - loss: 8.5607e-0 - 1s 997us/sample - loss: 8.5843e-04 - val_loss: 0.0037\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0034\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.7744e-04 - val_loss: 0.0035- ETA: 0s - loss: 0.001 - ETA: 0s - loss: \n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.2236e-04 - val_loss: 0.0032\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.3837e-04 - val_loss: 0.0036\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 1s 958us/sample - loss: 9.1425e-04 - val_loss: 0.0035\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.0238e-04 - val_loss: 0.0034\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.6448e-04 - val_loss: 0.0032\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 1s 955us/sample - loss: 8.8869e-04 - val_loss: 0.0033\n",
            "Epoch 00027: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 23s 29ms/sample - loss: 0.0358 - val_loss: 0.0093\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0020 - val_loss: 0.0071\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0016 - val_loss: 0.0072\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 984us/sample - loss: 0.0015 - val_loss: 0.0075\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0069\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 973us/sample - loss: 0.0018 - val_loss: 0.0073\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0061\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0054\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0062\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0051\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0050\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 974us/sample - loss: 0.0012 - val_loss: 0.0057\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0051\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0046\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.7331e-04 - val_loss: 0.0054\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.4869e-04 - val_loss: 0.0050\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0010 - val_loss: 0.0037\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 8.8885e-04 - val_loss: 0.0042- - ETA: 0s - loss: 9.0446e-0\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.7132e-04 - val_loss: 0.0040\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.2450e-04 - val_loss: 0.0038\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.8253e-04 - val_loss: 0.0037\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.7799e-04 - val_loss: 0.0047\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.9089e-04 - val_loss: 0.0034\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.2998e-04 - val_loss: 0.0039\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 1s 942us/sample - loss: 8.2720e-04 - val_loss: 0.0032\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.4641e-04 - val_loss: 0.0033\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.2560e-04 - val_loss: 0.0034\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 1s 979us/sample - loss: 9.0970e-04 - val_loss: 0.0046\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.5695e-04 - val_loss: 0.0033\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.6479e-04 - val_loss: 0.0039\n",
            "Epoch 00030: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 24s 30ms/sample - loss: 0.0331 - val_loss: 0.0091\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0019 - val_loss: 0.0083\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0020 - val_loss: 0.0080\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0017 - val_loss: 0.0081\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0073\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0016 - val_loss: 0.0076\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0016 - val_loss: 0.0065\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0060\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0059\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0054\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0052\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 996us/sample - loss: 0.0012 - val_loss: 0.0049\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0046\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 996us/sample - loss: 0.0011 - val_loss: 0.0050\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0054\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0010 - val_loss: 0.0039\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.2509e-04 - val_loss: 0.0040A: 0s - loss: 9.8701\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.0614e-04 - val_loss: 0.0037\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.4130e-04 - val_loss: 0.0042\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.4309e-04 - val_loss: 0.0035\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 1s 968us/sample - loss: 0.0011 - val_loss: 0.0035\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0010 - val_loss: 0.0035\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.9439e-04 - val_loss: 0.0038\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.4222e-04 - val_loss: 0.0035\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.0593e-04 - val_loss: 0.0034\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.4464e-04 - val_loss: 0.0034\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 1s 942us/sample - loss: 8.9316e-04 - val_loss: 0.0033\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.3794e-04 - val_loss: 0.0039\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.3505e-04 - val_loss: 0.0037\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 8.4345e-04 - val_loss: 0.0038\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 7.8618e-04 - val_loss: 0.0039\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 9.5100e-04 - val_loss: 0.0040\n",
            "Epoch 00035: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 26s 33ms/sample - loss: 0.0153 - val_loss: 0.0078\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0019 - val_loss: 0.0079\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0018 - val_loss: 0.0082\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0018 - val_loss: 0.0073\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0079\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0072\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0016 - val_loss: 0.0075\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0015 - val_loss: 0.0072\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0064\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0015 - val_loss: 0.0064\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0062\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0014 - val_loss: 0.0056\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0055\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0063\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0012 - val_loss: 0.0059\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0012 - val_loss: 0.0051\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0012 - val_loss: 0.0059 - ETA:  - ETA: 0s - loss: 0.00\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0011 - val_loss: 0.0048\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0011 - val_loss: 0.0050\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.7748e-04 - val_loss: 0.0044- loss\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.3096e-04 - val_loss: 0.0041\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 9.6897e-04 - val_loss: 0.0037\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 9.6114e-04 - val_loss: 0.0041 ETA: \n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.1357e-04 - val_loss: 0.0037s - loss: \n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.5425e-04 - val_loss: 0.0039\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.0561e-04 - val_loss: 0.0035\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.4828e-04 - val_loss: 0.0037\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.6932e-04 - val_loss: 0.0035\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.5036e-04 - val_loss: 0.0036\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.6726e-04 - val_loss: 0.0046\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.3645e-04 - val_loss: 0.0034oss: 7.5551e-0 - ETA: 0s - l\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.6961e-04 - val_loss: 0.0035oss: 9.1329e-0 - ETA: 0s - loss: 9.\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.6627e-04 - val_loss: 0.0035\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 8.0417e-04 - val_loss: 0.0035\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 8.0861e-04 - val_loss: 0.0033: 1s - loss: 9.2411 - ETA: 1s  - ETA: 0s - loss: 8.1348e-0\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.2406e-04 - val_loss: 0.0038\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.1822e-04 - val_loss: 0.0033\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 4s 4ms/sample - loss: 8.1538e-04 - val_loss: 0.0038\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.0341e-04 - val_loss: 0.0034-0\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.1861e-04 - val_loss: 0.0034\n",
            "Epoch 43/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.2581e-04 - val_loss: 0.0033\n",
            "Epoch 44/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.3650e-04 - val_loss: 0.0035\n",
            "Epoch 00044: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 35s 45ms/sample - loss: 0.0212 - val_loss: 0.0089\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0020 - val_loss: 0.0104\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0083\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0086\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0086\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0078\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0099\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0016 - val_loss: 0.0076: 1s - loss: 0.001 - ETA: 1s - loss: 0 \n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0015 - val_loss: 0.0080TA: 0s - \n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0015 - val_loss: 0.0072\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0015 - val_loss: 0.0069\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0015 - val_loss: 0.0064\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0013 - val_loss: 0.0059 ETA: - ETA: 0s - loss: 0.00\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0013 - val_loss: 0.0071- ETA: 2s - loss: 0.001 - \n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0013 - val_loss: 0.0061\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0012 - val_loss: 0.0055\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0012 - val_loss: 0.0048 - \n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0011 - val_loss: 0.0049\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0011 - val_loss: 0.0048TA: 1s - loss - ETA: - ETA: 0s - loss: 0.00 - ETA: 0s - loss: 0.0\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0011 - val_loss: 0.0049\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0010 - val_loss: 0.0041\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 9.9758e-04 - val_loss: 0.0040 2s - loss: 0.001 - ETA: 2s - loss: 0.00 - ETA: 2s \n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.7695e-04 - val_loss: 0.0041\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.0464e-04 - val_loss: 0.0044\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 4s 4ms/sample - loss: 9.2738e-04 - val_loss: 0.0037\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 4s 4ms/sample - loss: 8.9285e-04 - val_loss: 0.0038\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.7179e-04 - val_loss: 0.0035\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.2140e-04 - val_loss: 0.0036: 0s - loss: 8.1106e-\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.1625e-04 - val_loss: 0.0035\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.5635e-04 - val_loss: 0.0036: 1s - lo - ETA: 1s - l - ETA: 0s - lo\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.8442e-04 - val_loss: 0.0035\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.9264e-04 - val_loss: 0.0036\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.0521e-04 - val_loss: 0.0037\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.4980e-04 - val_loss: 0.0033s: 8.4870e-\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.4830e-04 - val_loss: 0.0039\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 7.9880e-04 - val_loss: 0.0036\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.1268e-04 - val_loss: 0.0033\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.5579e-04 - val_loss: 0.0033- ETA: 1s - loss: 8.787 -\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 7.9731e-04 - val_loss: 0.0035\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 7.9615e-04 - val_loss: 0.0033\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 7.8966e-04 - val_loss: 0.0032\n",
            "Epoch 43/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.3547e-04 - val_loss: 0.0035 0s - loss: 8\n",
            "Epoch 44/50\n",
            "787/787 [==============================] - 4s 4ms/sample - loss: 8.0753e-04 - val_loss: 0.0039\n",
            "Epoch 45/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.5654e-04 - val_loss: 0.0034\n",
            "Epoch 46/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.2989e-04 - val_loss: 0.0032\n",
            "Epoch 47/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.1272e-04 - val_loss: 0.0033TA:  - ETA: 2s - loss: 8 - ETA: 2s - lo - ETA: 1s\n",
            "Epoch 00047: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 28s 35ms/sample - loss: 0.0252 - val_loss: 0.0111\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0025 - val_loss: 0.0111\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0021 - val_loss: 0.0105\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0020 - val_loss: 0.0104\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0020 - val_loss: 0.0094\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0020 - val_loss: 0.0078\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0017 - val_loss: 0.0082\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0017 - val_loss: 0.0080\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0016 - val_loss: 0.0100\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0017 - val_loss: 0.0067\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0016 - val_loss: 0.0071\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0015 - val_loss: 0.0076\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0015 - val_loss: 0.0071\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0014 - val_loss: 0.0075\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0015 - val_loss: 0.0075\n",
            "Epoch 00015: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 33s 41ms/sample - loss: 0.0479 - val_loss: 0.0201\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0043 - val_loss: 0.0094\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0024 - val_loss: 0.0087\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0021 - val_loss: 0.0083\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0085\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0019 - val_loss: 0.0087\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0088\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0018 - val_loss: 0.0088\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0088\n",
            "Epoch 00009: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 30s 38ms/sample - loss: 0.0668 - val_loss: 0.0219\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0030 - val_loss: 0.0096\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0020 - val_loss: 0.0096\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0020 - val_loss: 0.0106\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0019 - val_loss: 0.0094\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0019 - val_loss: 0.0100\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0019 - val_loss: 0.0096: 0s - loss: 0.00\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0018 - val_loss: 0.0089\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0018 - val_loss: 0.0088\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0087\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0082\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0017 - val_loss: 0.0086\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0092\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0016 - val_loss: 0.0083\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0016 - val_loss: 0.0074\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.00790s - loss: 0.0\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0072\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0071\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0068\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0014 - val_loss: 0.0072\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0067\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0013 - val_loss: 0.0072\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0013 - val_loss: 0.0063\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0013 - val_loss: 0.0059\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0058\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0061\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0012 - val_loss: 0.00551s - loss: 0 - ETA: 0s \n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0055s \n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0055\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0050\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0049 0s - loss: - ETA: 0s - loss: \n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0049\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0010 - val_loss: 0.0048 ETA: 0s - loss\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0010 - val_loss: 0.0045\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.9509e-04 - val_loss: 0.0044\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0010 - val_loss: 0.0042 ETA: 0s - loss: 0.\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.3959e-04 - val_loss: 0.0045\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.0331e-04 - val_loss: 0.0040\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.8288e-04 - val_loss: 0.0040\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.6896e-04 - val_loss: 0.0038\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.1903e-04 - val_loss: 0.0038\n",
            "Epoch 43/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.8101e-04 - val_loss: 0.0038\n",
            "Epoch 44/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.8641e-04 - val_loss: 0.0037\n",
            "Epoch 45/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.9222e-04 - val_loss: 0.0037\n",
            "Epoch 46/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.8366e-04 - val_loss: 0.0036\n",
            "Epoch 47/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.0769e-04 - val_loss: 0.0036\n",
            "Epoch 48/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.5629e-04 - val_loss: 0.0038\n",
            "Epoch 49/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.7779e-04 - val_loss: 0.0036\n",
            "Epoch 50/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.2944e-04 - val_loss: 0.0036\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 29s 36ms/sample - loss: 0.0181 - val_loss: 0.0099\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0021 - val_loss: 0.0092\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0018 - val_loss: 0.0086\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0084\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0082\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0081\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0079\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0073\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0083\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0073 ETA: 0s - loss - ETA: 0s - loss: 0\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0071\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - ETA: 0s - loss: 0.001 - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0073\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0069\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0070\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0061\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0066\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0062\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0057\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0056\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0050\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0012 - val_loss: 0.0059A: 0s - loss: \n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0012 - val_loss: 0.0050\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0048oss: 0.0\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0047\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0051\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0044\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.8956e-04 - val_loss: 0.0044\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.7885e-04 - val_loss: 0.00414232e-0 - ETA: 0s - loss: 9.2\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.0093e-04 - val_loss: 0.0040\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.9108e-04 - val_loss: 0.0039\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.7205e-04 - val_loss: 0.0040\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.0271e-04 - val_loss: 0.0038\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.1919e-04 - val_loss: 0.0037\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.4015e-04 - val_loss: 0.0037oss:  - ETA: 0s - loss:\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.4011e-04 - val_loss: 0.0037\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 8.6976e-04 - val_loss: 0.0036\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.1531e-04 - val_loss: 0.0043\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 4s 4ms/sample - loss: 8.6040e-04 - val_loss: 0.0040\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.1524e-04 - val_loss: 0.0038\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.4047e-04 - val_loss: 0.0035\n",
            "Epoch 43/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 7.9347e-04 - val_loss: 0.0037\n",
            "Epoch 44/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.6245e-04 - val_loss: 0.0037\n",
            "Epoch 45/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.1485e-04 - val_loss: 0.0034-0\n",
            "Epoch 46/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.6573e-04 - val_loss: 0.0034\n",
            "Epoch 47/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.0399e-04 - val_loss: 0.0038\n",
            "Epoch 48/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 7.9113e-04 - val_loss: 0.0034\n",
            "Epoch 49/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 7.9975e-04 - val_loss: 0.0033\n",
            "Epoch 50/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 7.5613e-04 - val_loss: 0.0039\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 28s 36ms/sample - loss: 0.0368 - val_loss: 0.0397\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0087 - val_loss: 0.0140\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0031 - val_loss: 0.0096\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0025 - val_loss: 0.0084\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0021 - val_loss: 0.0082\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0020 - val_loss: 0.0085\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0019 - val_loss: 0.0085 ETA: 0s - loss: 0.00 - ETA: 0s - loss\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0087\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0019 - val_loss: 0.0091\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0018 - val_loss: 0.0087\n",
            "Epoch 00010: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 30s 38ms/sample - loss: 0.1299 - val_loss: 0.0479\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0166 - val_loss: 0.0238\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0065 - val_loss: 0.0111\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0026 - val_loss: 0.0107\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0023 - val_loss: 0.0102\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0022 - val_loss: 0.0100\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0022 - val_loss: 0.0097\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0022 - val_loss: 0.0098\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0021 - val_loss: 0.0096\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0021 - val_loss: 0.0096\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0021 - val_loss: 0.0096\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0020 - val_loss: 0.0093\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0020 - val_loss: 0.0093\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0019 - val_loss: 0.0091\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0020 - val_loss: 0.0094\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0092\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0019 - val_loss: 0.0090\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0018 - val_loss: 0.0091\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0018 - val_loss: 0.0087: 0s - loss: 0.001\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0020 - val_loss: 0.0090\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0089\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0018 - val_loss: 0.0087\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0089\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0085\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0017 - val_loss: 0.0089- loss: 0.0\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0086\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0083\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0082 ETA: 0s - loss: 0.001 - ETA: 0s - loss:  - ETA: 0s - loss: 0.0\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0080\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0017 - val_loss: 0.0078\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0079\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0016 - val_loss: 0.0078\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0016 - val_loss: 0.0076\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0015 - val_loss: 0.0075\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0078\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0015 - val_loss: 0.0073\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0015 - val_loss: 0.0073\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0073\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0014 - val_loss: 0.0070\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0068\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0074\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0015 - val_loss: 0.0067\n",
            "Epoch 43/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0067\n",
            "Epoch 44/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0066\n",
            "Epoch 45/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0066\n",
            "Epoch 46/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0068\n",
            "Epoch 47/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0013 - val_loss: 0.0062\n",
            "Epoch 48/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0061\n",
            "Epoch 49/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0061\n",
            "Epoch 50/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0059\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 39s 50ms/sample - loss: 0.0400 - val_loss: 0.0177\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0037 - val_loss: 0.0103\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0022 - val_loss: 0.0083\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0019 - val_loss: 0.0080\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0018 - val_loss: 0.0080\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0017 - val_loss: 0.0081\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0017 - val_loss: 0.0080\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0017 - val_loss: 0.0080\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0017 - val_loss: 0.0081\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0017 - val_loss: 0.0081\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0016 - val_loss: 0.0080\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0016 - val_loss: 0.0080\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0017 - val_loss: 0.0082\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0082\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - ETA: 0s - loss: 0.001 - 1s 2ms/sample - loss: 0.0017 - val_loss: 0.0082\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0016 - val_loss: 0.0079\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0079\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 969us/sample - loss: 0.0015 - val_loss: 0.0083\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 1s 923us/sample - loss: 0.0016 - val_loss: 0.0082\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0081\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 1s 897us/sample - loss: 0.0015 - val_loss: 0.0082\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0079\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 1s 954us/sample - loss: 0.0014 - val_loss: 0.0078\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0081\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0015 - val_loss: 0.0083 - loss: 0.001\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0078\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0079\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0079\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0072\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0073TA: 0s - loss: 0.00 - ETA: 0s - loss: 0.0 - ETA: 0s - loss: 0.001\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0014 - val_loss: 0.0078\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0077\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0072\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0071\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0075\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 1s 913us/sample - loss: 0.0012 - val_loss: 0.0071\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0013 - val_loss: 0.0081\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 1s 905us/sample - loss: 0.0012 - val_loss: 0.0066\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0068\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 1s 862us/sample - loss: 0.0012 - val_loss: 0.0070\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0011 - val_loss: 0.0069\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 1s 900us/sample - loss: 0.0012 - val_loss: 0.0069\n",
            "Epoch 43/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0012 - val_loss: 0.0068\n",
            "Epoch 00043: early stopping\n",
            "Best_hyper_parameters: \n",
            " {'model': [30], 'optimizer': 'Adam', 'learning_rate': 0.01, 'batch_size': 16, 'best_avg_rmse': 14.163673053416879}\n",
            "all_avg_rmse: \n",
            " [[[15.51865645 15.40961152 15.11372008]\n",
            "  [14.65261527 14.31553008 14.16367305]\n",
            "  [14.26631732 14.46145197 14.9144969 ]]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model': [30],\n",
              " 'optimizer': 'Adam',\n",
              " 'learning_rate': 0.01,\n",
              " 'batch_size': 16,\n",
              " 'best_avg_rmse': 14.163673053416879}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "layers = [30]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs =  50\n",
        "num_replicates = 3\n",
        "test_split = 0.2 \n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "N30_best_hyper_parameters = hyper_parameter_tuning(layers, train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "N30_best_hyper_parameters\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bAJ26vPr9JkW"
      },
      "source": [
        "### **Case III: Tuning parameter of 50N-singlelayer-LSTM**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "wqPnDy0bIvc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 34s 43ms/sample - loss: 0.2471 - val_loss: 0.0120\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0032 - val_loss: 0.0054 - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.0\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0032 - val_loss: 0.0083\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0019 - val_loss: 0.0042.001\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0023 - val_loss: 0.0044\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0027 - val_loss: 0.0039\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0016 - val_loss: 0.0063\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0020 - val_loss: 0.0057\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0024 - val_loss: 0.0102\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0014 - val_loss: 0.0042\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0030 - val_loss: 0.0061\n",
            "Epoch 00011: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 46s 59ms/sample - loss: 0.1643 - val_loss: 0.0046\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0052 - val_loss: 0.0048\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0040 - val_loss: 0.0042\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0024 - val_loss: 0.0053\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0064 - val_loss: 0.0096\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0039 - val_loss: 0.0065\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0035 - val_loss: 0.0041\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0032 - val_loss: 0.0049\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0053 - val_loss: 0.0220\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0038 - val_loss: 0.0069\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0034 - val_loss: 0.0048\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0027 - val_loss: 0.0079\n",
            "Epoch 00012: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 28s 36ms/sample - loss: 0.2134 - val_loss: 0.0508\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0130 - val_loss: 0.0319\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0134 - val_loss: 0.0346\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0088 - val_loss: 0.0118\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0100 - val_loss: 0.0175\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0081 - val_loss: 0.0070\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0055 - val_loss: 0.0131\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0094 - val_loss: 0.0359\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0125 - val_loss: 0.0060\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0133 - val_loss: 0.0247\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0096 - val_loss: 0.0075\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0113 - val_loss: 0.0044\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0066 - val_loss: 0.0142\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0133 - val_loss: 0.0041\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0060 - val_loss: 0.0116\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0051 - val_loss: 0.0090\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0076 - val_loss: 0.0109\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0308 - val_loss: 0.0132\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0110 - val_loss: 0.0242\n",
            "Epoch 00019: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 29s 37ms/sample - loss: 0.3183 - val_loss: 0.0173\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0027 - val_loss: 0.0061\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0046 - val_loss: 0.0040\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0019 - val_loss: 0.0039\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0023 - val_loss: 0.0037\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0064 - val_loss: 0.0150\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - ETA: 0s - loss: 0.0031 - 1s 2ms/sample - loss: 0.0030 - val_loss: 0.0037\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0026 - val_loss: 0.0061\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0039 - val_loss: 0.0064\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0055 - val_loss: 0.0058\n",
            "Epoch 00010: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 28s 35ms/sample - loss: 0.3425 - val_loss: 0.0067\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0026 - val_loss: 0.0063\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0085 - val_loss: 0.0040\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0030 - val_loss: 0.00350s - loss: 0. - ETA: 0s - loss: 0. - ETA: 0s - loss: 0.\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0031 - val_loss: 0.0040\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0021 - val_loss: 0.0070\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0028 - val_loss: 0.0090\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0023 - val_loss: 0.0119\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0027 - val_loss: 0.0046\n",
            "Epoch 00009: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 30s 37ms/sample - loss: 0.5023 - val_loss: 0.0145\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0034 - val_loss: 0.0064\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0043 - val_loss: 0.0039\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0116 - val_loss: 0.0058\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0036 - val_loss: 0.0071\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0027 - val_loss: 0.0036\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0025 - val_loss: 0.0092loss: 0.002\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0039 - val_loss: 0.0044\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0042 - val_loss: 0.0044\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0056 - val_loss: 0.0154\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0055 - val_loss: 0.0076\n",
            "Epoch 00011: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 28s 35ms/sample - loss: 0.3658 - val_loss: 0.0260\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0068 - val_loss: 0.0062\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 977us/sample - loss: 0.0045 - val_loss: 0.0053\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 980us/sample - loss: 0.0036 - val_loss: 0.0101\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0051 - val_loss: 0.0184\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0065 - val_loss: 0.0089\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0038 - val_loss: 0.0047\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0029 - val_loss: 0.0098 ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.00\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0031 - val_loss: 0.0046\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0030 - val_loss: 0.0059\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0052 - val_loss: 0.1025\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0175 - val_loss: 0.0124\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0033 - val_loss: 0.0044\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0028 - val_loss: 0.0076: 0s - loss: 0\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0045 - val_loss: 0.0110\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0031 - val_loss: 0.0053\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0048 - val_loss: 0.0065\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0066 - val_loss: 0.0053\n",
            "Epoch 00018: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 27s 34ms/sample - loss: 0.7260 - val_loss: 0.0356\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0069 - val_loss: 0.0105\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 963us/sample - loss: 0.0019 - val_loss: 0.0070\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0044 - val_loss: 0.0093\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0053 - val_loss: 0.0044\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 947us/sample - loss: 0.0032 - val_loss: 0.0104\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 918us/sample - loss: 0.0033 - val_loss: 0.0060\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0025 - val_loss: 0.0038\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0039 - val_loss: 0.0072\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 933us/sample - loss: 0.0043 - val_loss: 0.0047\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 958us/sample - loss: 0.0019 - val_loss: 0.0052\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0031 - val_loss: 0.0172\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 977us/sample - loss: 0.0025 - val_loss: 0.0035\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 991us/sample - loss: 0.0024 - val_loss: 0.0042\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 951us/sample - loss: 0.0034 - val_loss: 0.0075\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0027 - val_loss: 0.0048\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 1s 989us/sample - loss: 0.0027 - val_loss: 0.0034\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 945us/sample - loss: 0.0042 - val_loss: 0.0036\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0035 - val_loss: 0.0036\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 946us/sample - loss: 0.0027 - val_loss: 0.0045\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0022 - val_loss: 0.0080\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 1s 999us/sample - loss: 0.0060 - val_loss: 0.0136\n",
            "Epoch 00022: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 28s 35ms/sample - loss: 0.6898 - val_loss: 0.0111\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0047 - val_loss: 0.0058\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0024 - val_loss: 0.0145\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0042 - val_loss: 0.0065 ETA: 0s - loss: 0\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0037 - val_loss: 0.0061\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0024 - val_loss: 0.0044\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0045\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0027 - val_loss: 0.0040\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0018 - val_loss: 0.0039\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 998us/sample - loss: 0.0040 - val_loss: 0.0228\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0041 - val_loss: 0.0058\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0041 - val_loss: 0.0062\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0075 - val_loss: 0.0181\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 1ms/sample - loss: 0.0049 - val_loss: 0.0066\n",
            "Epoch 00014: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 30s 38ms/sample - loss: 0.0100 - val_loss: 0.0090\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0019 - val_loss: 0.0053\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0016 - val_loss: 0.0084\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0014 - val_loss: 0.0046ETA: 0s - - ETA: 0s - loss: 0.\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0015 - val_loss: 0.0037\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0012 - val_loss: 0.0036TA - ETA: 2s - loss: 0.0 - ETA: 1s - loss: 0.0 -\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0012 - val_loss: 0.0039\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0012 - val_loss: 0.0034\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0011 - val_loss: 0.0037 ETA: 0s - l\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0010 - val_loss: 0.0035\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0011 - val_loss: 0.0054\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0011 - val_loss: 0.0037\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0010 - val_loss: 0.00381s - loss - ETA: 0s - loss: 0.001 - ETA: 0s - loss: 0. - ETA: 0s - loss\n",
            "Epoch 00013: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 50s 64ms/sample - loss: 0.0098 - val_loss: 0.0068\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0020 - val_loss: 0.0068\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0018 - val_loss: 0.0048\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0015 - val_loss: 0.0045: 3s - l\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0014 - val_loss: 0.0040\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0014 - val_loss: 0.0036\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0010 - val_loss: 0.0038\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0011 - val_loss: 0.0036\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0013 - val_loss: 0.0037\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0011 - val_loss: 0.0048 ETA: 1 - ETA: 0s - \n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0010 - val_loss: 0.0035\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0011 - val_loss: 0.0032s - loss: 0.0\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0012 - val_loss: 0.0034\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0010 - val_loss: 0.0034479e- - ETA: 3 \n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0011 - val_loss: 0.0036\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0011 - val_loss: 0.0034 - loss:\n",
            "Epoch 00017: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 41s 52ms/sample - loss: 0.0101 - val_loss: 0.0101: 5s - los\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0020 - val_loss: 0.0062\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 7s 8ms/sample - loss: 0.0018 - val_loss: 0.0061\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0016 - val_loss: 0.0054 0.001 - ET\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0015 - val_loss: 0.0046\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0014 - val_loss: 0.0036 ETA: 0s - loss: 0.\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0013 - val_loss: 0.0042381 - E\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0012 - val_loss: 0.0046 - ETA: 3s - loss: 0. - ETA: 3s - loss: 0.0 - ETA: 1s - loss: - ETA: 1s - - ETA: 0s - loss: \n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0010 - val_loss: 0.0034- ETA:  - ETA: 0s - loss: 0. - ETA: 0s - los\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 9.7296e-04 - val_loss: 0.0033\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0013 - val_loss: 0.0038 ETA: 2s - l - ETA: 1 - ETA: 0s - l\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0010 - val_loss: 0.0041042 - ETA: 3s - lo - \n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 9.9899e-04 - val_loss: 0.0032- ETA: 2s - los\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 9.3925e-04 - val_loss: 0.0037\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0011 - val_loss: 0.0040A: 1s - loss - ETA: 1s - los - ETA: 0s\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0010 - val_loss: 0.0036\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 9.9052e-04 - val_loss: 0.0036- ETA: 1s - - ETA: 0s - loss: 9.91\n",
            "Epoch 00019: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 37s 47ms/sample - loss: 0.0095 - val_loss: 0.0093\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0021 - val_loss: 0.0067\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0014 - val_loss: 0.0061\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0013 - val_loss: 0.0058 ETA\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0064\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0014 - val_loss: 0.0048\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0041\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0010 - val_loss: 0.0034\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0012 - val_loss: 0.0063\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0037\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 9.7720e-04 - val_loss: 0.0036\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0052\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0013 - val_loss: 0.0045\n",
            "Epoch 00014: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 37s 47ms/sample - loss: 0.0072 - val_loss: 0.0076\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0019 - val_loss: 0.0071\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0016 - val_loss: 0.0059s:\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0014 - val_loss: 0.0047A: 1s - loss: 0 - ETA: 1s - loss: 0. - ETA: 1s - loss: 0 - ETA: 1s\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0048\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0012 - val_loss: 0.0034\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0010 - val_loss: 0.0043\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 9.7604e-04 - val_loss: 0.0055oss: 9.4893\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 9.2072e-04 - val_loss: 0.0033\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0035\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0013 - val_loss: 0.0049\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.4277e-04 - val_loss: 0.0039-0\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.1886e-04 - val_loss: 0.0032\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 9.8387e-04 - val_loss: 0.0034\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 9.0521e-04 - val_loss: 0.0041 - loss:  - ETA: 0s - loss: 9.2737e\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0010 - val_loss: 0.0055\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0014 - val_loss: 0.0037\n",
            "Epoch 00019: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 37s 48ms/sample - loss: 0.0083 - val_loss: 0.0081\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0076A: 0s - loss: 0.\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0022 - val_loss: 0.0084\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0063\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0014 - val_loss: 0.0043\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0015 - val_loss: 0.0048\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0037\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0037\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0037\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.1330e-04 - val_loss: 0.0036\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.0470e-04 - val_loss: 0.0045 0s - loss: 9.0129e-\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.2204e-04 - val_loss: 0.0038\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.3766e-04 - val_loss: 0.0044\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 9.8214e-04 - val_loss: 0.0033\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.8447e-04 - val_loss: 0.0035\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0035A: 0s - loss: 0.0\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 9.4065e-04 - val_loss: 0.0040 - ETA: 0s - loss:\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.2000e-04 - val_loss: 0.0046\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 9.9722e-04 - val_loss: 0.0038- - ETA: 0s - lo\n",
            "Epoch 00022: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 42s 53ms/sample - loss: 0.0251 - val_loss: 0.0129\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0019 - val_loss: 0.0085oss: 0.00 - ETA: 0s - loss:\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0072\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0078\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0076\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0071A: 0s - \n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0073\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0079\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0051\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0050TA: 0s - loss:\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0048\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0011 - val_loss: 0.0046\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0049\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0043- ETA: 0s - loss: 0.001 - ETA: 0s - loss: 9.1\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0010 - val_loss: 0.0037\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 9.2227e-04 - val_loss: 0.0039\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0035\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 8.7590e-04 - val_loss: 0.0034\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.8510e-04 - val_loss: 0.0041\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 9.8119e-04 - val_loss: 0.0033\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.8255e-04 - val_loss: 0.0033\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.0307e-04 - val_loss: 0.0036\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 8.3425e-04 - val_loss: 0.0035\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.6222e-04 - val_loss: 0.0035\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.3575e-04 - val_loss: 0.0042\n",
            "Epoch 00025: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 36s 46ms/sample - loss: 0.0408 - val_loss: 0.0090\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0019 - val_loss: 0.0072\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0070\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0016 - val_loss: 0.0071\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0071\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0066\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0067\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0070\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0053\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0052\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0057\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0012 - val_loss: 0.0050\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0058\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0053\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0011 - val_loss: 0.0054\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0045\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.3138e-04 - val_loss: 0.0040\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 9.9273e-04 - val_loss: 0.0040\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.6375e-04 - val_loss: 0.0040\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0048\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0040\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 9.1307e-04 - val_loss: 0.0036\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.7947e-04 - val_loss: 0.0041\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.2638e-04 - val_loss: 0.0039\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 7.8381e-04 - val_loss: 0.0036\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0037\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.6068e-04 - val_loss: 0.0035\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 8.7050e-04 - val_loss: 0.0035\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 7.7724e-04 - val_loss: 0.0034\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.3943e-04 - val_loss: 0.0036\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 7.9839e-04 - val_loss: 0.0038\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.5016e-04 - val_loss: 0.0036\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.7271e-04 - val_loss: 0.0042\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 9.8465e-04 - val_loss: 0.0041\n",
            "Epoch 00034: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 40s 50ms/sample - loss: 0.0196 - val_loss: 0.0159\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0023 - val_loss: 0.0093\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0018 - val_loss: 0.0073\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0087\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0016 - val_loss: 0.0066\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.00610s - loss: 0.0\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0056 ETA\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0057\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0053\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0013 - val_loss: 0.0065\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0046\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0038\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.9952e-04 - val_loss: 0.0037\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.7088e-04 - val_loss: 0.0036\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.1752e-04 - val_loss: 0.0037\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 7.9568e-04 - val_loss: 0.0039- ETA: 1s - loss: 8.8 - ETA: 0s - loss: 7.7356e-0 - ETA: 0s - loss: \n",
            "Epoch 18/50\n",
            "787/787 [==============================] - ETA: 0s - loss: 0.0010    - 2s 2ms/sample - loss: 9.8809e-04 - val_loss: 0.0044\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.5160e-04 - val_loss: 0.0035-0\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.8813e-04 - val_loss: 0.0035\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.0625e-04 - val_loss: 0.0034\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.6103e-04 - val_loss: 0.0035\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.1527e-04 - val_loss: 0.0035\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.7211e-04 - val_loss: 0.0034\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.0583e-04 - val_loss: 0.0033\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.8167e-04 - val_loss: 0.0041\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0037\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.7468e-04 - val_loss: 0.0034\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.0918e-04 - val_loss: 0.0037\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.5900e-04 - val_loss: 0.0036\n",
            "Epoch 00030: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 43s 54ms/sample - loss: 0.0255 - val_loss: 0.0099\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0025 - val_loss: 0.0092\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0021 - val_loss: 0.0091 ETA: 1s - loss: 0.0 - ETA: 0s\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0021 - val_loss: 0.0086 ETA: 2s\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0020 - val_loss: 0.0102\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0019 - val_loss: 0.0082\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0019 - val_loss: 0.0087 1s - loss: 0.\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0018 - val_loss: 0.0079\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0017 - val_loss: 0.0072\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0016 - val_loss: 0.0077\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0015 - val_loss: 0.0069\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0015 - val_loss: 0.0059\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0014 - val_loss: 0.0056\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0014 - val_loss: 0.0057s - loss: 0.00 - ETA: 0s - loss:\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0013 - val_loss: 0.0053s - loss: 0.0 \n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0012 - val_loss: 0.0059\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0012 - val_loss: 0.00500.\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0012 - val_loss: 0.0042\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0012 - val_loss: 0.0043\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0011 - val_loss: 0.0041\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 9.8563e-04 - val_loss: 0.0045A: 1s\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0011 - val_loss: 0.0041\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 9.4181e-04 - val_loss: 0.0037\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 9.1314e-04 - val_loss: 0.0037\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 8.6506e-04 - val_loss: 0.0035\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 8.6606e-04 - val_loss: 0.00332s - loss: 7.99 - ETA: 2s - l - ETA: 1s - l - ETA: 0s - loss: 8\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 9.1524e-04 - val_loss: 0.0035TA: 1s - loss: 8.82 - ETA: 0s - loss:\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 8.8541e-04 - val_loss: 0.0034TA: 0s - loss: \n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 8.4328e-04 - val_loss: 0.0034\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 8.1101e-04 - val_loss: 0.0038\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 8.9332e-04 - val_loss: 0.0034: 2s - - ETA: 1s -\n",
            "Epoch 00031: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 42s 54ms/sample - loss: 0.0181 - val_loss: 0.0083\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0022 - val_loss: 0.0088\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0021 - val_loss: 0.0081\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0018 - val_loss: 0.0082oss: 0.001\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0018 - val_loss: 0.0073\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0018 - val_loss: 0.0074\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0018 - val_loss: 0.0068\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0017 - val_loss: 0.0064\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0016 - val_loss: 0.0069\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0015 - val_loss: 0.0060 - loss: 0\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0014 - val_loss: 0.0064TA: 0\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0013 - val_loss: 0.0067\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0014 - val_loss: 0.0051TA: 0s - los\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0013 - val_loss: 0.0052\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0012 - val_loss: 0.0048\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0012 - val_loss: 0.0046ss: 0.0\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0011 - val_loss: 0.0040\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0011 - val_loss: 0.0041s: 0.\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0012 - val_loss: 0.0040\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 9.8443e-04 - val_loss: 0.00449.7915\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 9.1186e-04 - val_loss: 0.0040: 2s - - \n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 7s 8ms/sample - loss: 9.3415e-04 - val_loss: 0.0042s - loss: 9.3194e\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 9.9207e-04 - val_loss: 0.0040TA: 0s - loss: \n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 8.7849e-04 - val_loss: 0.0035\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 8.8566e-04 - val_loss: 0.0035\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 9.1251e-04 - val_loss: 0.0034: 0s - loss: \n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 9.2616e-04 - val_loss: 0.0036\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 8.3807e-04 - val_loss: 0.0034\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 8.2536e-04 - val_loss: 0.0034A: 2s - los - ETA: 1s\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 8.5373e-04 - val_loss: 0.0036\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 7s 8ms/sample - loss: 7.8785e-04 - val_loss: 0.0032 loss: 7.8\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 8.2832e-04 - val_loss: 0.0032\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 8.4275e-04 - val_loss: 0.0033\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 8.3410e-04 - val_loss: 0.0035\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 7.9912e-04 - val_loss: 0.0032- - ETA: 3s - loss: - ETA: \n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 8.4822e-04 - val_loss: 0.0041 2s - loss:  - ETA: 2s - loss: 9. -\n",
            "Epoch 00036: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 44s 56ms/sample - loss: 0.0128 - val_loss: 0.0076\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0020 - val_loss: 0.0083\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0019 - val_loss: 0.0096 ETA: 3s - loss - ETA: 2s -  - ETA: 1s - loss: 0. \n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0018 - val_loss: 0.0090\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0019 - val_loss: 0.0080\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 7s 8ms/sample - loss: 0.0017 - val_loss: 0.00752s - - ETA: 2s - l -\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 7s 8ms/sample - loss: 0.0018 - val_loss: 0.0072\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0016 - val_loss: 0.0081\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0015 - val_loss: 0.0090: 1s - loss: 0\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0014 - val_loss: 0.0066\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0015 - val_loss: 0.0065 \n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0014 - val_loss: 0.0059\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0013 - val_loss: 0.00590s - loss: 0.0\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0013 - val_loss: 0.0064\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0013 - val_loss: 0.0051\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 7s 8ms/sample - loss: 0.0012 - val_loss: 0.0048\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0012 - val_loss: 0.0044 4s - loss - ETA - ETA: 3s - ETA: 2 - ETA:\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0011 - val_loss: 0.0046\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0011 - val_loss: 0.0048\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 9.9449e-04 - val_loss: 0.0038\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 9.4983e-04 - val_loss: 0.0045\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 7s 8ms/sample - loss: 0.0010 - val_loss: 0.0040- ETA: 0s - loss: 0.001\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 9.3771e-04 - val_loss: 0.0047\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 9.2014e-04 - val_loss: 0.0042- loss:\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 9.3060e-04 - val_loss: 0.0039- ETA: 1s \n",
            "Epoch 00025: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 42s 54ms/sample - loss: 0.0168 - val_loss: 0.0105\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0019 - val_loss: 0.0094\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0018 - val_loss: 0.0088\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0019 - val_loss: 0.0098 ET\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0018 - val_loss: 0.0085\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0086\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0084\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0081: 0s - loss:\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0081\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0079\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0074\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0016 - val_loss: 0.0072\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0069\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0066\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0073\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0061\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0014 - val_loss: 0.0065TA: 0s - l\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0064\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0070\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0054\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0055\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0056\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0011 - val_loss: 0.0045 - ETA: 0s - loss: 0.00\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0051\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 4s 4ms/sample - loss: 0.0010 - val_loss: 0.0044\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0043: 0s - loss: 0.00\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0045\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0041471 - ETA: 1s - los - ETA: 0s - loss: 9.0 - ETA: 0s - loss: 9.9474e\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0040\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.6915e-04 - val_loss: 0.0041s - loss: 9.738 - ETA: 0s - loss: 9\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.0542e-04 - val_loss: 0.0041\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 4s 4ms/sample - loss: 8.5462e-04 - val_loss: 0.0043\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.1393e-04 - val_loss: 0.0037\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.5364e-04 - val_loss: 0.0036\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.1853e-04 - val_loss: 0.0035\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.8889e-04 - val_loss: 0.0035\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.8692e-04 - val_loss: 0.0035s -\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.4533e-04 - val_loss: 0.0036\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.4149e-04 - val_loss: 0.0035\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.7287e-04 - val_loss: 0.0041 ETA: 0s \n",
            "Epoch 00041: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 41s 52ms/sample - loss: 0.0265 - val_loss: 0.0103\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 4s 4ms/sample - loss: 0.0023 - val_loss: 0.0091\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0021 - val_loss: 0.0090\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0022 - val_loss: 0.0083\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0020 - val_loss: 0.0088\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0019 - val_loss: 0.0084 ETA: 0s - \n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0081\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0081 ETA: 1s - loss - ETA: 1s - loss:  - ETA: 0s - loss: 0. - ETA: 0s \n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0080\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0081\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0016 - val_loss: 0.0076\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0075ss\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0073\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0068- loss: 0.0\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0015 - val_loss: 0.0069\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0015 - val_loss: 0.0066\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0062\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0060\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0062\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0058 0.001\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0062\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0054\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0053\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0052\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0012 - val_loss: 0.0050\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0011 - val_loss: 0.0048\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0012 - val_loss: 0.0048 los - ETA: 0s - loss: 0.\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0047\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0045\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.8455e-04 - val_loss: 0.0043\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.8039e-04 - val_loss: 0.0043\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0047\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.7860e-04 - val_loss: 0.0042\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 9.7027e-04 - val_loss: 0.0042-0 - ETA:\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.1924e-04 - val_loss: 0.0041\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.0038e-04 - val_loss: 0.0039\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.7202e-04 - val_loss: 0.0038\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.4363e-04 - val_loss: 0.0040\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.8859e-04 - val_loss: 0.0038\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.5084e-04 - val_loss: 0.0036\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 8.4715e-04 - val_loss: 0.0040\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.4697e-04 - val_loss: 0.0036\n",
            "Epoch 43/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 7.6845e-04 - val_loss: 0.0038\n",
            "Epoch 44/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.2434e-04 - val_loss: 0.0035\n",
            "Epoch 45/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 8.2468e-04 - val_loss: 0.0035\n",
            "Epoch 46/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.7556e-04 - val_loss: 0.0036\n",
            "Epoch 47/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 7.9087e-04 - val_loss: 0.0035\n",
            "Epoch 48/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 7.8203e-04 - val_loss: 0.0039\n",
            "Epoch 49/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.0418e-04 - val_loss: 0.0036\n",
            "Epoch 50/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.3600e-04 - val_loss: 0.0036\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 42s 53ms/sample - loss: 0.0305 - val_loss: 0.0110\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0025 - val_loss: 0.0091\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0021 - val_loss: 0.0100\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0021 - val_loss: 0.0092\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0021 - val_loss: 0.0086\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0090\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0098\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0018 - val_loss: 0.0085\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0078\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0017 - val_loss: 0.0074\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0017 - val_loss: 0.0071\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0017 - val_loss: 0.0075\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0075\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0064 ETA: 1s - loss: 0.001 \n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0063loss:\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0063 - loss: 0.00 - ETA: 0s - loss: 0.\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0062\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0057\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0055\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0056\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0012 - val_loss: 0.00490s - loss: \n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0012 - val_loss: 0.0048\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0047\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0049\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 9.7004e-04 - val_loss: 0.0041\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.8501e-04 - val_loss: 0.0038\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.7771e-04 - val_loss: 0.0039\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.4775e-04 - val_loss: 0.0040\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0010 - val_loss: 0.0036\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.2137e-04 - val_loss: 0.0037\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.9023e-04 - val_loss: 0.0039\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.1354e-04 - val_loss: 0.0038\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 8.5255e-04 - val_loss: 0.0037\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 8.9539e-04 - val_loss: 0.0034\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.2639e-04 - val_loss: 0.0038\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 8.6376e-04 - val_loss: 0.0036\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 8.7966e-04 - val_loss: 0.0034\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 7.9494e-04 - val_loss: 0.0036\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.1367e-04 - val_loss: 0.0036\n",
            "Epoch 00042: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 43s 54ms/sample - loss: 0.0605 - val_loss: 0.0236\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0055 - val_loss: 0.0112\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0025 - val_loss: 0.0107\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0022 - val_loss: 0.0100\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0021 - val_loss: 0.0096\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0022 - val_loss: 0.0097\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0020 - val_loss: 0.0093\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0020 - val_loss: 0.0095\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0093\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0020 - val_loss: 0.0096\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0019 - val_loss: 0.0092\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0019 - val_loss: 0.0092\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0019 - val_loss: 0.0098.001\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0018 - val_loss: 0.0093\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0018 - val_loss: 0.0086\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0018 - val_loss: 0.0083\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0018 - val_loss: 0.0079\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0018 - val_loss: 0.0083\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0079\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.00810s - loss: 0. - ETA: 0s - loss: 0\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0080A: 0s - loss: 0.0\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0077\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0017 - val_loss: 0.0076s: \n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0073\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0073\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0072\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0074\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0071\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0065\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0066\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0068\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0065\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0071\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0062\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0066\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0056\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0059\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0012 - val_loss: 0.0061\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0012 - val_loss: 0.0052\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0050 ETA: 0s - loss: 0\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0012 - val_loss: 0.0053oss: 0\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0053A: 0s - loss:\n",
            "Epoch 43/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0050\n",
            "Epoch 44/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0046\n",
            "Epoch 45/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0049\n",
            "Epoch 46/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0044 - lo\n",
            "Epoch 47/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0052\n",
            "Epoch 48/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 49/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 50/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0010 - val_loss: 0.0043\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 43s 54ms/sample - loss: 0.0871 - val_loss: 0.0469\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0118 - val_loss: 0.0193\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0038 - val_loss: 0.0170\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - ETA: 0s - loss: 0.002 - 2s 2ms/sample - loss: 0.0028 - val_loss: 0.0134\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0025 - val_loss: 0.0120\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0023 - val_loss: 0.0105\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0023 - val_loss: 0.0099\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0022 - val_loss: 0.0096ETA: - ETA: 0s - loss: 0.002\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0021 - val_loss: 0.0095\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0020 - val_loss: 0.0098\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0099TA: \n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0020 - val_loss: 0.0095\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0094\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0093\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0092\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0092\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0094\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0093\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0088\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0088\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0087\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0084\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0086\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0076\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0081\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0075\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0080\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0074\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0077 los\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0078\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0072\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0071\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0068\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0070\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0065\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0065\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0062\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0066\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0059\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0012 - val_loss: 0.0059\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0012 - val_loss: 0.0057\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0012 - val_loss: 0.0057\n",
            "Epoch 43/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0056\n",
            "Epoch 44/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0012 - val_loss: 0.0052\n",
            "Epoch 45/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0055\n",
            "Epoch 46/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0051\n",
            "Epoch 47/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0051\n",
            "Epoch 48/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0050\n",
            "Epoch 49/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0061\n",
            "Epoch 50/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0051\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 43s 55ms/sample - loss: 0.0604 - val_loss: 0.0351\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0074 - val_loss: 0.0111\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0024 - val_loss: 0.0104\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0023 - val_loss: 0.0097\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0022 - val_loss: 0.0097\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0021 - val_loss: 0.0098\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0021 - val_loss: 0.0098\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0021 - val_loss: 0.0095\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0020 - val_loss: 0.0107\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0020 - val_loss: 0.0100\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0019 - val_loss: 0.0100: 0s - loss: 0.001 - ETA: 0s - loss: 0.\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0019 - val_loss: 0.0097\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0097 0s - loss: 0\n",
            "Epoch 00013: early stopping\n",
            "Best_hyper_parameters: \n",
            " {'model': [50], 'optimizer': 'Adam', 'learning_rate': 0.001, 'batch_size': 8, 'best_avg_rmse': 16.04985696460467}\n",
            "all_avg_rmse: \n",
            " [[[23.6449353  20.286676   20.14461466]\n",
            "  [18.41373591 17.50633595 16.89797045]\n",
            "  [16.41647332 16.04985696 16.18768118]]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model': [50],\n",
              " 'optimizer': 'Adam',\n",
              " 'learning_rate': 0.001,\n",
              " 'batch_size': 8,\n",
              " 'best_avg_rmse': 16.04985696460467}"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "layers = [50]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs = 50\n",
        "num_replicates = 3\n",
        "test_split = 0.2 \n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "N50_best_hyper_parameters = hyper_parameter_tuning(layers, train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "N50_best_hyper_parameters\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DPt8YYZ4Oc6-"
      },
      "source": [
        "### **Case IV: Tuning parameter of 100N-singlelayer-LSTM**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "W_gm5CWzOYh5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 62s 79ms/sample - loss: 1.0485 - val_loss: 0.0162\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0053 - val_loss: 0.0063\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0037 - val_loss: 0.0126\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0042 - val_loss: 0.0050\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0028 - val_loss: 0.0050\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0029 - val_loss: 0.0039\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0018 - val_loss: 0.0081\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0044 - val_loss: 0.0038\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0029 - val_loss: 0.0075\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0029 - val_loss: 0.0043\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0037 - val_loss: 0.0039\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0032 - val_loss: 0.0088\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 00013: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 61s 78ms/sample - loss: 0.6081 - val_loss: 0.0167\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0250 - val_loss: 0.0377\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0175 - val_loss: 0.0217\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0233 - val_loss: 0.1396\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0249 - val_loss: 0.0488\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0401 - val_loss: 0.0230\n",
            "Epoch 00006: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 62s 79ms/sample - loss: 1.2774 - val_loss: 0.0217TA: 52s  - ETA: 46s - loss: 2.60 - ETA: 45s  \n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 22s 29ms/sample - loss: 0.0120 - val_loss: 0.0180\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0256 - val_loss: 0.0208\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0476 - val_loss: 0.0196\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 0.0232 - val_loss: 0.0546\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 0.0215 - val_loss: 0.0243\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 0.0304 - val_loss: 0.1326\n",
            "Epoch 00007: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 52s 66ms/sample - loss: 2.4640 - val_loss: 0.0140\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0073 - val_loss: 0.0062\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0099 - val_loss: 0.0057\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0034 - val_loss: 0.0052\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0031 - val_loss: 0.0098\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0040 - val_loss: 0.0037\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0024 - val_loss: 0.0095\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0022 - val_loss: 0.0049\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0022 - val_loss: 0.0053\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0014 - val_loss: 0.0046\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0026 - val_loss: 0.0256\n",
            "Epoch 00011: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 54s 68ms/sample - loss: 1.2937 - val_loss: 0.0607\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0070 - val_loss: 0.0108\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0068 - val_loss: 0.0164\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0060 - val_loss: 0.0069\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0043 - val_loss: 0.0069\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0060 - val_loss: 0.0058\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0061 - val_loss: 0.0069\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0069 - val_loss: 0.0052\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0060 - val_loss: 0.0250\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0081 - val_loss: 0.0055\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0050 - val_loss: 0.0108\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0085 - val_loss: 0.0123\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0280 - val_loss: 0.0301\n",
            "Epoch 00013: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 57s 72ms/sample - loss: 0.9518 - val_loss: 0.0047\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0013 - val_loss: 0.0041\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0011 - val_loss: 0.0058\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0012 - val_loss: 0.0038\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 10s 12ms/sample - loss: 0.0012 - val_loss: 0.0036\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0016 - val_loss: 0.0040\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0012 - val_loss: 0.0041\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0010 - val_loss: 0.0037\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0011 - val_loss: 0.0037\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 9.1492e-04 - val_loss: 0.0037\n",
            "Epoch 00010: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 83s 105ms/sample - loss: 1.6077 - val_loss: 0.0796\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 7s 8ms/sample - loss: 0.0084 - val_loss: 0.0470\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0150 - val_loss: 0.0236\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0078 - val_loss: 0.0298\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0060 - val_loss: 0.0137\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0163 - val_loss: 0.0132\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0048 - val_loss: 0.0268\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0080 - val_loss: 0.0185\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0107 - val_loss: 0.0110\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0087 - val_loss: 0.0252\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0150 - val_loss: 0.0195\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0101 - val_loss: 0.0495\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0110 - val_loss: 0.0151\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0062 - val_loss: 0.0204\n",
            "Epoch 00014: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 60s 76ms/sample - loss: 2.1063 - val_loss: 0.0527\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0073 - val_loss: 0.0081\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0023 - val_loss: 0.0069\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0021 - val_loss: 0.0172\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0052 - val_loss: 0.0046\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 7s 8ms/sample - loss: 0.0015 - val_loss: 0.0093\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0035 - val_loss: 0.0052\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0018 - val_loss: 0.0073\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0017 - val_loss: 0.0054\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0059 - val_loss: 0.0065\n",
            "Epoch 00010: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 58s 73ms/sample - loss: 4.6018 - val_loss: 0.0542\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0299 - val_loss: 0.0366\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0308 - val_loss: 0.0812\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0131 - val_loss: 0.0190\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0220 - val_loss: 0.0332\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0114 - val_loss: 0.0606\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0099 - val_loss: 0.1208\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0143 - val_loss: 0.0358\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0222 - val_loss: 0.0147\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0150 - val_loss: 0.0142\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0132 - val_loss: 0.0313\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0123 - val_loss: 0.0536\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0059 - val_loss: 0.0105\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0063 - val_loss: 0.0102\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0149 - val_loss: 0.0094\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0081 - val_loss: 0.0142\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0106 - val_loss: 0.0112E\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0060 - val_loss: 0.0179\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0203 - val_loss: 0.0122\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0114 - val_loss: 0.0237\n",
            "Epoch 00020: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 74s 95ms/sample - loss: 0.0120 - val_loss: 0.0077\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 27s 35ms/sample - loss: 0.0018 - val_loss: 0.0071\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 30s 38ms/sample - loss: 0.0017 - val_loss: 0.0061\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 19s 25ms/sample - loss: 0.0015 - val_loss: 0.0080\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0012 - val_loss: 0.0040\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0011 - val_loss: 0.0054\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0011 - val_loss: 0.0060\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0012 - val_loss: 0.0035\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0012 - val_loss: 0.0040\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0011 - val_loss: 0.0036\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 19s 25ms/sample - loss: 0.0012 - val_loss: 0.0036\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0015 - val_loss: 0.0037\n",
            "Epoch 00013: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 66s 83ms/sample - loss: 0.0087 - val_loss: 0.0094\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 20s 26ms/sample - loss: 0.0026 - val_loss: 0.0069\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0016 - val_loss: 0.0042\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0015 - val_loss: 0.0039\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0011 - val_loss: 0.0041\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0013 - val_loss: 0.0056\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0012 - val_loss: 0.0034\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0012 - val_loss: 0.0041\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0012 - val_loss: 0.0043\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0012 - val_loss: 0.0048\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 9.8113e-04 - val_loss: 0.0039\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0012 - val_loss: 0.0035\n",
            "Epoch 00013: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 67s 86ms/sample - loss: 0.0108 - val_loss: 0.0078\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 0.0018 - val_loss: 0.0053\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 0.0019 - val_loss: 0.0053\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0018 - val_loss: 0.0038\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0015 - val_loss: 0.0042\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0011 - val_loss: 0.0034\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0013 - val_loss: 0.0047\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0014 - val_loss: 0.0037\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0011 - val_loss: 0.0036\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0012 - val_loss: 0.0036\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0012 - val_loss: 0.0034\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0013 - val_loss: 0.0034\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0013 - val_loss: 0.0037\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0011 - val_loss: 0.0033\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 9.9923e-04 - val_loss: 0.0037\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0010 - val_loss: 0.0036\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0010 - val_loss: 0.0039\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 9.9174e-04 - val_loss: 0.0044 - loss: 0.00\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 19s 25ms/sample - loss: 0.0012 - val_loss: 0.0041\n",
            "Epoch 00019: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 59s 75ms/sample - loss: 0.0122 - val_loss: 0.0097\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0022 - val_loss: 0.0068\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0018 - val_loss: 0.0060\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0015 - val_loss: 0.0047\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0014 - val_loss: 0.0053\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0012 - val_loss: 0.0041\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 10s 12ms/sample - loss: 0.0015 - val_loss: 0.0051\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 10s 13ms/sample - loss: 0.0011 - val_loss: 0.0041\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 10s 13ms/sample - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 10s 13ms/sample - loss: 0.0010 - val_loss: 0.0047\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0012 - val_loss: 0.0036\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.4124e-04 - val_loss: 0.0039\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 10s 13ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.3152e-04 - val_loss: 0.0038\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 00016: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 59s 75ms/sample - loss: 0.0114 - val_loss: 0.0087\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0019 - val_loss: 0.0085: 0s - loss: 0\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0018 - val_loss: 0.0057\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0015 - val_loss: 0.0050\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 10s 12ms/sample - loss: 0.0014 - val_loss: 0.0055\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 10s 12ms/sample - loss: 0.0012 - val_loss: 0.0058\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0012 - val_loss: 0.0048\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0011 - val_loss: 0.0034\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 9.5531e-04 - val_loss: 0.0042\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 9.6354e-04 - val_loss: 0.0038\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0011 - val_loss: 0.0035\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 9.6970e-04 - val_loss: 0.0037\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0010 - val_loss: 0.0035\n",
            "Epoch 00013: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 60s 77ms/sample - loss: 0.0094 - val_loss: 0.0067\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0020 - val_loss: 0.0059\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0024 - val_loss: 0.0068\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0013 - val_loss: 0.0049\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0016 - val_loss: 0.0047\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0012 - val_loss: 0.0035\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0012 - val_loss: 0.0034\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 9.4562e-04 - val_loss: 0.0045\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0011 - val_loss: 0.0046\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.8574e-04 - val_loss: 0.0042\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0011 - val_loss: 0.0034\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0012 - val_loss: 0.0034\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 9.9043e-04 - val_loss: 0.0051\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.0303e-04 - val_loss: 0.0050\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0011 - val_loss: 0.0040\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.5435e-04 - val_loss: 0.0033\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.4309e-04 - val_loss: 0.0036\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.4387e-04 - val_loss: 0.0037\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.4295e-04 - val_loss: 0.0033\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.8901e-04 - val_loss: 0.0034\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 10s 12ms/sample - loss: 0.0011 - val_loss: 0.0045\n",
            "Epoch 00022: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 56s 71ms/sample - loss: 0.0275 - val_loss: 0.0135\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0018 - val_loss: 0.0087\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0017 - val_loss: 0.0075\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0017 - val_loss: 0.0070\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0015 - val_loss: 0.0066\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0014 - val_loss: 0.0060\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0015 - val_loss: 0.0056\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0013 - val_loss: 0.0051\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0014 - val_loss: 0.0063\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0014 - val_loss: 0.0048\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0012 - val_loss: 0.0043\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0011 - val_loss: 0.0052\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0011 - val_loss: 0.0048\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0011 - val_loss: 0.0037\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0012 - val_loss: 0.0041\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 9.6903e-04 - val_loss: 0.0037\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 8.7187e-04 - val_loss: 0.0034\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 9.1988e-04 - val_loss: 0.0044\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 8.9907e-04 - val_loss: 0.0036\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 8.6374e-04 - val_loss: 0.0037\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 9.3582e-04 - val_loss: 0.0042\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 9.6825e-04 - val_loss: 0.0035\n",
            "Epoch 00023: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 56s 71ms/sample - loss: 0.0273 - val_loss: 0.0084\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0020 - val_loss: 0.0065\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0018 - val_loss: 0.0069\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0018 - val_loss: 0.0067\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0017 - val_loss: 0.0063\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0016 - val_loss: 0.0063\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0013 - val_loss: 0.0095\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0015 - val_loss: 0.0049\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0012 - val_loss: 0.0047\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0012 - val_loss: 0.0044\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0012 - val_loss: 0.0049\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0010 - val_loss: 0.0051\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0010 - val_loss: 0.0055\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0014 - val_loss: 0.0038\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 9.2832e-04 - val_loss: 0.0037\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 8.5280e-04 - val_loss: 0.0040\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0012 - val_loss: 0.0035\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 8.7089e-04 - val_loss: 0.0039\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 9.9684e-04 - val_loss: 0.0038\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 8.2424e-04 - val_loss: 0.0037\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 8.3608e-04 - val_loss: 0.0034\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 9.0311e-04 - val_loss: 0.0049\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 8.4120e-04 - val_loss: 0.0032\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0014 - val_loss: 0.0044\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 9.4511e-04 - val_loss: 0.0038\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 8.5495e-04 - val_loss: 0.0041\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 9.1336e-04 - val_loss: 0.0041\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 9.0138e-04 - val_loss: 0.0033\n",
            "Epoch 00031: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 59s 75ms/sample - loss: 0.0256 - val_loss: 0.0096\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0019 - val_loss: 0.0076\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0017 - val_loss: 0.0082\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0018 - val_loss: 0.0072\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0017 - val_loss: 0.0058\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0016 - val_loss: 0.0077\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0017 - val_loss: 0.0054\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0012 - val_loss: 0.0059\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0012 - val_loss: 0.0061\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0012 - val_loss: 0.0044\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0014 - val_loss: 0.0045\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0013 - val_loss: 0.0052\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0012 - val_loss: 0.0037\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0011 - val_loss: 0.0052\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0011 - val_loss: 0.0036\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 8.6273e-04 - val_loss: 0.0049\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0010 - val_loss: 0.0043\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 8.7798e-04 - val_loss: 0.0034\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 8.2113e-04 - val_loss: 0.0034\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 9.3575e-04 - val_loss: 0.0045\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 8.4509e-04 - val_loss: 0.0041\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 8.3978e-04 - val_loss: 0.0037\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 8.3719e-04 - val_loss: 0.0037\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 8.5023e-04 - val_loss: 0.0056\n",
            "Epoch 00027: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 71s 90ms/sample - loss: 0.0124 - val_loss: 0.0094\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 0.0024 - val_loss: 0.0104\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 19s 25ms/sample - loss: 0.0025 - val_loss: 0.0083\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 0.0021 - val_loss: 0.0082\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0018 - val_loss: 0.0074\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0016 - val_loss: 0.0068\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0016 - val_loss: 0.0074\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0014 - val_loss: 0.0062lo\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0014 - val_loss: 0.0056\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0014 - val_loss: 0.0057\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0014 - val_loss: 0.0048\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0012 - val_loss: 0.0056\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0011 - val_loss: 0.0041\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0011 - val_loss: 0.0048\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0010 - val_loss: 0.0040\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0011 - val_loss: 0.0036\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 9.9682e-04 - val_loss: 0.0043\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0011 - val_loss: 0.0035\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 9.6023e-04 - val_loss: 0.0035\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 9.0269e-04 - val_loss: 0.0043\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 9.0946e-04 - val_loss: 0.0033\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 8.8005e-04 - val_loss: 0.0033\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 8.7288e-04 - val_loss: 0.0037\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 9.6778e-04 - val_loss: 0.0035\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 9.0269e-04 - val_loss: 0.0033\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 9.1361e-04 - val_loss: 0.0034\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 8.4388e-04 - val_loss: 0.0034\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 8.5532e-04 - val_loss: 0.0032\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 8.5267e-04 - val_loss: 0.0036\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 8.6747e-04 - val_loss: 0.0034\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 8.0280e-04 - val_loss: 0.0032\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 19s 25ms/sample - loss: 0.0010 - val_loss: 0.0040\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 8.5582e-04 - val_loss: 0.0040\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 8.2382e-04 - val_loss: 0.0033\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 8.5063e-04 - val_loss: 0.0033\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 8.7560e-04 - val_loss: 0.0053\n",
            "Epoch 00036: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 73s 92ms/sample - loss: 0.0118 - val_loss: 0.0093\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0022 - val_loss: 0.0084\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0023 - val_loss: 0.0085\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 0.0020 - val_loss: 0.0072\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 19s 25ms/sample - loss: 0.0018 - val_loss: 0.0077\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0016 - val_loss: 0.0071\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0016 - val_loss: 0.0061\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0016 - val_loss: 0.0064\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0014 - val_loss: 0.0061\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0013 - val_loss: 0.0052\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 21s 26ms/sample - loss: 0.0012 - val_loss: 0.0052\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 0.0012 - val_loss: 0.0042\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 20s 26ms/sample - loss: 0.0011 - val_loss: 0.0041\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 0.0010 - val_loss: 0.0039\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 21s 26ms/sample - loss: 9.8833e-04 - val_loss: 0.0037\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 9.6378e-04 - val_loss: 0.0039\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 24s 30ms/sample - loss: 0.0010 - val_loss: 0.0040\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 22s 28ms/sample - loss: 9.0523e-04 - val_loss: 0.0034\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 23s 29ms/sample - loss: 9.3548e-04 - val_loss: 0.0036\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 23s 29ms/sample - loss: 8.7574e-04 - val_loss: 0.0043\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 22s 28ms/sample - loss: 8.5796e-04 - val_loss: 0.0035\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 22s 27ms/sample - loss: 8.5837e-04 - val_loss: 0.0032\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 9.0295e-04 - val_loss: 0.0037\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 9.2842e-04 - val_loss: 0.0040\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 8.6093e-04 - val_loss: 0.0035\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 21s 26ms/sample - loss: 8.8894e-04 - val_loss: 0.0034\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 22s 27ms/sample - loss: 9.1215e-04 - val_loss: 0.0043\n",
            "Epoch 00027: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 83s 105ms/sample - loss: 0.0128 - val_loss: 0.0082\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0023 - val_loss: 0.0082\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0021 - val_loss: 0.0092\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 16s 21ms/sample - loss: 0.0020 - val_loss: 0.0079\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 16s 21ms/sample - loss: 0.0020 - val_loss: 0.0071\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 17s 21ms/sample - loss: 0.0020 - val_loss: 0.0073\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0016 - val_loss: 0.0063\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 16s 21ms/sample - loss: 0.0016 - val_loss: 0.0065\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 16s 20ms/sample - loss: 0.0016 - val_loss: 0.0058\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 16s 21ms/sample - loss: 0.0014 - val_loss: 0.0053\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0014 - val_loss: 0.0053\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 0.0013 - val_loss: 0.0052\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 0.0013 - val_loss: 0.0050\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 16s 21ms/sample - loss: 0.0012 - val_loss: 0.0040\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 16s 21ms/sample - loss: 0.0011 - val_loss: 0.0048\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 16s 20ms/sample - loss: 0.0012 - val_loss: 0.0037\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 16s 21ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 16s 21ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 23s 29ms/sample - loss: 9.4087e-04 - val_loss: 0.0038\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 8.9495e-04 - val_loss: 0.0044\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 8.4404e-04 - val_loss: 0.0035\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 9.3393e-04 - val_loss: 0.0043\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 22s 28ms/sample - loss: 8.8379e-04 - val_loss: 0.0036\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 23s 29ms/sample - loss: 9.0804e-04 - val_loss: 0.0034\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 22s 28ms/sample - loss: 9.5920e-04 - val_loss: 0.0035\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 8.8028e-04 - val_loss: 0.0033\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 8.6695e-04 - val_loss: 0.0034\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 8.7920e-04 - val_loss: 0.0041\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 22s 27ms/sample - loss: 8.3854e-04 - val_loss: 0.0038\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 9.2985e-04 - val_loss: 0.0032\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 22s 28ms/sample - loss: 8.0049e-04 - val_loss: 0.0037\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 22s 27ms/sample - loss: 8.2649e-04 - val_loss: 0.0034\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 24s 30ms/sample - loss: 8.0551e-04 - val_loss: 0.0034\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 8.7693e-04 - val_loss: 0.0033\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 21s 26ms/sample - loss: 7.8396e-04 - val_loss: 0.0034\n",
            "Epoch 00035: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 69s 87ms/sample - loss: 0.0136 - val_loss: 0.0081\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0021 - val_loss: 0.0089\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0021 - val_loss: 0.0085\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0018 - val_loss: 0.0082\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0019 - val_loss: 0.0081\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0018 - val_loss: 0.0084\n",
            "Epoch 00006: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 65s 83ms/sample - loss: 0.0197 - val_loss: 0.0104\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0022 - val_loss: 0.0087\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0021 - val_loss: 0.0087\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0022 - val_loss: 0.0099\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0020 - val_loss: 0.0093\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0019 - val_loss: 0.0095\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0019 - val_loss: 0.0081\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0018 - val_loss: 0.0072\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0017 - val_loss: 0.0075\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0017 - val_loss: 0.0072\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0016 - val_loss: 0.0066\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0015 - val_loss: 0.0065\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0014 - val_loss: 0.0076\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0014 - val_loss: 0.0062\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0014 - val_loss: 0.0058\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0013 - val_loss: 0.0055\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0013 - val_loss: 0.0059\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0012 - val_loss: 0.0061\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0013 - val_loss: 0.0049\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0013 - val_loss: 0.0045\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0010 - val_loss: 0.0045\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 12s 15ms/sample - loss: 9.9422e-04 - val_loss: 0.0038\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 15s 19ms/sample - loss: 0.0010 - val_loss: 0.0038\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 10s 13ms/sample - loss: 0.0011 - val_loss: 0.0040\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.3303e-04 - val_loss: 0.0041\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.7601e-04 - val_loss: 0.0049\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 9.2010e-04 - val_loss: 0.0035\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 8.9855e-04 - val_loss: 0.0037\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 9.3140e-04 - val_loss: 0.0039\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 8.8759e-04 - val_loss: 0.0037\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.0217e-04 - val_loss: 0.0035\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 8.1726e-04 - val_loss: 0.0037\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.0782e-04 - val_loss: 0.0036\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 8.6763e-04 - val_loss: 0.0034\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 8.3177e-04 - val_loss: 0.0041 - ETA: 0s - loss: 8.38\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 8.2704e-04 - val_loss: 0.0034\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 8.3226e-04 - val_loss: 0.0033\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 8.6568e-04 - val_loss: 0.0033\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 8.5649e-04 - val_loss: 0.0036\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 7.9359e-04 - val_loss: 0.0033\n",
            "Epoch 43/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 7.7991e-04 - val_loss: 0.0033\n",
            "Epoch 44/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 7.8689e-04 - val_loss: 0.0034\n",
            "Epoch 45/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 7.7051e-04 - val_loss: 0.0033\n",
            "Epoch 46/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 8.9278e-04 - val_loss: 0.0033\n",
            "Epoch 47/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 8.2654e-04 - val_loss: 0.0033\n",
            "Epoch 48/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 8.1406e-04 - val_loss: 0.0033\n",
            "Epoch 49/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 8.0684e-04 - val_loss: 0.0036\n",
            "Epoch 50/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 7.7902e-04 - val_loss: 0.0032\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 69s 88ms/sample - loss: 0.0191 - val_loss: 0.0104\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0022 - val_loss: 0.0096\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0020 - val_loss: 0.0095TA:  - ETA\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 11s 13ms/sample - loss: 0.0019 - val_loss: 0.0078\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0019 - val_loss: 0.0097\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0019 - val_loss: 0.0095\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0016 - val_loss: 0.0080\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0018 - val_loss: 0.0087\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 13s 17ms/sample - loss: 0.0017 - val_loss: 0.0080\n",
            "Epoch 00009: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 68s 86ms/sample - loss: 0.0369 - val_loss: 0.0182\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0036 - val_loss: 0.0123\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0023 - val_loss: 0.0097\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0022 - val_loss: 0.0095\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0020 - val_loss: 0.0091\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0020 - val_loss: 0.0087\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0019 - val_loss: 0.0097\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0020 - val_loss: 0.0094\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0019 - val_loss: 0.0093\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0018 - val_loss: 0.0091\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0018 - val_loss: 0.0103\n",
            "Epoch 00011: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 60s 76ms/sample - loss: 0.0606 - val_loss: 0.0177\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0034 - val_loss: 0.0106A: 0s - loss: 0\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0024 - val_loss: 0.0099ETA: 1s - los - ETA: 0s - loss: 0.00 - ETA: 0s - loss: \n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0024 - val_loss: 0.0103\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0023 - val_loss: 0.0101\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0023 - val_loss: 0.0110\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0021 - val_loss: 0.0094\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0021 - val_loss: 0.0104\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0020 - val_loss: 0.0095\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0020 - val_loss: 0.0103\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0021 - val_loss: 0.0096\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0020 - val_loss: 0.0109\n",
            "Epoch 00012: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 72s 92ms/sample - loss: 0.0525 - val_loss: 0.0159\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0033 - val_loss: 0.0092\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0024 - val_loss: 0.0094\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0022 - val_loss: 0.0091\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0021 - val_loss: 0.0093\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0020 - val_loss: 0.0090\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0020 - val_loss: 0.0092\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0019 - val_loss: 0.0091\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 7s 8ms/sample - loss: 0.0019 - val_loss: 0.0096: 1\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0019 - val_loss: 0.0090\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0019 - val_loss: 0.0089\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0019 - val_loss: 0.008700\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0018 - val_loss: 0.0084\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0018 - val_loss: 0.0088\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0017 - val_loss: 0.0094\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0017 - val_loss: 0.0088\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0016 - val_loss: 0.0084\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0016 - val_loss: 0.0088\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0017 - val_loss: 0.0080\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0016 - val_loss: 0.0084\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0015 - val_loss: 0.0084\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0015 - val_loss: 0.0071\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0015 - val_loss: 0.0080\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0014 - val_loss: 0.0071\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0014 - val_loss: 0.0070\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0014 - val_loss: 0.0077\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0014 - val_loss: 0.0076\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0013 - val_loss: 0.0066 loss: 0.0\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0014 - val_loss: 0.0065\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0013 - val_loss: 0.0063\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0012 - val_loss: 0.0062\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0012 - val_loss: 0.0056\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0012 - val_loss: 0.0069\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0013 - val_loss: 0.0057\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0012 - val_loss: 0.0055\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0012 - val_loss: 0.0052\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0012 - val_loss: 0.0051\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0011 - val_loss: 0.0047s - loss: 0\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0014 - val_loss: 0.0046\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0010 - val_loss: 0.0047\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0010 - val_loss: 0.0049\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 43/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 44/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 9.1907e-04 - val_loss: 0.0042\n",
            "Epoch 45/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 9.5214e-04 - val_loss: 0.0047\n",
            "Epoch 46/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 47/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 9.5939e-04 - val_loss: 0.0042: 1s - l\n",
            "Epoch 48/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0010 - val_loss: 0.0043\n",
            "Epoch 00048: early stopping\n",
            "Best_hyper_parameters: \n",
            " {'model': [100], 'optimizer': 'Adam', 'learning_rate': 0.001, 'batch_size': 16, 'best_avg_rmse': 21.379305620149044}\n",
            "all_avg_rmse: \n",
            " [[[42.69928942 35.81131802 33.0891716 ]\n",
            "  [28.19007753 25.34842356 23.46193814]\n",
            "  [22.16360439 21.567633   21.37930562]]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model': [100],\n",
              " 'optimizer': 'Adam',\n",
              " 'learning_rate': 0.001,\n",
              " 'batch_size': 16,\n",
              " 'best_avg_rmse': 21.379305620149044}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "layers = [100]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs =  50\n",
        "num_replicates = 3\n",
        "test_split = 0.2 \n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "N100_best_hyper_parameters = hyper_parameter_tuning(layers, train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "N100_best_hyper_parameters\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A99NZ7EeOjlP"
      },
      "source": [
        "### **Case V: Tuning parameter of 150N-singlelayer-LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "6YaGcabQOqBs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 0 replicate \n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 1.4098 - val_loss: 0.0080\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0040 - val_loss: 0.0099\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0052 - val_loss: 0.0039\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0067 - val_loss: 0.0084\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0035 - val_loss: 0.0045\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0068 - val_loss: 0.0264\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 4s 4ms/sample - loss: 0.0061 - val_loss: 0.0055\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0039 - val_loss: 0.0141\n",
            "Epoch 00008: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 7s 8ms/sample - loss: 1.1022 - val_loss: 0.0102\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0049 - val_loss: 0.0078\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0058 - val_loss: 0.0093\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0041 - val_loss: 0.0073\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0047 - val_loss: 0.0296\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0080 - val_loss: 0.0087\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0089 - val_loss: 0.0108\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0218 - val_loss: 0.0230\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0186 - val_loss: 0.0268\n",
            "Epoch 00009: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 1.9005 - val_loss: 0.0446\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0155 - val_loss: 0.0248\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0076 - val_loss: 0.0338\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0106 - val_loss: 0.0231\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0123 - val_loss: 0.0374\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0138 - val_loss: 0.0496\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0150 - val_loss: 0.0739\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0279 - val_loss: 0.0574\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0206 - val_loss: 0.1169\n",
            "Epoch 00009: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 3.2590 - val_loss: 0.0213\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0106 - val_loss: 0.0170\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0119 - val_loss: 0.0090\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0064 - val_loss: 0.0395\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0039 - val_loss: 0.0077\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0042 - val_loss: 0.0060\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0028 - val_loss: 0.0120\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0038 - val_loss: 0.0186\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0037 - val_loss: 0.0108\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0042 - val_loss: 0.0063\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0035 - val_loss: 0.0089\n",
            "Epoch 00011: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 2.3242 - val_loss: 0.0124\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0092 - val_loss: 0.0134\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0069 - val_loss: 0.0109\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0077 - val_loss: 0.0248\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0085 - val_loss: 0.0112\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0064 - val_loss: 0.0383\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0087 - val_loss: 0.0176\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0158 - val_loss: 0.0567\n",
            "Epoch 00008: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 4.3597 - val_loss: 0.0087\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0039 - val_loss: 0.0054\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0038 - val_loss: 0.0069\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0048 - val_loss: 0.0068\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0025 - val_loss: 0.0054\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0041 - val_loss: 0.0108\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0034 - val_loss: 0.0059\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0023 - val_loss: 0.0065\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0024 - val_loss: 0.0043\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0020 - val_loss: 0.0041\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0042\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0043\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0023 - val_loss: 0.0054\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0044\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0035 - val_loss: 0.0068\n",
            "Epoch 00015: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 11.3127 - val_loss: 0.6037\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0945 - val_loss: 0.1168\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0455 - val_loss: 0.1116\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0377 - val_loss: 0.0814\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0300 - val_loss: 0.0451\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0214 - val_loss: 0.0414\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0191 - val_loss: 0.0239\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0096 - val_loss: 0.0259\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0069 - val_loss: 0.0219\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0056 - val_loss: 0.0307\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0051 - val_loss: 0.0198\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0044 - val_loss: 0.0201\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0043 - val_loss: 0.0205\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0044 - val_loss: 0.0261\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0043 - val_loss: 0.0293\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0047 - val_loss: 0.0233\n",
            "Epoch 00016: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 5.4241 - val_loss: 0.1136\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0984 - val_loss: 0.0352\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0162 - val_loss: 0.0208\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0087 - val_loss: 0.0306\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0072 - val_loss: 0.0346\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0073 - val_loss: 0.0187\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0066 - val_loss: 0.0230\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0101 - val_loss: 0.0164\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0065 - val_loss: 0.0184\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0053 - val_loss: 0.0268A: 0s - \n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0095 - val_loss: 0.0219\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0062 - val_loss: 0.0316\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0066 - val_loss: 0.0231\n",
            "Epoch 00013: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 3.8001 - val_loss: 0.0798\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0122 - val_loss: 0.0090\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0067 - val_loss: 0.0091\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0037 - val_loss: 0.0098\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0176 - val_loss: 0.1617\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0170 - val_loss: 0.0105\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 1s 2ms/sample - loss: 0.0177 - val_loss: 0.0485ss:\n",
            "Epoch 00007: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0114 - val_loss: 0.0076\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0019 - val_loss: 0.0052\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0019 - val_loss: 0.0039\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0012 - val_loss: 0.0039\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0012 - val_loss: 0.0045\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0014 - val_loss: 0.0036\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0012 - val_loss: 0.0044\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0012 - val_loss: 0.0046\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0015 - val_loss: 0.0035\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0014 - val_loss: 0.0056\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0012 - val_loss: 0.0048\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0011 - val_loss: 0.0034\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0011 - val_loss: 0.0040\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0011 - val_loss: 0.0036\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0013 - val_loss: 0.0040\n",
            "Epoch 00017: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0093 - val_loss: 0.0073\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0019 - val_loss: 0.0059\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0015 - val_loss: 0.0057\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0015 - val_loss: 0.0035\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0012 - val_loss: 0.0053\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0012 - val_loss: 0.0042\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0012 - val_loss: 0.0040\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0012 - val_loss: 0.0046\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0012 - val_loss: 0.0036\n",
            "Epoch 00009: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0102 - val_loss: 0.0064\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 5s 7ms/sample - loss: 0.0020 - val_loss: 0.0054\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0019 - val_loss: 0.0040\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0013 - val_loss: 0.0049\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0014 - val_loss: 0.0043 0s - loss: \n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0011 - val_loss: 0.0052\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 6s 7ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 00008: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0221 - val_loss: 0.0071\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0022 - val_loss: 0.0071\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0018 - val_loss: 0.0068\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0019 - val_loss: 0.0062\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0051\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0056\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0046\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0043\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0039\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0041\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0036\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0053\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0033\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.3945e-04 - val_loss: 0.0033\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.3893e-04 - val_loss: 0.0036\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.2581e-04 - val_loss: 0.0042\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0035\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.7108e-04 - val_loss: 0.0033\n",
            "Epoch 00019: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0178 - val_loss: 0.0101\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0023 - val_loss: 0.0073\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0016 - val_loss: 0.0065\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0018 - val_loss: 0.0062\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0019 - val_loss: 0.0049\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0066\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0052\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0048\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0012 - val_loss: 0.0036\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.3266e-04 - val_loss: 0.0043\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.4965e-04 - val_loss: 0.0049\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0041\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0011 - val_loss: 0.0047\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0013 - val_loss: 0.0040\n",
            "Epoch 00014: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0158 - val_loss: 0.0071\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0021 - val_loss: 0.0082\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0064\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0016 - val_loss: 0.0053\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0058\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0014 - val_loss: 0.0044\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0015 - val_loss: 0.0060\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0033\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.3172e-04 - val_loss: 0.0047\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.6510e-04 - val_loss: 0.0039\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0010 - val_loss: 0.0038\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 9.0163e-04 - val_loss: 0.0037\n",
            "Epoch 00014: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 6s 8ms/sample - loss: 0.0221 - val_loss: 0.0091\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0020 - val_loss: 0.0080\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0018 - val_loss: 0.0064\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0067\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0058\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0069\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0071\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0059\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0043\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0061\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0042\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0044\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.5940e-04 - val_loss: 0.0034\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.7050e-04 - val_loss: 0.0042\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.2600e-04 - val_loss: 0.0041-\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.2179e-04 - val_loss: 0.0034\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.5053e-04 - val_loss: 0.0033\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.6332e-04 - val_loss: 0.0039\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0038- loss: 0.00\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.7519e-04 - val_loss: 0.0033\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.0231e-04 - val_loss: 0.0033\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0046\n",
            "Epoch 00023: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0239 - val_loss: 0.0116\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0018 - val_loss: 0.0083\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0073\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0014 - val_loss: 0.0070\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0069\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0064\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0050\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0050\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0053\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0046\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0063\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0039\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0012 - val_loss: 0.0039\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0012 - val_loss: 0.0040\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.1483e-04 - val_loss: 0.0036\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.0233e-04 - val_loss: 0.0036\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0012 - val_loss: 0.0033\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.5971e-04 - val_loss: 0.0039\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.2719e-04 - val_loss: 0.0036\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.3861e-04 - val_loss: 0.0045\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.5533e-04 - val_loss: 0.0037\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0010 - val_loss: 0.0033\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.9062e-04 - val_loss: 0.0037\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.7554e-04 - val_loss: 0.0039\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.1354e-04 - val_loss: 0.0037\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.4041e-04 - val_loss: 0.0034\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.1571e-04 - val_loss: 0.0035-0\n",
            "Epoch 00028: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0313 - val_loss: 0.0113\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0019 - val_loss: 0.0090\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0070\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0015 - val_loss: 0.0064\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0060\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0016 - val_loss: 0.0051\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0015 - val_loss: 0.0064\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0050\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0014 - val_loss: 0.0045\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0013 - val_loss: 0.0041\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 0.0011 - val_loss: 0.0046\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 9.9584e-04 - val_loss: 0.0040\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0060\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0012 - val_loss: 0.0040\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0013 - val_loss: 0.0052\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0010 - val_loss: 0.0039\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.4676e-04 - val_loss: 0.0041\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.0511e-04 - val_loss: 0.0038\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.1355e-04 - val_loss: 0.0037\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.9584e-04 - val_loss: 0.0039\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.2058e-04 - val_loss: 0.0034\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 8.3277e-04 - val_loss: 0.0033\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0011 - val_loss: 0.0052\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.5777e-04 - val_loss: 0.0034\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0012 - val_loss: 0.0037\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.5786e-04 - val_loss: 0.0033\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.0534e-04 - val_loss: 0.0032\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.8577e-04 - val_loss: 0.0036\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0010 - val_loss: 0.0038\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 8.0255e-04 - val_loss: 0.0033\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 2s 2ms/sample - loss: 7.9623e-04 - val_loss: 0.0036\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 9.1726e-04 - val_loss: 0.0041\n",
            "Epoch 00032: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 13s 16ms/sample - loss: 0.0127 - val_loss: 0.0099\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0023 - val_loss: 0.0088\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0022 - val_loss: 0.0085\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0021 - val_loss: 0.0101\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0018 - val_loss: 0.0097\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0017 - val_loss: 0.0073\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0017 - val_loss: 0.0060\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 7s 10ms/sample - loss: 0.0016 - val_loss: 0.0059\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0015 - val_loss: 0.0065\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0013 - val_loss: 0.0054\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0012 - val_loss: 0.0050\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0012 - val_loss: 0.0046\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0012 - val_loss: 0.0045\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0011 - val_loss: 0.0040\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 9.3582e-04 - val_loss: 0.0038\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 9.4112e-04 - val_loss: 0.0037\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 9.1744e-04 - val_loss: 0.0035\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 9.4987e-04 - val_loss: 0.0036\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 9.2136e-04 - val_loss: 0.0033\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 9.0468e-04 - val_loss: 0.0033\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 8.6989e-04 - val_loss: 0.0033\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 8.6885e-04 - val_loss: 0.0039\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 9.6685e-04 - val_loss: 0.0042\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 8.2215e-04 - val_loss: 0.0037\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 8.9435e-04 - val_loss: 0.0035\n",
            "Epoch 00026: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 13s 17ms/sample - loss: 0.0115 - val_loss: 0.0099\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0022 - val_loss: 0.0095\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0019 - val_loss: 0.0079\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0019 - val_loss: 0.0085\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0018 - val_loss: 0.0091\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0019 - val_loss: 0.0089\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0016 - val_loss: 0.0055\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0015 - val_loss: 0.0062\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0013 - val_loss: 0.0057\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0014 - val_loss: 0.0052\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0012 - val_loss: 0.0050\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0012 - val_loss: 0.0060\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0012 - val_loss: 0.0041\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0011 - val_loss: 0.0047\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0010 - val_loss: 0.0035\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 9.9147e-04 - val_loss: 0.0035\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 9.2854e-04 - val_loss: 0.0035\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0010 - val_loss: 0.0038\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 8.9180e-04 - val_loss: 0.0039\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 9.3857e-04 - val_loss: 0.0034\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 9.0840e-04 - val_loss: 0.0038\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 9.0031e-04 - val_loss: 0.0036\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 9.1740e-04 - val_loss: 0.0038\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 7s 10ms/sample - loss: 8.9447e-04 - val_loss: 0.0040\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 8.8489e-04 - val_loss: 0.0036\n",
            "Epoch 00025: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 14s 18ms/sample - loss: 0.0136 - val_loss: 0.0094\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0024 - val_loss: 0.0100\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0022 - val_loss: 0.0081\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0022 - val_loss: 0.0085\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0018 - val_loss: 0.0076\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0017 - val_loss: 0.0064\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0016 - val_loss: 0.0064\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0015 - val_loss: 0.0069\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0014 - val_loss: 0.0056\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0015 - val_loss: 0.0055\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0013 - val_loss: 0.0050\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0012 - val_loss: 0.0045\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0010 - val_loss: 0.0054\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0010 - val_loss: 0.0047\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0011 - val_loss: 0.0047\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0010 - val_loss: 0.0036\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0010 - val_loss: 0.0037\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 9.6718e-04 - val_loss: 0.0039\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 8.9818e-04 - val_loss: 0.0035\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 9.1495e-04 - val_loss: 0.0033\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 8.6850e-04 - val_loss: 0.0032\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 8.7964e-04 - val_loss: 0.0038\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 9.2351e-04 - val_loss: 0.0032\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 8.1979e-04 - val_loss: 0.0041\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 8.4334e-04 - val_loss: 0.0032\n",
            "Epoch 00027: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 10s 13ms/sample - loss: 0.0219 - val_loss: 0.0107\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0022 - val_loss: 0.0089\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0022 - val_loss: 0.0090\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0020 - val_loss: 0.0089\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0019 - val_loss: 0.0086\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0018 - val_loss: 0.0091\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0018 - val_loss: 0.0076\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0019 - val_loss: 0.0088\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0017 - val_loss: 0.0081\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0016 - val_loss: 0.0072\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0017 - val_loss: 0.0067\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0016 - val_loss: 0.0065\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0015 - val_loss: 0.0061\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0014 - val_loss: 0.0061\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0013 - val_loss: 0.0064\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0014 - val_loss: 0.0061\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0013 - val_loss: 0.0056\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0014 - val_loss: 0.0066\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0012 - val_loss: 0.0047\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0012 - val_loss: 0.0047\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0012 - val_loss: 0.0046\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0010 - val_loss: 0.0048\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.5239e-04 - val_loss: 0.0039\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.8077e-04 - val_loss: 0.0039\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.8043e-04 - val_loss: 0.0037\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.5239e-04 - val_loss: 0.0043\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0011 - val_loss: 0.0036\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.0003e-04 - val_loss: 0.0036\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.4306e-04 - val_loss: 0.0035\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.3066e-04 - val_loss: 0.0036\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.7451e-04 - val_loss: 0.0036\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.2889e-04 - val_loss: 0.0039\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.4263e-04 - val_loss: 0.0038\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.8325e-04 - val_loss: 0.0039\n",
            "Epoch 00036: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 12s 15ms/sample - loss: 0.0171 - val_loss: 0.0092\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0022 - val_loss: 0.0096\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0020 - val_loss: 0.0095\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0020 - val_loss: 0.0081\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0019 - val_loss: 0.0087\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0017 - val_loss: 0.0081\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0017 - val_loss: 0.0076\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0019 - val_loss: 0.0089\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0018 - val_loss: 0.0071\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0017 - val_loss: 0.0072\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0015 - val_loss: 0.0073\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0015 - val_loss: 0.0070\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0013 - val_loss: 0.0065\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0013 - val_loss: 0.0059\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0013 - val_loss: 0.0064\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0013 - val_loss: 0.0051\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0013 - val_loss: 0.0048\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0011 - val_loss: 0.0045\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0011 - val_loss: 0.0052\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0012 - val_loss: 0.0049\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0010 - val_loss: 0.0040\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.9854e-04 - val_loss: 0.0042\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.8352e-04 - val_loss: 0.0041\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0010 - val_loss: 0.0039\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0010 - val_loss: 0.0043\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.9929e-04 - val_loss: 0.0036\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.6948e-04 - val_loss: 0.0037\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.7376e-04 - val_loss: 0.0035\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.4140e-04 - val_loss: 0.0039\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.9195e-04 - val_loss: 0.0033\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.6348e-04 - val_loss: 0.0037\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.7936e-04 - val_loss: 0.0034\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.0165e-04 - val_loss: 0.0032\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 8.6099e-04 - val_loss: 0.0034\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 8.2232e-04 - val_loss: 0.0033\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 8.7597e-04 - val_loss: 0.0034\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.6261e-04 - val_loss: 0.0034\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.0790e-04 - val_loss: 0.0034\n",
            "Epoch 00039: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0119 - val_loss: 0.0096\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0022 - val_loss: 0.0106\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0020 - val_loss: 0.0095\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0019 - val_loss: 0.0085\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0018 - val_loss: 0.0086\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0019 - val_loss: 0.0080\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0018 - val_loss: 0.0072\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0016 - val_loss: 0.0067\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0016 - val_loss: 0.0072\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0014 - val_loss: 0.0085\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0016 - val_loss: 0.0065\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0015 - val_loss: 0.0058\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0014 - val_loss: 0.0057\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 5s 6ms/sample - loss: 0.0013 - val_loss: 0.0054\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0013 - val_loss: 0.0048\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0011 - val_loss: 0.0057\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0012 - val_loss: 0.0044\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0012 - val_loss: 0.0045\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0011 - val_loss: 0.0046\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 0.0011 - val_loss: 0.0051\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0010 - val_loss: 0.0044\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.8819e-04 - val_loss: 0.0041\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0010 - val_loss: 0.0051\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 0.0010 - val_loss: 0.0041\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.8879e-04 - val_loss: 0.0041\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.6281e-04 - val_loss: 0.0039\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 8.8696e-04 - val_loss: 0.0035\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 9.8136e-04 - val_loss: 0.0035\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.3678e-04 - val_loss: 0.0039\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.4518e-04 - val_loss: 0.0036\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 7.8479e-04 - val_loss: 0.0034\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.2236e-04 - val_loss: 0.0034\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 8.2211e-04 - val_loss: 0.0055\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 4s 5ms/sample - loss: 9.0413e-04 - val_loss: 0.0045\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 8.6692e-04 - val_loss: 0.0036\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 4s 6ms/sample - loss: 8.3573e-04 - val_loss: 0.0037\n",
            "Epoch 00036: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0493 - val_loss: 0.0111\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0026 - val_loss: 0.0084\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0020 - val_loss: 0.0088\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0085\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0081\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0093\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0082\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0084\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0020 - val_loss: 0.0082\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0091\n",
            "Epoch 00010: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0463 - val_loss: 0.0168\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0036 - val_loss: 0.0118\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0023 - val_loss: 0.0104\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0021 - val_loss: 0.0098\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0021 - val_loss: 0.0091\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0021 - val_loss: 0.0092\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0020 - val_loss: 0.0095\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0021 - val_loss: 0.0097\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0020 - val_loss: 0.0101\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0097\n",
            "Epoch 00010: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 12s 15ms/sample - loss: 0.0358 - val_loss: 0.0152\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0031 - val_loss: 0.0088\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0021 - val_loss: 0.0087\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0020 - val_loss: 0.0091\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 3s 3ms/sample - loss: 0.0020 - val_loss: 0.0089\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 3s 4ms/sample - loss: 0.0018 - val_loss: 0.0087\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0019 - val_loss: 0.0082\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0088\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0093\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0018 - val_loss: 0.0084\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0017 - val_loss: 0.0085\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 2s 3ms/sample - loss: 0.0016 - val_loss: 0.0083\n",
            "Epoch 00012: early stopping\n",
            "Best_hyper_parameters: \n",
            " {'model': [150], 'optimizer': 'Adam', 'learning_rate': 0.001, 'batch_size': 16, 'best_avg_rmse': 22.52703664401859}\n",
            "all_avg_rmse: \n",
            " [[[45.76956377 38.09520369 38.21384606]\n",
            "  [32.08936792 28.3393475  25.94443933]\n",
            "  [24.08099081 22.73319289 22.52703664]]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model': [150],\n",
              " 'optimizer': 'Adam',\n",
              " 'learning_rate': 0.001,\n",
              " 'batch_size': 16,\n",
              " 'best_avg_rmse': 22.52703664401859}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "layers = [150]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs =  50\n",
        "num_replicates = 3\n",
        "test_split = 0.2 \n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "N150_best_hyper_parameters = hyper_parameter_tuning(layers, train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "N150_best_hyper_parameters\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UQpYD_UFOvJu"
      },
      "source": [
        "### **Case VI: Tuning parameter of 200N-singlelayer-LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "KYPP2AMRO0Fj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 175s 222ms/sample - loss: 2.0518 - val_loss: 0.0268\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 43s 55ms/sample - loss: 0.0096 - val_loss: 0.0153\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 52s 67ms/sample - loss: 0.0048 - val_loss: 0.0055\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 49s 62ms/sample - loss: 0.0043 - val_loss: 0.0058\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 39s 50ms/sample - loss: 0.0055 - val_loss: 0.0093\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 38s 49ms/sample - loss: 0.0060 - val_loss: 0.0193TA: 2s - loss: 0 - ETA: 1s - loss\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 38s 48ms/sample - loss: 0.0096 - val_loss: 0.0154\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 37s 46ms/sample - loss: 0.0314 - val_loss: 0.0142\n",
            "Epoch 00008: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 123s 156ms/sample - loss: 1.8376 - val_loss: 0.0203 0s - loss: 1.844\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 41s 52ms/sample - loss: 0.0212 - val_loss: 0.0632\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 40s 51ms/sample - loss: 0.0462 - val_loss: 0.0565\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 44s 56ms/sample - loss: 0.0217 - val_loss: 0.0346\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 39s 50ms/sample - loss: 0.0430 - val_loss: 0.0271\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 39s 49ms/sample - loss: 0.0493 - val_loss: 0.0289\n",
            "Epoch 00006: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 107s 136ms/sample - loss: 1.5706 - val_loss: 0.0617\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 50s 63ms/sample - loss: 0.0383 - val_loss: 0.0304\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 53s 67ms/sample - loss: 0.0669 - val_loss: 0.0543\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 41s 52ms/sample - loss: 0.0483 - val_loss: 0.1974\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 41s 52ms/sample - loss: 0.0513 - val_loss: 0.0334\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 41s 52ms/sample - loss: 0.0723 - val_loss: 0.0353\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 37s 48ms/sample - loss: 0.0334 - val_loss: 0.0447\n",
            "Epoch 00007: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 92s 117ms/sample - loss: 7.0289 - val_loss: 0.1090\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 17s 21ms/sample - loss: 0.0527 - val_loss: 0.0590\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 17s 21ms/sample - loss: 0.0302 - val_loss: 0.0428\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 17s 21ms/sample - loss: 0.0176 - val_loss: 0.0262\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 17s 21ms/sample - loss: 0.0116 - val_loss: 0.0244\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 17s 21ms/sample - loss: 0.0080 - val_loss: 0.0277\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 17s 21ms/sample - loss: 0.0086 - val_loss: 0.0401\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 17s 21ms/sample - loss: 0.0107 - val_loss: 0.0221\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 17s 21ms/sample - loss: 0.0158 - val_loss: 0.0299\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 16s 21ms/sample - loss: 0.0093 - val_loss: 0.0628\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0137 - val_loss: 0.0223\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 19s 25ms/sample - loss: 0.0144 - val_loss: 0.0244\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 0.0124 - val_loss: 0.0424\n",
            "Epoch 00013: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 86s 109ms/sample - loss: 7.7169 - val_loss: 0.0830\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0335 - val_loss: 0.0568\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0141 - val_loss: 0.0283\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 43s 54ms/sample - loss: 0.0070 - val_loss: 0.0178\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 69s 88ms/sample - loss: 0.0061 - val_loss: 0.0209\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 35s 44ms/sample - loss: 0.0062 - val_loss: 0.0178\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0069 - val_loss: 0.0207\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0079 - val_loss: 0.0194\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 23s 29ms/sample - loss: 0.0053 - val_loss: 0.0248\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0058 - val_loss: 0.0209\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0073 - val_loss: 0.0202\n",
            "Epoch 00011: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 106s 135ms/sample - loss: 8.4666 - val_loss: 0.1402\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0148 - val_loss: 0.0989\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0159 - val_loss: 0.1126\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 24s 31ms/sample - loss: 0.0090 - val_loss: 0.0923\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 0.0085 - val_loss: 0.1188\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0233 - val_loss: 0.0931\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0378 - val_loss: 0.1134\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0251 - val_loss: 0.1002\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0144 - val_loss: 0.1303\n",
            "Epoch 00009: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 79s 100ms/sample - loss: 8.0159 - val_loss: 0.0270\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 10s 13ms/sample - loss: 0.0055 - val_loss: 0.0124\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0043 - val_loss: 0.0129\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 10s 12ms/sample - loss: 0.0037 - val_loss: 0.0103\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0029 - val_loss: 0.0098\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0043 - val_loss: 0.0077\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 10s 12ms/sample - loss: 0.0030 - val_loss: 0.0137\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 10s 12ms/sample - loss: 0.0039 - val_loss: 0.0115\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 10s 12ms/sample - loss: 0.0056 - val_loss: 0.0457\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0069 - val_loss: 0.0109\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0048 - val_loss: 0.0239\n",
            "Epoch 00011: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 78s 100ms/sample - loss: 9.9006 - val_loss: 0.0631\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0593 - val_loss: 0.0328\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0064 - val_loss: 0.0433\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0132 - val_loss: 0.0330\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0118 - val_loss: 0.0469\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 10s 13ms/sample - loss: 0.0073 - val_loss: 0.0141\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0097 - val_loss: 0.0137\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0066 - val_loss: 0.0207\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0054 - val_loss: 0.0113\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 10s 13ms/sample - loss: 0.0031 - val_loss: 0.0117\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0045 - val_loss: 0.0073\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0050 - val_loss: 0.0093\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0038 - val_loss: 0.0280\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0105 - val_loss: 0.0070 - ETA: 3s - loss:  - ETA: 2s \n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 10s 13ms/sample - loss: 0.0043 - val_loss: 0.0061\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0048 - val_loss: 0.0159\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 10s 12ms/sample - loss: 0.0053 - val_loss: 0.0169\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 10s 13ms/sample - loss: 0.0078 - val_loss: 0.0436\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 10s 12ms/sample - loss: 0.0152 - val_loss: 0.0186\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0053 - val_loss: 0.0119\n",
            "Epoch 00020: early stopping\n",
            "Running for Adam optimizer 0.1 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 114s 144ms/sample - loss: 7.7556 - val_loss: 0.1072\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0489 - val_loss: 0.0992\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0335 - val_loss: 0.0639\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0275 - val_loss: 0.0391\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0175 - val_loss: 0.0301\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 7s 9ms/sample - loss: 0.0126 - val_loss: 0.0266\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 7s 10ms/sample - loss: 0.0079 - val_loss: 0.0226\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 8s 10ms/sample - loss: 0.0080 - val_loss: 0.0223\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0072 - val_loss: 0.0349\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 13s 17ms/sample - loss: 0.0059 - val_loss: 0.0492\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 12s 15ms/sample - loss: 0.0071 - val_loss: 0.0312\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0063 - val_loss: 0.0269\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0068 - val_loss: 0.0240\n",
            "Epoch 00013: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 94s 119ms/sample - loss: 0.0119 - val_loss: 0.0074\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 33s 41ms/sample - loss: 0.0019 - val_loss: 0.0058\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 33s 41ms/sample - loss: 0.0014 - val_loss: 0.0053\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 36s 46ms/sample - loss: 0.0014 - val_loss: 0.0038\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 35s 44ms/sample - loss: 0.0018 - val_loss: 0.0039\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 35s 45ms/sample - loss: 0.0013 - val_loss: 0.0062\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 36s 46ms/sample - loss: 0.0010 - val_loss: 0.0034\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 45s 57ms/sample - loss: 0.0012 - val_loss: 0.0041\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 35s 44ms/sample - loss: 0.0011 - val_loss: 0.0036\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 34s 43ms/sample - loss: 0.0014 - val_loss: 0.0042\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 38s 49ms/sample - loss: 0.0012 - val_loss: 0.0041\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 39s 50ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 00012: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 102s 130ms/sample - loss: 0.0102 - val_loss: 0.0061\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 35s 45ms/sample - loss: 0.0020 - val_loss: 0.0048\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 32s 41ms/sample - loss: 0.0015 - val_loss: 0.0053\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 31s 40ms/sample - loss: 0.0014 - val_loss: 0.0058\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 31s 40ms/sample - loss: 0.0015 - val_loss: 0.0042\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 35s 44ms/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 33s 42ms/sample - loss: 0.0013 - val_loss: 0.0039\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 33s 42ms/sample - loss: 0.0011 - val_loss: 0.0048\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 34s 44ms/sample - loss: 0.0011 - val_loss: 0.0036\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 33s 41ms/sample - loss: 0.0013 - val_loss: 0.0036\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 34s 43ms/sample - loss: 0.0010 - val_loss: 0.0037\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 42s 54ms/sample - loss: 0.0012 - val_loss: 0.0038\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 31s 40ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 32s 40ms/sample - loss: 0.0011 - val_loss: 0.0036\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 37s 47ms/sample - loss: 0.0012 - val_loss: 0.0036\n",
            "Epoch 00015: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 108s 137ms/sample - loss: 0.0099 - val_loss: 0.0070\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 39s 49ms/sample - loss: 0.0019 - val_loss: 0.0048\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 35s 44ms/sample - loss: 0.0019 - val_loss: 0.0048\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 34s 44ms/sample - loss: 0.0014 - val_loss: 0.0040\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 34s 43ms/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 35s 45ms/sample - loss: 0.0012 - val_loss: 0.0046\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 38s 48ms/sample - loss: 0.0013 - val_loss: 0.0040A: 2s\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 35s 45ms/sample - loss: 0.0012 - val_loss: 0.0044\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 35s 44ms/sample - loss: 0.0010 - val_loss: 0.0043TA: 4s - loss: 0 - ET\n",
            "Epoch 00009: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 88s 112ms/sample - loss: 0.0217 - val_loss: 0.0103\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0021 - val_loss: 0.0077\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0017 - val_loss: 0.0061\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0017 - val_loss: 0.0066\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0015 - val_loss: 0.0061\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 0.0016 - val_loss: 0.0078\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0014 - val_loss: 0.0040\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0013 - val_loss: 0.0037\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0010 - val_loss: 0.0035\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 21s 26ms/sample - loss: 9.8796e-04 - val_loss: 0.0036\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 9.3327e-04 - val_loss: 0.0054\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0011 - val_loss: 0.0040\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0010 - val_loss: 0.0035\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 00014: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 103s 131ms/sample - loss: 0.0264 - val_loss: 0.0088\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 28s 36ms/sample - loss: 0.0021 - val_loss: 0.0067\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 27s 34ms/sample - loss: 0.0017 - val_loss: 0.0128\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 26s 33ms/sample - loss: 0.0019 - val_loss: 0.0060\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 22s 28ms/sample - loss: 0.0020 - val_loss: 0.0062\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 26s 33ms/sample - loss: 0.0011 - val_loss: 0.0054\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 23s 30ms/sample - loss: 0.0013 - val_loss: 0.0046\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 22s 29ms/sample - loss: 0.0011 - val_loss: 0.0040\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 0.0012 - val_loss: 0.0040\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 25s 32ms/sample - loss: 9.6814e-04 - val_loss: 0.0038\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 0.0012 - val_loss: 0.0058\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 22s 27ms/sample - loss: 0.0010 - val_loss: 0.0042\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 9.4595e-04 - val_loss: 0.0034\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 9.4244e-04 - val_loss: 0.0035\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 0.0010 - val_loss: 0.0041\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 9.7554e-04 - val_loss: 0.0040\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 22s 28ms/sample - loss: 9.2827e-04 - val_loss: 0.0033\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 22s 28ms/sample - loss: 0.0010 - val_loss: 0.0036\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 22s 27ms/sample - loss: 0.0010 - val_loss: 0.0034\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 22s 27ms/sample - loss: 8.7796e-04 - val_loss: 0.0041\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 23s 30ms/sample - loss: 9.4372e-04 - val_loss: 0.0041\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 27s 34ms/sample - loss: 9.4916e-04 - val_loss: 0.0043\n",
            "Epoch 00022: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 122s 155ms/sample - loss: 0.0168 - val_loss: 0.0071\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 0.0018 - val_loss: 0.0063\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 20s 26ms/sample - loss: 0.0023 - val_loss: 0.0054\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 21s 26ms/sample - loss: 0.0016 - val_loss: 0.0053\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 21s 26ms/sample - loss: 0.0013 - val_loss: 0.0047\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 0.0012 - val_loss: 0.0053 - ETA: 0s - loss: 0.0\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 0.0012 - val_loss: 0.0046TA: 3s - loss: 0.0 - ETA: 2\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 22s 28ms/sample - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 21s 26ms/sample - loss: 9.5370e-04 - val_loss: 0.0034s - loss: 8.6754e-0 - ETA: - ETA: 4s - loss: 9 - ETA: 2s - l\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 9.6834e-04 - val_loss: 0.0045\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 9.6689e-04 - val_loss: 0.0040A: 2s - los\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 20s 26ms/sample - loss: 0.0010 - val_loss: 0.0039\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 9.2615e-04 - val_loss: 0.0037 9.7047e- - ETA: 4s - loss: 9.53 - ETA: 3s - loss - ETA: 0s - loss: 9.171\n",
            "Epoch 00014: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 119s 151ms/sample - loss: 0.0578 - val_loss: 0.0175\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0027 - val_loss: 0.0104\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0018 - val_loss: 0.0081\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0017 - val_loss: 0.0075\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0017 - val_loss: 0.0076\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0016 - val_loss: 0.0073\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0016 - val_loss: 0.0064\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 13s 17ms/sample - loss: 0.0016 - val_loss: 0.0064\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0018 - val_loss: 0.0065\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0016 - val_loss: 0.0056\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0013 - val_loss: 0.0049\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0013 - val_loss: 0.0054\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0011 - val_loss: 0.0053\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 11s 13ms/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 11s 13ms/sample - loss: 0.0012 - val_loss: 0.0039\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 9.9364e-04 - val_loss: 0.0036\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 8.7229e-04 - val_loss: 0.0035\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0010 - val_loss: 0.0044\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 12s 15ms/sample - loss: 8.3784e-04 - val_loss: 0.0039\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 9.3159e-04 - val_loss: 0.0056\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 12s 15ms/sample - loss: 0.0010 - val_loss: 0.0048\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 8.6807e-04 - val_loss: 0.0040\n",
            "Epoch 00023: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 96s 121ms/sample - loss: 0.0520 - val_loss: 0.0176\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0021 - val_loss: 0.0116\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0018 - val_loss: 0.0074\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0018 - val_loss: 0.0083\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0018 - val_loss: 0.0069\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0017 - val_loss: 0.0074\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0021 - val_loss: 0.0062\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0014 - val_loss: 0.0058\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0012 - val_loss: 0.0059\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0013 - val_loss: 0.0060\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0012 - val_loss: 0.0095\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0012 - val_loss: 0.0044\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0012 - val_loss: 0.0042\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0010 - val_loss: 0.0065\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0011 - val_loss: 0.0046\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0010 - val_loss: 0.0045\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0011 - val_loss: 0.0034\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 8.6078e-04 - val_loss: 0.0039\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 12s 15ms/sample - loss: 8.5346e-04 - val_loss: 0.0050\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 13s 16ms/sample - loss: 8.9764e-04 - val_loss: 0.0038\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 12s 15ms/sample - loss: 9.9582e-04 - val_loss: 0.0035\n",
            "Epoch 00023: early stopping\n",
            "Running for Adam optimizer 0.01 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 83s 106ms/sample - loss: 0.0346 - val_loss: 0.0076\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0022 - val_loss: 0.0085\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0021 - val_loss: 0.0066\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0020 - val_loss: 0.0072\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0017 - val_loss: 0.0060\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0014 - val_loss: 0.0071\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0017 - val_loss: 0.0069\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 11s 15ms/sample - loss: 0.0017 - val_loss: 0.0056\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0012 - val_loss: 0.0056\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0012 - val_loss: 0.0060\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 11s 15ms/sample - loss: 0.0011 - val_loss: 0.0058\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0014 - val_loss: 0.0046\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 11s 15ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 11s 14ms/sample - loss: 0.0011 - val_loss: 0.0056\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 12s 15ms/sample - loss: 9.4215e-04 - val_loss: 0.0037\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 12s 15ms/sample - loss: 0.0011 - val_loss: 0.0054\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 13s 16ms/sample - loss: 0.0011 - val_loss: 0.0039\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 13s 16ms/sample - loss: 8.7899e-04 - val_loss: 0.0035\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 13s 16ms/sample - loss: 8.9347e-04 - val_loss: 0.0044\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 13s 16ms/sample - loss: 8.9033e-04 - val_loss: 0.0043\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 12s 15ms/sample - loss: 9.0783e-04 - val_loss: 0.0038\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 12s 15ms/sample - loss: 9.2422e-04 - val_loss: 0.0040\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 12s 15ms/sample - loss: 8.4948e-04 - val_loss: 0.0038\n",
            "Epoch 00024: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 105s 134ms/sample - loss: 0.0102 - val_loss: 0.0087\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 38s 48ms/sample - loss: 0.0020 - val_loss: 0.0091 - loss: 0.00\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 37s 47ms/sample - loss: 0.0021 - val_loss: 0.0073\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 39s 49ms/sample - loss: 0.0025 - val_loss: 0.0083\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 42s 53ms/sample - loss: 0.0017 - val_loss: 0.0076\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 36s 46ms/sample - loss: 0.0015 - val_loss: 0.0064\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 36s 45ms/sample - loss: 0.0015 - val_loss: 0.0066\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 36s 45ms/sample - loss: 0.0015 - val_loss: 0.0053\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 36s 45ms/sample - loss: 0.0012 - val_loss: 0.0054\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 36s 46ms/sample - loss: 0.0013 - val_loss: 0.0053\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 36s 45ms/sample - loss: 0.0011 - val_loss: 0.0051\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 36s 46ms/sample - loss: 0.0012 - val_loss: 0.0044\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 36s 46ms/sample - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 36s 45ms/sample - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 36s 46ms/sample - loss: 9.0997e-04 - val_loss: 0.0043\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 35s 45ms/sample - loss: 9.7589e-04 - val_loss: 0.0038\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 36s 46ms/sample - loss: 9.6877e-04 - val_loss: 0.0040\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 36s 46ms/sample - loss: 9.0632e-04 - val_loss: 0.0033\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 39s 50ms/sample - loss: 8.7386e-04 - val_loss: 0.0032-\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 39s 50ms/sample - loss: 8.9589e-04 - val_loss: 0.0036\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 40s 50ms/sample - loss: 8.7016e-04 - val_loss: 0.0042\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 40s 51ms/sample - loss: 8.6351e-04 - val_loss: 0.0041\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 40s 51ms/sample - loss: 8.4489e-04 - val_loss: 0.0032s - loss: 7.89 - ETA: 3s - lo - ETA: 1s - loss: 8.33\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 40s 50ms/sample - loss: 9.3896e-04 - val_loss: 0.0035\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 41s 52ms/sample - loss: 9.6045e-04 - val_loss: 0.0036s: 9 - ETA: 3s - loss: 9.63 - ETA: 2s - loss: 9.6517e-0 - ETA: 2s - loss:\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 41s 52ms/sample - loss: 8.6826e-04 - val_loss: 0.0034s - loss:  - ETA: 6s - loss: 9.2818e - ETA: - ETA: 1s - loss: 8.\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 40s 50ms/sample - loss: 8.5736e-04 - val_loss: 0.0032\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 40s 51ms/sample - loss: 9.1626e-04 - val_loss: 0.0034oss:  - ETA: 2s - los\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 41s 52ms/sample - loss: 9.1891e-04 - val_loss: 0.0035\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 39s 50ms/sample - loss: 8.3105e-04 - val_loss: 0.0031\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 40s 50ms/sample - loss: 9.1438e-04 - val_loss: 0.0033: 2s - loss: 9.11 - ETA: 1s - loss: 8.98\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 45s 57ms/sample - loss: 9.3588e-04 - val_loss: 0.0035\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 32s 41ms/sample - loss: 8.7733e-04 - val_loss: 0.0035\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 33s 41ms/sample - loss: 8.7288e-04 - val_loss: 0.0035\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 34s 44ms/sample - loss: 8.8090e-04 - val_loss: 0.0032\n",
            "Epoch 00035: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 123s 157ms/sample - loss: 0.0136 - val_loss: 0.0095\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 35s 44ms/sample - loss: 0.0026 - val_loss: 0.0093\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 43s 54ms/sample - loss: 0.0025 - val_loss: 0.0092TA: 2s - loss: 0.002 - ETA: 1s - lo\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 38s 48ms/sample - loss: 0.0019 - val_loss: 0.0097\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 35s 45ms/sample - loss: 0.0018 - val_loss: 0.0070\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 41s 52ms/sample - loss: 0.0019 - val_loss: 0.0078\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 44s 56ms/sample - loss: 0.0017 - val_loss: 0.0066\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 36s 46ms/sample - loss: 0.0015 - val_loss: 0.0057\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 36s 46ms/sample - loss: 0.0014 - val_loss: 0.0049\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 39s 50ms/sample - loss: 0.0013 - val_loss: 0.0058\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 39s 50ms/sample - loss: 0.0013 - val_loss: 0.0051\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 41s 52ms/sample - loss: 0.0013 - val_loss: 0.0048\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 34s 43ms/sample - loss: 0.0011 - val_loss: 0.0055\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 35s 45ms/sample - loss: 0.0011 - val_loss: 0.0037\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 34s 43ms/sample - loss: 0.0010 - val_loss: 0.0036\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 32s 41ms/sample - loss: 0.0010 - val_loss: 0.0036\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 33s 42ms/sample - loss: 9.5709e-04 - val_loss: 0.0042\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 35s 44ms/sample - loss: 9.1301e-04 - val_loss: 0.0036\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 37s 47ms/sample - loss: 8.2851e-04 - val_loss: 0.0035\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 35s 44ms/sample - loss: 9.5078e-04 - val_loss: 0.0036\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 34s 44ms/sample - loss: 9.1753e-04 - val_loss: 0.0061\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 32s 41ms/sample - loss: 8.8440e-04 - val_loss: 0.0036\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 39s 49ms/sample - loss: 8.9234e-04 - val_loss: 0.0040\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 41s 52ms/sample - loss: 9.2859e-04 - val_loss: 0.0037\n",
            "Epoch 00024: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 4 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 152s 193ms/sample - loss: 0.0130 - val_loss: 0.0086\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 46s 58ms/sample - loss: 0.0025 - val_loss: 0.0082\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 46s 58ms/sample - loss: 0.0022 - val_loss: 0.0089\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 39s 50ms/sample - loss: 0.0021 - val_loss: 0.0070\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 39s 50ms/sample - loss: 0.0019 - val_loss: 0.0075\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 40s 50ms/sample - loss: 0.0016 - val_loss: 0.0068- loss: 0.001\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 38s 48ms/sample - loss: 0.0016 - val_loss: 0.0054\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 38s 48ms/sample - loss: 0.0016 - val_loss: 0.0056\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 36s 45ms/sample - loss: 0.0015 - val_loss: 0.0052\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 44s 56ms/sample - loss: 0.0012 - val_loss: 0.0049\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 39s 50ms/sample - loss: 0.0012 - val_loss: 0.0050\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 37s 47ms/sample - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 34s 43ms/sample - loss: 0.0011 - val_loss: 0.0048\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 35s 45ms/sample - loss: 0.0011 - val_loss: 0.0050\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 34s 43ms/sample - loss: 9.4617e-04 - val_loss: 0.0038\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 34s 44ms/sample - loss: 9.4151e-04 - val_loss: 0.0036\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 33s 41ms/sample - loss: 9.6894e-04 - val_loss: 0.0037\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 34s 43ms/sample - loss: 9.0945e-04 - val_loss: 0.0037\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 35s 44ms/sample - loss: 9.7726e-04 - val_loss: 0.0033\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 35s 44ms/sample - loss: 9.1524e-04 - val_loss: 0.0034\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 33s 42ms/sample - loss: 9.1087e-04 - val_loss: 0.0036\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 32s 41ms/sample - loss: 8.8230e-04 - val_loss: 0.0039\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 34s 43ms/sample - loss: 8.6687e-04 - val_loss: 0.0034\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 33s 42ms/sample - loss: 8.6967e-04 - val_loss: 0.0051\n",
            "Epoch 00024: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 92s 117ms/sample - loss: 0.0117 - val_loss: 0.0088\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0022 - val_loss: 0.0090\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 0.0021 - val_loss: 0.0111\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0019 - val_loss: 0.0094\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0019 - val_loss: 0.0073\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0017 - val_loss: 0.0095\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 17s 21ms/sample - loss: 0.0019 - val_loss: 0.0086\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 16s 21ms/sample - loss: 0.0016 - val_loss: 0.0071\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0015 - val_loss: 0.0080\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 16s 21ms/sample - loss: 0.0016 - val_loss: 0.0061\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0015 - val_loss: 0.0067\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 16s 21ms/sample - loss: 0.0016 - val_loss: 0.0051\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 16s 21ms/sample - loss: 0.0013 - val_loss: 0.0050\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 17s 21ms/sample - loss: 0.0013 - val_loss: 0.0047\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 0.0011 - val_loss: 0.0056\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 17s 21ms/sample - loss: 0.0013 - val_loss: 0.0051\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 0.0013 - val_loss: 0.0044\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 23s 29ms/sample - loss: 0.0011 - val_loss: 0.0050\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 21s 27ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 9.9264e-04 - val_loss: 0.0047\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0010 - val_loss: 0.0041\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 9.0989e-04 - val_loss: 0.0041\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0011 - val_loss: 0.0045\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 8.7637e-04 - val_loss: 0.0044\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 17s 21ms/sample - loss: 8.8624e-04 - val_loss: 0.0035\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 9.0000e-04 - val_loss: 0.0040\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 9.7445e-04 - val_loss: 0.0044\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 8.9046e-04 - val_loss: 0.0037\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 9.4471e-04 - val_loss: 0.0033\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 9.1204e-04 - val_loss: 0.0034\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 7.8605e-04 - val_loss: 0.0037\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 7.9858e-04 - val_loss: 0.0033\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 8.9307e-04 - val_loss: 0.0035\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 22s 28ms/sample - loss: 8.1751e-04 - val_loss: 0.0034\n",
            "Epoch 00034: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 93s 118ms/sample - loss: 0.0183 - val_loss: 0.0090\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0022 - val_loss: 0.0093\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0021 - val_loss: 0.0104los\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 19s 25ms/sample - loss: 0.0019 - val_loss: 0.0103\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 0.0019 - val_loss: 0.0085\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 19s 25ms/sample - loss: 0.0018 - val_loss: 0.0083\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0019 - val_loss: 0.0077\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 23s 30ms/sample - loss: 0.0020 - val_loss: 0.0082\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 22s 28ms/sample - loss: 0.0017 - val_loss: 0.0065\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 23s 29ms/sample - loss: 0.0016 - val_loss: 0.0065\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 22s 27ms/sample - loss: 0.0016 - val_loss: 0.0071\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 0.0015 - val_loss: 0.0056\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0013 - val_loss: 0.0057\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0013 - val_loss: 0.0065\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0013 - val_loss: 0.0049\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 0.0012 - val_loss: 0.0054\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 0.0013 - val_loss: 0.0051\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0011 - val_loss: 0.0052\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 24s 31ms/sample - loss: 0.0012 - val_loss: 0.0041\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 22s 28ms/sample - loss: 0.0010 - val_loss: 0.0046\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0010 - val_loss: 0.0047\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0011 - val_loss: 0.0042\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 0.0010 - val_loss: 0.0038\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 9.8260e-04 - val_loss: 0.0039\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0010 - val_loss: 0.0039\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 8.5951e-04 - val_loss: 0.0040\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 20s 26ms/sample - loss: 8.8252e-04 - val_loss: 0.0044\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 22s 27ms/sample - loss: 9.4171e-04 - val_loss: 0.0037\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 8.8535e-04 - val_loss: 0.0033: 2s -\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 9.2547e-04 - val_loss: 0.0034\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 8.4686e-04 - val_loss: 0.0034\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 9.2808e-04 - val_loss: 0.0040\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 9.1061e-04 - val_loss: 0.0039\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 9.0094e-04 - val_loss: 0.0037\n",
            "Epoch 00034: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 8 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 91s 115ms/sample - loss: 0.0236 - val_loss: 0.0116\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0023 - val_loss: 0.0088\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0020 - val_loss: 0.0101\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0023 - val_loss: 0.0088\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0020 - val_loss: 0.0081\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0020 - val_loss: 0.0081\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0020 - val_loss: 0.0079\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0018 - val_loss: 0.0081\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0016 - val_loss: 0.0085\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 24s 31ms/sample - loss: 0.0016 - val_loss: 0.0076\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 0.0016 - val_loss: 0.0066\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0018 - val_loss: 0.0072\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 0.0014 - val_loss: 0.0060\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0014 - val_loss: 0.0056\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 0.0015 - val_loss: 0.0052\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0012 - val_loss: 0.0054\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0013 - val_loss: 0.0060\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0012 - val_loss: 0.0047\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0013 - val_loss: 0.0046\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 0.0012 - val_loss: 0.0051\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0010 - val_loss: 0.0049\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 9.8311e-04 - val_loss: 0.0046\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 19s 24ms/sample - loss: 9.1655e-04 - val_loss: 0.0053\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 0.0011 - val_loss: 0.0043\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 0.0010 - val_loss: 0.0038\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 9.3123e-04 - val_loss: 0.0040\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 18s 23ms/sample - loss: 9.0581e-04 - val_loss: 0.0039\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 8.7398e-04 - val_loss: 0.0038\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 20s 25ms/sample - loss: 8.6885e-04 - val_loss: 0.0034\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 8.8861e-04 - val_loss: 0.0035\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 8.4712e-04 - val_loss: 0.0037\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 9.6040e-04 - val_loss: 0.0035\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 18s 22ms/sample - loss: 8.4768e-04 - val_loss: 0.0043\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 17s 22ms/sample - loss: 9.5465e-04 - val_loss: 0.0036\n",
            "Epoch 00035: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 0 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 75s 96ms/sample - loss: 0.0250 - val_loss: 0.0093\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0023 - val_loss: 0.0092\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0021 - val_loss: 0.0084\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0020 - val_loss: 0.0088\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0021 - val_loss: 0.0095\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0019 - val_loss: 0.0086\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0017 - val_loss: 0.0085\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0018 - val_loss: 0.0082\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0017 - val_loss: 0.0081\n",
            "Epoch 10/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0018 - val_loss: 0.0078\n",
            "Epoch 11/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0017 - val_loss: 0.0074\n",
            "Epoch 12/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0016 - val_loss: 0.0072\n",
            "Epoch 13/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0017 - val_loss: 0.0073\n",
            "Epoch 14/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0016 - val_loss: 0.0070\n",
            "Epoch 15/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0015 - val_loss: 0.0066\n",
            "Epoch 16/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0015 - val_loss: 0.0075\n",
            "Epoch 17/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0016 - val_loss: 0.0081\n",
            "Epoch 18/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0014 - val_loss: 0.0058\n",
            "Epoch 19/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0013 - val_loss: 0.0068\n",
            "Epoch 20/50\n",
            "787/787 [==============================] - 9s 12ms/sample - loss: 0.0014 - val_loss: 0.0058\n",
            "Epoch 21/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0013 - val_loss: 0.0065\n",
            "Epoch 22/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0013 - val_loss: 0.0062\n",
            "Epoch 23/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0013 - val_loss: 0.0051\n",
            "Epoch 24/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0012 - val_loss: 0.0054\n",
            "Epoch 25/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0012 - val_loss: 0.0055\n",
            "Epoch 26/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0011 - val_loss: 0.0048\n",
            "Epoch 27/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0011 - val_loss: 0.0055\n",
            "Epoch 28/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0011 - val_loss: 0.0044\n",
            "Epoch 29/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0010 - val_loss: 0.0047\n",
            "Epoch 30/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0013 - val_loss: 0.0046\n",
            "Epoch 31/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0010 - val_loss: 0.0043\n",
            "Epoch 32/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0010 - val_loss: 0.0041\n",
            "Epoch 33/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0011 - val_loss: 0.0045\n",
            "Epoch 34/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.6611e-04 - val_loss: 0.0039\n",
            "Epoch 35/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.9999e-04 - val_loss: 0.0040\n",
            "Epoch 36/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.4484e-04 - val_loss: 0.0046\n",
            "Epoch 37/50\n",
            "787/787 [==============================] - 10s 12ms/sample - loss: 9.1941e-04 - val_loss: 0.0037\n",
            "Epoch 38/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.5433e-04 - val_loss: 0.0041\n",
            "Epoch 39/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.9832e-04 - val_loss: 0.0039\n",
            "Epoch 40/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.2661e-04 - val_loss: 0.0038\n",
            "Epoch 41/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 9.7544e-04 - val_loss: 0.0036\n",
            "Epoch 42/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 9.5265e-04 - val_loss: 0.0037\n",
            "Epoch 43/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 9.5286e-04 - val_loss: 0.0046\n",
            "Epoch 44/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 8.8289e-04 - val_loss: 0.0036\n",
            "Epoch 45/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 9.0382e-04 - val_loss: 0.0044\n",
            "Epoch 46/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 8.7296e-04 - val_loss: 0.0037\n",
            "Epoch 47/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 8.4345e-04 - val_loss: 0.0037\n",
            "Epoch 48/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 8.4016e-04 - val_loss: 0.0039\n",
            "Epoch 49/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 8.1748e-04 - val_loss: 0.0036\n",
            "Epoch 50/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 8.2040e-04 - val_loss: 0.0038\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 1 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 75s 95ms/sample - loss: 0.0253 - val_loss: 0.0114\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0023 - val_loss: 0.0104\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0021 - val_loss: 0.0096\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0020 - val_loss: 0.0085\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0019 - val_loss: 0.0100\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0020 - val_loss: 0.0096\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0019 - val_loss: 0.0092\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0019 - val_loss: 0.0092\n",
            "Epoch 9/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0018 - val_loss: 0.0092\n",
            "Epoch 00009: early stopping\n",
            "Running for Adam optimizer 0.001 learning_rate 16 batch_size and 2 replicate \n",
            "\n",
            "Train on 787 samples, validate on 193 samples\n",
            "Epoch 1/50\n",
            "787/787 [==============================] - 77s 98ms/sample - loss: 0.0293 - val_loss: 0.0103\n",
            "Epoch 2/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0025 - val_loss: 0.0111\n",
            "Epoch 3/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0022 - val_loss: 0.0091\n",
            "Epoch 4/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0021 - val_loss: 0.0092\n",
            "Epoch 5/50\n",
            "787/787 [==============================] - 10s 12ms/sample - loss: 0.0020 - val_loss: 0.0091\n",
            "Epoch 6/50\n",
            "787/787 [==============================] - 9s 11ms/sample - loss: 0.0020 - val_loss: 0.0092\n",
            "Epoch 7/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0020 - val_loss: 0.0094\n",
            "Epoch 8/50\n",
            "787/787 [==============================] - 8s 11ms/sample - loss: 0.0019 - val_loss: 0.0091\n",
            "Epoch 00008: early stopping\n",
            "Best_hyper_parameters: \n",
            " {'model': [200], 'optimizer': 'Adam', 'learning_rate': 0.001, 'batch_size': 16, 'best_avg_rmse': 22.911247495331782}\n",
            "all_avg_rmse: \n",
            " [[[36.67154704 44.3304732  39.78324748]\n",
            "  [33.34619388 29.44036766 26.7805073 ]\n",
            "  [24.93265191 23.45520915 22.9112475 ]]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model': [200],\n",
              " 'optimizer': 'Adam',\n",
              " 'learning_rate': 0.001,\n",
              " 'batch_size': 16,\n",
              " 'best_avg_rmse': 22.911247495331782}"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "layers = [200]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs =  50\n",
        "num_replicates = 3\n",
        "test_split = 0.2 \n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "N200_best_hyper_parameters = hyper_parameter_tuning(layers, train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "N200_best_hyper_parameters\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aA13pmJ6keN0"
      },
      "source": [
        "## **Building and Running  Single-Layer Models in Full Scale**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YTI2jGWgRQg8"
      },
      "outputs": [],
      "source": [
        "#========== Model hyper parameters settting ==========================#\n",
        "def LSTM_model(neurons, hyper_parameters, data, time_step , test_split , epochs ,  num_replicates ):\n",
        "\n",
        "  #====== data transformation==========#\n",
        "  print(\"Progress: Performing data preparation steps.......\\n\")\n",
        "\n",
        "  #======= creating training and test data===#\n",
        "\n",
        "  train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "  num_features = train_data.shape[1]\n",
        "  \n",
        "  min_train, max_train  = train_data[\"Terakhir\"].min(), train_data[\"Terakhir\"].max()\n",
        "  min_test, max_test   =  test_data[\"Terakhir\"].min(), test_data[\"Terakhir\"].max()\n",
        "\n",
        "  train_data_scaled  =  min_max_transform(train_data)\n",
        "  test_data_scaled   = min_max_transform(test_data)\n",
        "  \n",
        " \n",
        "  X_train, y_train  =   DatasetCreation(train_data_scaled, time_step)\n",
        "  X_test, y_test    =   DatasetCreation(test_data_scaled, time_step)\n",
        "\n",
        "  y_train_original  =  min_max_inverse_transform(y_train, min_train, max_train) #in original scale\n",
        "  y_test_original  =  min_max_inverse_transform(y_test, min_test, max_test) #in original scale\n",
        "  \n",
        "  \n",
        "  print(\"Progress: Building and training models.......\\n\")\n",
        "  \n",
        "  neurons = np.array(neurons)\n",
        "  #============ arrays for collecting test scores ================#\n",
        "  rmse_array = np.zeros((len(neurons), num_replicates))\n",
        "  #mae_array  = np.zeros((len(neurons), num_replicates))\n",
        "  mape_array = np.zeros((len(neurons), num_replicates))\n",
        "  #R2_array   = np.zeros((len(neurons), num_replicates))\n",
        "  R_array    = np.zeros((len(neurons), num_replicates))\n",
        "  elapsed_time_array = np.zeros((len(neurons), num_replicates))\n",
        "\n",
        "  \n",
        "  #========== array for collecting history and predictions =======#\n",
        "  models_history = []\n",
        "  train_predictions = []\n",
        "  test_predictions  = []\n",
        "\n",
        "  for i in range(len(neurons)):\n",
        "\n",
        "    print(\"Model hyperparameters used: \\n \", hyper_parameters[i])\n",
        "    #========== saving history and predictions per replicate=====#\n",
        "    model_history_per_replicate = []\n",
        "    train_predictions_per_replicate = []\n",
        "    test_predictions_per_replicate  = []\n",
        "     \n",
        "    hidden_nodes = np.int(neurons[i])\n",
        "\n",
        "   # print(\"Program is running for %d neurons ----->\\n\" %np.int(neurons[i]))\n",
        "  \n",
        "\n",
        "    for k in range(num_replicates):\n",
        "\n",
        "      print(\"Program is running for %d neurons and %d replicate ----->\\n\" %(hidden_nodes, k))\n",
        "       \n",
        "      \n",
        "\n",
        "      layers = [hidden_nodes]\n",
        "      \n",
        "\n",
        "      model = build_model(layers, time_step, num_features, optimizer = hyper_parameters[i][0], learning_rate = hyper_parameters[i][1], verbose = 0)\n",
        "      callback = tensorflow.keras.callbacks.EarlyStopping(monitor='loss', patience= 5,verbose=1)\n",
        "      # This callback will stop the training when there is no improvement in\n",
        "      # the loss for three consecutive epochs\n",
        "      start = time.time()\n",
        "      history = model.fit(X_train, y_train, batch_size = hyper_parameters[i][2], epochs= epochs, callbacks=[callback])\n",
        "      end = time.time()\n",
        "      elapsed_time = end - start\n",
        "      \n",
        "      model_history_per_replicate.append(history)\n",
        "\n",
        "\n",
        "      #==============Making train and test prediction in original scales ==========\n",
        "      train_pred   =  min_max_inverse_transform(model.predict(X_train).ravel(), min_train, max_train) #in original scale\n",
        "      test_pred    =  min_max_inverse_transform(model.predict(X_test).ravel(), min_test, max_test)\n",
        "\n",
        "      train_predictions_per_replicate.append(train_pred)\n",
        "      test_predictions_per_replicate.append(test_pred)\n",
        "      \n",
        "      #============== Calculating performance scores=========\n",
        "      scores =   calculate_scores(min_max_inverse_transform(y_test, min_test, max_test),test_pred)\n",
        "      rmse_array[i][k] = scores['rmse']\n",
        "      mape_array[i][k] =  scores['mape']\n",
        "      R_array[i][k] = scores['R']\n",
        "      elapsed_time_array[i][k] = elapsed_time\n",
        "\n",
        "    models_history.append(model_history_per_replicate)\n",
        "    train_predictions.append(train_predictions_per_replicate)\n",
        "    test_predictions.append(test_predictions_per_replicate)\n",
        "\n",
        "  print(\"Progress: Collecting outputs.......\\n\")\n",
        "\n",
        "  neurons_df = pd.DataFrame(neurons)\n",
        "  rmse_df = pd.DataFrame(rmse_array) \n",
        "  #mae_df  = pd.DataFrame(mae_array)\n",
        "  mape_df  = pd.DataFrame(mape_array) \n",
        "  #R2_df   = pd.DataFrame(R2_array) \n",
        "  R_df    = pd.DataFrame(R_array) \n",
        "  elapsed_time_df =  pd.DataFrame(elapsed_time_array)\n",
        "\n",
        "  train_predictions  = np.array(train_predictions)\n",
        "  test_predictions   = np.array(test_predictions)\n",
        " \n",
        "  #==== Idendifying  the best model results based on rmse ===============#\n",
        "  min_index = pd.DataFrame(rmse_df.min(axis = 1)).idxmin()[0] \n",
        "  min_col =   pd.DataFrame(rmse_df.min(axis = 0)).idxmin()[0]\n",
        "  \n",
        "  num_neurons_with_best_rmse = neurons_df.loc[min_index,0]\n",
        "\n",
        "  best_rmse = rmse_df.loc[min_index, min_col]\n",
        "  #mae_with_best_rmse = mae_df.loc[min_index, min_col]\n",
        "  mape_with_best_rmse = mape_df.loc[min_index, min_col]\n",
        "  #R2_with_best_rmse = R2_df.loc[min_index, min_col]\n",
        "  R_with_best_rmse =  R_df.loc[min_index, min_col]\n",
        "  elapsed_time_with_best_rmse = elapsed_time_df.loc[min_index, min_col]\n",
        "\n",
        "  train_predictions_with_best_rmse = train_predictions[min_index][min_col]\n",
        "  test_predictions_with_best_rmse = test_predictions[min_index][min_col]\n",
        "\n",
        "  loss_with_best_rmse = models_history[min_index][min_col].history['loss']\n",
        "  #val_loss_with_best_rmse = models_history[min_index][min_col].history['val_loss']\n",
        "\n",
        "  #======= Collecting hyperparameters=============#                           \n",
        "  hyper_parameters = { 'neurons': neurons,\n",
        "                       'model_specific_hyper_parameters': hyper_parameters,#additional best_hyper_parmeters for each models\n",
        "                       'epochs': epochs,\n",
        "                       'time_step':time_step,\n",
        "                       'num_replicates': num_replicates,\n",
        "                       'test_split':test_split\n",
        "                       #'validataion_split':validation_split\n",
        "                        }\n",
        "\n",
        "  #======= Collecting test scores =============#    \n",
        "  scores = {'neurons': neurons_df, 'rmse': rmse_df, 'mape': mape_df, 'R': R_df, 'elapsed_time': elapsed_time_df}\n",
        "\n",
        "  #======= Collecting average test scores =============#  \n",
        "  avg_scores = pd.DataFrame({'neurons': neurons,\n",
        "                            'rmse': rmse_df.mean(axis = 1),            \n",
        "                            'mape': mape_df.mean(axis = 1),                           \n",
        "                            'R': R_df.mean(axis = 1), \n",
        "                            'elapsed_time': elapsed_time_df.mean(axis = 1)})\n",
        "  \n",
        " #======= Collecting average test scores =============#  \n",
        "  all_stds = pd.DataFrame({'neurons': neurons,\n",
        "                            'rmse': rmse_df.std(axis = 1),                           \n",
        "                            'mape': mape_df.std(axis = 1),                             \n",
        "                            'R': R_df.std(axis = 1), \n",
        "                            'elapsed_time': elapsed_time_df.std(axis = 1)})\n",
        "  \n",
        "   \n",
        " #======= Collecting average test scores =============#  \n",
        "  all_minimums = pd.DataFrame({'neurons': neurons,\n",
        "                            'rmse': rmse_df.min(axis = 1),                          \n",
        "                            'mape': mape_df.min(axis = 1), \n",
        "                            'R': R_df.min(axis = 1), \n",
        "                            'elapsed_time': elapsed_time_df.min(axis = 1)})\n",
        "  \n",
        "  #======= Collecting average test scores =============#  \n",
        "  all_maximums = pd.DataFrame({'neurons': neurons,\n",
        "                            'rmse': rmse_df.max(axis = 1), \n",
        "                            'mape': mape_df.max(axis = 1), \n",
        "                            'R': R_df.max(axis = 1), \n",
        "                            'elapsed_time': elapsed_time_df.max(axis = 1)})\n",
        "  \n",
        "  \n",
        "\n",
        "  #======= Collecting the best model results =============#  \n",
        "  model_with_best_rmse = {  'neurons': num_neurons_with_best_rmse,\n",
        "                            'replicate': min_col,\n",
        "                            'rmse': best_rmse,\n",
        "                            'mape': mape_with_best_rmse,\n",
        "                            'R':  R_with_best_rmse,\n",
        "                            'elapsed_time': elapsed_time_with_best_rmse,\n",
        "                            'train_predictions':train_predictions_with_best_rmse,\n",
        "                            'test_predictions': test_predictions_with_best_rmse,\n",
        "                            'loss':loss_with_best_rmse,\n",
        "                            \n",
        "                         }\n",
        "\n",
        "  datasets  =    {'data': data, \n",
        "                  'X_train': X_train,\n",
        "                  'X_test': X_test,\n",
        "                  'y_train': y_train_original,\n",
        "                  'y_test': y_test_original\n",
        "                  }\n",
        "                \n",
        "  #======= Collecting all the outputs together =============#  \n",
        "  output_dictionary = { 'hyper_parameters': hyper_parameters,\n",
        "                        'best_model': model_with_best_rmse,\n",
        "                        'scores': scores,\n",
        "                        'avg_scores': avg_scores,\n",
        "                        'all_stds': all_stds,\n",
        "                        'all_minimums': all_minimums,\n",
        "                        'all_maximums': all_maximums,\n",
        "                        'train_predictions': train_predictions,\n",
        "                        'test_predictions':  test_predictions,\n",
        "                        'models_history': models_history,\n",
        "                        'datasets': datasets\n",
        "                       }\n",
        "  \n",
        "  print(\"\\nBest model (neurons, replicate, rmse): \", num_neurons_with_best_rmse, min_col, best_rmse)\n",
        "  print('\\nAverage scores:\\n', avg_scores)\n",
        "  print('\\nStandard_deviations:\\n', all_stds)\n",
        "  print('\\nMinimums:\\n', all_minimums)\n",
        "  print('\\nMaximums:\\n', all_maximums)\n",
        "  print(\"\\nProgress: All works are done successfully, congratulations!!\\n\")\n",
        "\n",
        " \n",
        "\n",
        "  #Save all rmses in a file for statistical study\n",
        "  scores['rmse'].to_csv(output_dir_path+'sl-lstm-all-rmse.csv')\n",
        "  \n",
        "  #writing output dictionary in the file\n",
        "  file_name = output_dir_path + \"sl-lstm-results.txt\"\n",
        "  write_dic_to_file(output_dictionary, file_name)\n",
        "\n",
        "  return (output_dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "esscOyB2nCG8"
      },
      "source": [
        "# **Building Multi-Layer LSTM Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hF61ZHIRdvyC"
      },
      "outputs": [],
      "source": [
        "def hyper_parameter_tuning_multilayer(layers, data, time_step, split, optimizers_names, learning_rates, batch_sizes, epochs, num_replicates = 2):\n",
        "  #======= creating training and test data===#\n",
        "  train_data, val_data = data_split(data, split)\n",
        "\n",
        "  num_features = train_data.shape[1]\n",
        "\n",
        "  min_train, max_train  = train_data[\"Close\"].min(), train_data[\"Close\"].max()\n",
        "  min_val, max_val   =    val_data[\"Close\"].min(), val_data[\"Close\"].max()\n",
        "\n",
        "  train_data_scaled  =  min_max_transform(train_data)\n",
        "  val_data_scaled    = min_max_transform(val_data)\n",
        "\n",
        "  X_train, y_train =   DatasetCreation(train_data_scaled, time_step)\n",
        "  X_val, y_val     =   DatasetCreation(val_data_scaled, time_step)\n",
        "   \n",
        "  #========dealing with time series=========#\n",
        "  \n",
        "  best_avg_rmse = 99999999999\n",
        "  \n",
        "  collect_rmse = []\n",
        "  \n",
        "  all_avg_rmse = np.zeros((len(optimizers_names), len(learning_rates), len(batch_sizes)))\n",
        "\n",
        "  best_hyper_parameters = {\"model\": layers, \"optimizer\": None, \"learning_rate\": None, \"batch_size\": None,\"best_avg_rmse\": None}\n",
        "  \n",
        " \n",
        "  for opt in range(len(optimizers_names)):\n",
        "    \n",
        "    for lr in range(len(learning_rates)):\n",
        "      \n",
        "      for batch_size in range(len(batch_sizes)):\n",
        "        \n",
        "        for i in range(num_replicates):\n",
        "\n",
        "          print(\"Running for \" + optimizers_names[opt] + \" optimizer \" + str(learning_rates[lr]) +  \" learning_rate \" +  str(batch_sizes[batch_size]) + \" batch_size and \" + str(i) +  \" replicate \" +  \"\\n\")\n",
        "                \n",
        "          model = build_model(layers, time_step, num_features, optimizers_names[opt], learning_rate = learning_rates[lr], verbose = 0)\n",
        "          \n",
        "          callback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience= 5,verbose=1)\n",
        "\n",
        "          history = model.fit(X_train, y_train, batch_size = batch_sizes[batch_size], epochs= epochs, validation_data = (X_val, y_val), callbacks=[callback])\n",
        "                  \n",
        "\n",
        "          #==============Making predictions in original scale ==========\n",
        "          #train_pred  =  min_max_inverse_transform(model.predict(X_train).ravel(), min_train, max_train) \n",
        "          val_pred    =  min_max_inverse_transform(model.predict(X_val).ravel(), min_val, max_val)\n",
        "\n",
        "          #train_scores =  calculate_scores(min_max_inverse_transform(y_train, min_train, max_train),train_pred)\n",
        "          #scores =   calculate_scores(min_max_inverse_transform(y_val, min_val, max_val),val_pred)\n",
        "\n",
        "          collect_rmse.append(math.sqrt(mean_squared_error(min_max_inverse_transform(y_val, min_val, max_val),val_pred)))\n",
        "                  \n",
        "        avg_rmse = np.mean(np.array(collect_rmse))\n",
        "        all_avg_rmse[opt][lr][batch_size] = avg_rmse\n",
        "\n",
        "        if avg_rmse < best_avg_rmse:\n",
        "          best_avg_rmse = avg_rmse\n",
        "          best_hyper_parameters = {\"model\": layers,  \"optimizer\": optimizers_names[opt], \"learning_rate\": learning_rates[lr], \"batch_size\": batch_sizes[batch_size], \"best_avg_rmse\": best_avg_rmse} \n",
        "\n",
        "\n",
        "  output_dictionary = {\n",
        "      \"best_hyper_parameters\":  best_hyper_parameters,\n",
        "      \"all_avg_rmse\": all_avg_rmse\n",
        "       } \n",
        "\n",
        "  #writing output dictionary in the file\n",
        "\n",
        "  file_name = output_dir_path+ \"ml-lstm-\" + str(layers)+ \"-neurons-validation_results.txt\"\n",
        "  write_dic_to_file(output_dictionary, file_name)\n",
        "\n",
        "  print(\"Best_hyper_parameters: \\n\", output_dictionary['best_hyper_parameters']) \n",
        "  print(\"all_avg_rmse: \\n\", output_dictionary['all_avg_rmse'])\n",
        "\n",
        "  return output_dictionary['best_hyper_parameters']\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KhO55nHFgPU6"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "723GijDtjUcd",
        "outputId": "21034062-bc52-4f0f-cd47-0152c30064db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 5, 10)             840       \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 5)                 320       \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 1,166\n",
            "Trainable params: 1,166\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.sequential.Sequential at 0x294538e7748>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "layers = [[10, 5]]\n",
        "optimizers_names = ['Adam', 'Adagrad', 'Nadam']\n",
        "build_model(layers[0], 5, 10, optimizers_names[2], 0.001, 1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DA34iwsmiKEl"
      },
      "source": [
        "## **Case I: Tuning Hyperparameters of 10-5N LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX3YK-hOeFDZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#hidden_layers = [[10, 5], [20, 10], [50, 20], [100, 50], [150, 100], [100, 50, 20]]\n",
        "layers = [[10, 5]]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam', 'Adagrad', 'Nadam']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs =  2\n",
        "num_replicates = 2\n",
        "test_split = 0.2 \n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "N10_5__best_hyper_parameters = hyper_parameter_tuning_multilayer(layers[0], train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "N10_5__best_hyper_parameters\n",
        "\"\"\"\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9PUeeIF7nQq1"
      },
      "source": [
        "## **Case II: Tuning Hyperparameters of 20-10N LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CE5fU4HVnKH9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#hidden_layers = [[10, 5], [20, 10], [50, 20], [100, 50], [150, 100], [100, 50, 20]]\n",
        "layers = [[20, 10]]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam', 'Adagrad', 'Nadam']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs =  2\n",
        "num_replicates = 2\n",
        "test_split = 0.2 \n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "N20_10__best_hyper_parameters = hyper_parameter_tuning_multilayer(layers[0], train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "N20_10__best_hyper_parameters\n",
        "\"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7rkakLo_nmOK"
      },
      "source": [
        "## **Case III: Tuning Hyperparameters of 50-20N LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KS3h5fzSnlJD"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#hidden_layers = [[10, 5], [20, 10], [50, 20], [100, 50], [150, 100], [100, 50, 20]]\n",
        "layers = [[50, 20]]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam', 'Adagrad', 'Nadam']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs =  2\n",
        "num_replicates = 2\n",
        "test_split = 0.2 \n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "N50_20__best_hyper_parameters = hyper_parameter_tuning_multilayer(layers[0], train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "N50_20__best_hyper_parameters\n",
        "\"\"\"\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ueI4qZkrn6DR"
      },
      "source": [
        "## **Case IV: Tuning Hyperparameters of 100-50N LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8OQluGrn-BR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#hidden_layers = [[10, 5], [20, 10], [50, 20], [100, 50], [150, 100], [100, 50, 20]]\n",
        "layers = [[100, 50]]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam', 'Adagrad', 'Nadam']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs =  2\n",
        "num_replicates = 2\n",
        "test_split = 0.2 \n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "N100_50__best_hyper_parameters = hyper_parameter_tuning_multilayer(layers[0], train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "N100_50__best_hyper_parameters\n",
        "\"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JEAwGO5zoZKN"
      },
      "source": [
        "## **Case V: Tuning Hyperparameters of 150-100N LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPD3Z1Y9oYxc"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#hidden_layers = [[10, 5], [20, 10], [50, 20], [100, 50], [150, 100], [100, 50, 20]]\n",
        "layers = [[150, 100]]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam', 'Adagrad', 'Nadam']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs =  2\n",
        "num_replicates = 2\n",
        "test_split = 0.2 \n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "N150_100__best_hyper_parameters = hyper_parameter_tuning_multilayer(layers[0], train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "N150_100__best_hyper_parameters\n",
        "\"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q8klpfxhot5y"
      },
      "source": [
        "## **Case VI: Tuning Hyperparameters of 100-50-20N LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jClFlbJUouiV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#hidden_layers = [[10, 5], [20, 10], [50, 20], [100, 50], [150, 100], [100, 50, 20]]\n",
        "layers = [[100, 50, 20]]\n",
        "time_step = 5\n",
        "optimizers_names = ['Adam', 'Adagrad', 'Nadam']\n",
        "learning_rates =  [0.1, 0.01, 0.001]\n",
        "batch_sizes =  [4, 8, 16]\n",
        "epochs =  2\n",
        "num_replicates = 2\n",
        "test_split = 0.2 \n",
        "val_split = 0.2\n",
        "\n",
        "train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "N100_50_20__best_hyper_parameters = hyper_parameter_tuning_multilayer(layers[0], train_data, time_step, val_split, optimizers_names, learning_rates, batch_sizes, epochs = epochs, num_replicates = num_replicates)\n",
        "N100_50_20__best_hyper_parameters\n",
        "\"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b40CrlyMcrlv"
      },
      "source": [
        "## **Building and Running Multi-Layers LSTM Models in Full Scale**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RbAtH3KenO9D"
      },
      "outputs": [],
      "source": [
        "def multi_layer_LSTM_Model(layers, hyper_parameters, data, time_step = 5, test_split = 0.2, epochs = 5,  num_replicates = 2):\n",
        "    #====== data transformation==========#\n",
        "    print(\"Progress: Performing data preparation steps.......\\n\")\n",
        "\n",
        "    #======= creating training and test data===#\n",
        "\n",
        "    train_data, test_data = data_split(data, test_split)\n",
        "\n",
        "    num_features = train_data.shape[1]\n",
        "  \n",
        "    min_train, max_train  = train_data[\"Close\"].min(), train_data[\"Close\"].max()\n",
        "    min_test, max_test   =  test_data[\"Close\"].min(), test_data[\"Close\"].max()\n",
        "\n",
        "    train_data_scaled  =  min_max_transform(train_data)\n",
        "    test_data_scaled   = min_max_transform(test_data)\n",
        "  \n",
        " \n",
        "    X_train, y_train  =   DatasetCreation(train_data_scaled, time_step)\n",
        "    X_test, y_test    =   DatasetCreation(test_data_scaled, time_step)\n",
        "\n",
        "    y_train_original  =  min_max_inverse_transform(y_train, min_train, max_train) #in original scale\n",
        "    y_test_original  =  min_max_inverse_transform(y_test, min_test, max_test) #in original scale\n",
        "  \n",
        "  \n",
        "    #============ arrays for collecting test scores ================#\n",
        "    rmse_array = np.zeros(num_replicates)\n",
        "    mape_array = np.zeros(num_replicates)\n",
        "    R_array    = np.zeros(num_replicates)\n",
        "    elapsed_time_array = np.zeros(num_replicates)\n",
        "\n",
        "    models_history = []\n",
        "    train_predictions = []\n",
        "    test_predictions = []\n",
        "\n",
        "    for i in range(num_replicates):\n",
        "\n",
        "      print(\"Program is running for %d replicate ----->\\n\" %i)\n",
        "     \n",
        "      model = build_model(layers, time_step, num_features, optimizer = hyper_parameters[0], learning_rate = hyper_parameters[1], verbose = 0)\n",
        "      callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience= 5)\n",
        "      # This callback will stop the training when there is no improvement in\n",
        "      # the loss for three consecutive epochs\n",
        "      start = time.time()\n",
        "      history = model.fit(X_train, y_train, batch_size = hyper_parameters[2], epochs= epochs, callbacks=[callback], verbose = 1)\n",
        "      end = time.time()\n",
        "      elapsed_time = end - start\n",
        "      \n",
        "      models_history.append(history)\n",
        "\n",
        "\n",
        "      #==============Making train and test prediction in original scales ==========\n",
        "      train_pred   =  min_max_inverse_transform(model.predict(X_train).ravel(), min_train, max_train) #in original scale\n",
        "      test_pred    =  min_max_inverse_transform(model.predict(X_test).ravel(), min_test, max_test)\n",
        "\n",
        "      train_predictions.append(train_pred)\n",
        "      test_predictions.append(test_pred)\n",
        "      \n",
        "      #============== Calculating performance scores==========\n",
        "      scores =   calculate_scores(min_max_inverse_transform(y_test, min_test, max_test),test_pred)\n",
        "\n",
        "      rmse_array[i] =  scores['rmse']\n",
        "      mape_array[i] =  scores['mape']\n",
        "      R_array[i] = scores['R']\n",
        "      elapsed_time_array[i] = elapsed_time\n",
        "\n",
        "    min_index = rmse_array.argmin()\n",
        "    best_rmse = rmse_array[min_index]\n",
        "    mape_with_best_rmse = mape_array[min_index]\n",
        "    R_with_best_rmse =  R_array[min_index]\n",
        "    elapsed_time_with_best_rmse = elapsed_time_array[min_index]\n",
        "\n",
        "    train_predictions_with_best_rmse = train_predictions[min_index]\n",
        "    test_predictions_with_best_rmse = test_predictions[min_index]\n",
        "\n",
        "    loss_with_best_rmse = models_history[min_index].history['loss']\n",
        "\n",
        "\n",
        "    #======= Collecting average test scores =============# \n",
        "    all_scores = {'rmse': rmse_array, 'mape': mape_array, 'R': R_array, 'elapsed_time': elapsed_time_array}\n",
        "\n",
        "    avg_scores = {'rmse': np.mean(rmse_array),\n",
        "                  'mape': np.mean(mape_array),\n",
        "                  'R': np.mean(R_array), \n",
        "                  'elapsed_time': np.mean(elapsed_time_array)}\n",
        "\n",
        "    #======= Collecting standard deviations of scores =============#  \n",
        "    stds = {'rmse': np.std(rmse_array),\n",
        "              'mape': np.std(mape_array),\n",
        "                  'R': np.std(R_array), \n",
        "                  'elapsed_time': np.std(elapsed_time_array)}\n",
        "   \n",
        "    #======= Collecting minimum values of test scores =============#  \n",
        "    minimums = {'rmse': np.min(rmse_array),\n",
        "                'mape': np.min(mape_array),\n",
        "                'R': np.min(R_array), \n",
        "                'elapsed_time': np.min(elapsed_time_array)}\n",
        "   \n",
        "  \n",
        "     \n",
        "    #======= Collecting maximum values of test scores =============#  \n",
        "    maximums = {'rmse': np.max(rmse_array),\n",
        "                'mape': np.max(mape_array),\n",
        "                'R': np.max(R_array), \n",
        "                'elapsed_time': np.max(elapsed_time_array)}\n",
        "   \n",
        "  \n",
        "    model_with_best_rmse = {\n",
        "                            'replicate': min_index,\n",
        "                            'rmse': best_rmse,\n",
        "                            'mape': mape_with_best_rmse,\n",
        "                            'R':  R_with_best_rmse,\n",
        "                            'elapsed_time': elapsed_time_with_best_rmse,\n",
        "                            'train_predictions':train_predictions_with_best_rmse,\n",
        "                            'test_predictions': test_predictions_with_best_rmse,\n",
        "                            'loss':loss_with_best_rmse,\n",
        "                         }\n",
        "     #======= Collecting hyperparameters=============#                           \n",
        "    hyper_parameters = {'layers': layers,\n",
        "                        'model_specific_hyper_parameters': hyper_parameters,#additional best_hyper_parmeters for each models\n",
        "                       'epochs': epochs,\n",
        "                       'time_step':time_step,\n",
        "                       'num_replicates': num_replicates,\n",
        "                       'test_split':test_split\n",
        "                        }\n",
        "\n",
        "                \n",
        "    \n",
        "    datasets  =   {'data': data, \n",
        "                  'X_train': X_train,\n",
        "                  'X_test': X_test,\n",
        "                  'y_train': y_train_original,\n",
        "                  'y_test': y_test_original\n",
        "                  }\n",
        "                \n",
        "    #======= Collecting all the outputs together =============#  \n",
        "    output_dictionary = {'hyper_parameters': hyper_parameters,\n",
        "                        'best_model': model_with_best_rmse,\n",
        "                        'all_scores': all_scores,\n",
        "                        'avg_scores': avg_scores,\n",
        "                        'standard deviations': stds,\n",
        "                        'minimums': minimums, \n",
        "                        'maximums': maximums, \n",
        "                        'train_predictions': train_predictions,\n",
        "                        'test_predictions':  test_predictions,\n",
        "                        'datasets': datasets \n",
        "                       }\n",
        "    \n",
        "    return output_dictionary\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sb2MaY_kynwh"
      },
      "source": [
        "## **Training Multiple Models Together**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1GugZupYxeSc"
      },
      "outputs": [],
      "source": [
        "def run_multi_layer_LSTM_Model(hidden_layers, hyper_parameters, data, time_step = 5, test_split = 0.2, epochs = 5,  num_replicates = 2):\n",
        "   \n",
        "  num_models = len(hidden_layers)\n",
        "\n",
        "  #== to collect all scores===#\n",
        "  rmse = []\n",
        "  mape = []\n",
        "  R = []\n",
        "  elapsed_time = []\n",
        "\n",
        "  #===to collect all avg scores===#\n",
        "  avg_rmse = []\n",
        "  avg_mape = []\n",
        "  avg_R = []\n",
        "  avg_elapsed_time = []\n",
        "  \n",
        "  #=== to iteratively update the best rmse and the corresponding model\n",
        "  best_avg_rmse = 99999999999\n",
        "  best_rmse = 99999999999\n",
        "  best_model_hidden_layers = None\n",
        "  best_model_output = None\n",
        "  \n",
        "  for i in range(num_models):\n",
        "    print(\"Running model with hidden neurons: \", hidden_layers[i])\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"Best Hyper_parameters used: \", hyper_parameters[i])\n",
        "    \n",
        "    print(\"\\n\")\n",
        "\n",
        "    output = multi_layer_LSTM_Model(hidden_layers[i], hyper_parameters[i], data, time_step, test_split, epochs, num_replicates)\n",
        "    \n",
        "    rmse.append(output['all_scores']['rmse'])\n",
        "    mape.append(output['all_scores']['mape'])\n",
        "    R.append(output['all_scores']['R'])\n",
        "    elapsed_time.append(output['all_scores']['elapsed_time'])\n",
        "\n",
        "    avg_rmse.append(output['avg_scores']['rmse'])\n",
        "    avg_mape.append(output['avg_scores']['mape'])\n",
        "    avg_R.append(output['avg_scores']['R'])\n",
        "    avg_elapsed_time.append(output['avg_scores']['elapsed_time'])\n",
        "  \n",
        "    if avg_rmse[i] < best_avg_rmse:\n",
        "      best_avg_rmse = avg_rmse[i]\n",
        "      best_rmse = output['best_model']['rmse']\n",
        "      best_model_hidden_layers = hidden_layers[i] \n",
        "      best_model_output = output\n",
        "\n",
        "\n",
        "  rmse = np.array(rmse)\n",
        "  mape = np.array(mape)\n",
        "  R =  np.array(R)\n",
        "\n",
        "  # ===== Collecting all  scores================================#\n",
        "  scores = {'layers': hidden_layers, 'rmse': rmse, 'mape': mape, 'R':R, 'elapsed_time': elapsed_time}\n",
        "\n",
        "  # ======= Collecting avg scores ===============================#\n",
        "  avg_scores = pd.DataFrame({'layers': hidden_layers, 'rmse': np.array(avg_rmse), 'mape': np.array(avg_mape), 'R':np.array(avg_R), 'elapsed_time':np.array(avg_elapsed_time)})\n",
        "  \n",
        "  #======= Collecting standard deviations of scores =============#  \n",
        "  stds = pd.DataFrame({'layers': hidden_layers, 'rmse': np.std(rmse, axis = 1), 'mape': np.std(mape, axis = 1), 'R':  np.std(R, axis = 1 ),  'elapsed_time': np.std(elapsed_time, axis = 1 )})\n",
        "   \n",
        "  #======= Collecting minimum values of test scores =============#  \n",
        "  minimums = pd.DataFrame({'layers': hidden_layers, 'rmse': np.min(rmse, axis =1 ), 'mape': np.min(mape, axis= 1), 'R': np.min(R, axis =1), 'elapsed_time': np.min(elapsed_time, axis =1)})\n",
        "     \n",
        "  #======= Collecting maximum values of test scores =============#  \n",
        "  maximums = pd.DataFrame({'layers': hidden_layers, 'rmse': np.max(rmse, axis =1), 'mape': np.max(mape, axis =1), 'R': np.max(R, axis =1),  'elapsed_time': np.max(elapsed_time,axis =1)})\n",
        "\n",
        "  output_dictionary = {\n",
        "                     'hyper_parameters': hyper_parameters[i],\n",
        "                     'scores': scores,\n",
        "                     'avg_scores': avg_scores,\n",
        "                     'stds':stds,\n",
        "                     'minimums': minimums,\n",
        "                     'maximums': maximums,\n",
        "                      'best_avg_rmse': best_avg_rmse, \n",
        "                      'best_rmse': best_rmse, \n",
        "                      'best_model_hidden_layers': best_model_hidden_layers, \n",
        "                      'best_model_output': best_model_output                                \n",
        "                      }\n",
        "\n",
        "\n",
        "  print(\"\\nBest model and its avg rmse and minimum rmse):\\n\", best_model_hidden_layers, best_avg_rmse, best_rmse)\n",
        "  print('\\nAverage scores:\\n', avg_scores)\n",
        "  print('\\nStandard_deviations:\\n', stds)\n",
        "  print('\\nMinimums:\\n', minimums)\n",
        "  print('\\nMaximums:\\n', maximums)\n",
        "\n",
        "  \n",
        "  #Save all rmse scores in the file for future analysis \n",
        "  pd.DataFrame(scores['rmse']).to_csv(output_dir_path+'ml-lstm-all-rmse.csv')\n",
        "\n",
        "  #writing output dictionary in the file\n",
        "  file_name = output_dir_path + \"ml-lstm-results.txt\"\n",
        "  write_dic_to_file(output_dictionary, file_name)\n",
        "\n",
        "  print(\"Progress: All works are done successfully, congratulations!!\\n\")\n",
        "  \n",
        "  return output_dictionary                                 \n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fnSVFxltgwGi"
      },
      "source": [
        "# **Functions for Visualizing Multilayer Scores Boxplots**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JdkLSYHeoNa4"
      },
      "outputs": [],
      "source": [
        "def multi_layers_all_scores_boxplots(model_output):\n",
        "\n",
        "  fig = plt.figure(figsize = (18,5))\n",
        "  plt.subplot(131)\n",
        "  p1 = plt.boxplot(pd.DataFrame(model_output['scores']['rmse']), patch_artist= True )\n",
        "  for i, box in enumerate(p1['boxes']):\n",
        "    #change outline color\n",
        "    box.set(color= 'blue', linewidth = 1.2)\n",
        "    # change fill color\n",
        "    box.set(facecolor = 'mediumblue')\n",
        "  plt.xticks([1,2,3,4,5,6], ['(10, 5)', '(20, 10)', '(50, 20)', '(100, 50)', '(150, 100)', '(100, 50, 20)'], rotation = 30)\n",
        "  plt.title(\"(a)\")\n",
        "  plt.xlabel('Number of neurons')\n",
        "  plt.ylabel('RMSE')\n",
        "\n",
        "  plt.subplot(132)\n",
        "  p2 = plt.boxplot(pd.DataFrame(model_output['scores']['mape']), patch_artist= True )\n",
        "  for i, box in enumerate(p2['boxes']):\n",
        "    #change outline color\n",
        "    box.set(color= 'blue', linewidth = 1.2)\n",
        "    # change fill color\n",
        "    box.set(facecolor = 'indigo')\n",
        "  plt.xticks([1,2,3,4,5,6], ['(10, 5)', '(20, 10)', '(50, 20)', '(100, 50)', '(150, 100)', '(100, 50, 20)'],  rotation = 30)\n",
        "  plt.title(\"(b)\")\n",
        "  plt.xlabel('Number of neurons')\n",
        "  plt.ylabel('MAPE')\n",
        "\n",
        "  plt.subplot(133)\n",
        "  p3 = plt.boxplot(pd.DataFrame(model_output['scores']['R']), patch_artist= True )\n",
        "  for i, box in enumerate(p3['boxes']):\n",
        "    #change outline color\n",
        "    box.set(color= 'blue', linewidth = 1.2)\n",
        "    # change fill color\n",
        "    box.set(facecolor = 'darkgreen')\n",
        "\n",
        "  plt.xticks([1,2,3,4,5,6], ['(10, 5)', '(20, 10)', '(50, 20)', '(100, 50)', '(150, 100)', '(100, 50, 20)'],  rotation = 30)\n",
        "  plt.title(\"(c)\")\n",
        "  plt.xlabel('Number of neurons')\n",
        "  plt.ylabel('R')\n",
        "  \n",
        "  fig.savefig(output_dir_path+ \"multi_layers_all_scores_boxplots.png\",dpi=600)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XU9whQmJ-FdN"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "\n",
        "def read_df_from_file(file_name):\n",
        "   return pd.read_csv(file_name)\n",
        "\n",
        "def comparative_qq_plots(data1, data2):\n",
        "  fig = plt.figure(figsize = (14,5))\n",
        "  ax1= fig.add_subplot(121)\n",
        "  scipy.stats.probplot(data1, dist=scipy.stats.norm, sparams=(0,1), plot=ax1)\n",
        "  ax1.get_lines()[0].set_marker('o')\n",
        "  ax1.get_lines()[0].set_markerfacecolor('mediumblue')\n",
        "  ax1.get_lines()[0].set_markersize(8.0)\n",
        "  ax1.get_lines()[1].set_linewidth(3.0)\n",
        "  plt.title(\"(a)\")\n",
        "  \n",
        "  ax2= fig.add_subplot(122)\n",
        "  scipy.stats.probplot(data2, dist=scipy.stats.norm, sparams=(0,1), plot=ax2)\n",
        "  ax2.get_lines()[0].set_marker('o')\n",
        "  ax2.get_lines()[0].set_markerfacecolor('mediumblue')\n",
        "  ax2.get_lines()[0].set_markersize(8.0)\n",
        "  ax2.get_lines()[1].set_linewidth(3.0)\n",
        "  plt.title(\"(b)\")\n",
        "\n",
        "  fig.savefig(output_dir_path+ \"Errors_QQ_Plots.png\",dpi=600)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def comparative_boxplots(data1, data2):\n",
        "  data = pd.DataFrame()\n",
        "  data['Single-layer-LSTM'] = data1\n",
        "  data['Multi-layer-LSTM'] = data2\n",
        "\n",
        "  fig = plt.figure(figsize = (6,4))\n",
        "  p = plt.boxplot(data.T, patch_artist= True)\n",
        "  colors = ['mediumblue', 'darkred']\n",
        "  for i, box in enumerate(p['boxes']):\n",
        "    # change outline color\n",
        "    box.set(color= 'blue', linewidth = 1.2)\n",
        "    # change fill color\n",
        "    box.set(facecolor = colors[i])\n",
        "\n",
        "  plt.xticks([1,2], ['Single-layer LSTM','Multi-layer LSTM'])\n",
        "  plt.ylabel('RMSE')\n",
        "  fig.savefig(output_dir_path+\"comparative_boxplots.png\",dpi=600)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def perform_normality_test(data1, data2):\n",
        "\n",
        "  print(\"Performaing Normality Tests\\n\")\n",
        "  print(\"Data1: \")\n",
        "  print(scipy.stats.normaltest(data1))\n",
        "  print(\"Data 2:\")\n",
        "  print(scipy.stats.normaltest(data2))\n",
        "\n",
        "#=========F test for equality of variances =====================================\n",
        "def ftest(data1, data2):\n",
        "    x = np.array(data1)\n",
        "    y = np.array(data2)\n",
        "    f = np.var(x, ddof=1)/np.var(y, ddof=1) #calculate F test statistic \n",
        "    dfn = x.size-1 #define degrees of freedom numerator \n",
        "    dfd = y.size-1 #define degrees of freedom denominator \n",
        "    p = 1-scipy.stats.f.cdf(f, dfn, dfd) #find p-value of F test statistic \n",
        "    return f, p\n",
        "\n",
        "\n",
        "def perform_ttest(data1, data2):\n",
        "  #student_ttest, pvalue_student=scipy.stats.ttest_ind(data1, data2, equal_var = True) # It requires variences to be equal.\n",
        "  print(\"\\nTwo-sample ttest\")\n",
        "  print(scipy.stats.ttest_ind(data1, data2, equal_var = False))  # It does not require variences to be equal.\n",
        "  \n",
        "\n",
        "def perform_statistical_analysis(data1, data2):\n",
        "  comparative_qq_plots(data1, data2)\n",
        "  comparative_boxplots(data1, data2)\n",
        "  perform_normality_test(data1, data2)\n",
        "  perform_ttest(data1, data2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fuzAIoWqzDGX"
      },
      "source": [
        "# **Final Step: Models Executions and Results Visualizatoin**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gU_3lOc2l6oN"
      },
      "source": [
        "### **Case I: Executing Single-Layer Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FsxPTsekmClV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: Performing data preparation steps.......\n",
            "\n",
            "Progress: Building and training models.......\n",
            "\n",
            "Model hyperparameters used: \n",
            "  ['Adam', 0.01, 16]\n",
            "Program is running for 30 neurons and 0 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "986/986 [==============================] - 5s 5ms/sample - loss: 0.0222\n",
            "Epoch 2/50\n",
            "986/986 [==============================] - 1s 555us/sample - loss: 0.0018\n",
            "Epoch 3/50\n",
            "986/986 [==============================] - 1s 553us/sample - loss: 0.0018\n",
            "Epoch 4/50\n",
            "986/986 [==============================] - 1s 512us/sample - loss: 0.0016\n",
            "Epoch 5/50\n",
            "986/986 [==============================] - 1s 663us/sample - loss: 0.0015\n",
            "Epoch 6/50\n",
            "986/986 [==============================] - 1s 624us/sample - loss: 0.0016\n",
            "Epoch 7/50\n",
            "986/986 [==============================] - 1s 614us/sample - loss: 0.0013\n",
            "Epoch 8/50\n",
            "986/986 [==============================] - 1s 567us/sample - loss: 0.0013\n",
            "Epoch 9/50\n",
            "986/986 [==============================] - 1s 523us/sample - loss: 0.0014\n",
            "Epoch 10/50\n",
            "986/986 [==============================] - 1s 716us/sample - loss: 0.0013\n",
            "Epoch 11/50\n",
            "986/986 [==============================] - 1s 594us/sample - loss: 0.0012\n",
            "Epoch 12/50\n",
            "986/986 [==============================] - 1s 623us/sample - loss: 0.0011\n",
            "Epoch 13/50\n",
            "986/986 [==============================] - 1s 642us/sample - loss: 0.0012\n",
            "Epoch 14/50\n",
            "986/986 [==============================] - 1s 712us/sample - loss: 0.0011\n",
            "Epoch 15/50\n",
            "986/986 [==============================] - 1s 919us/sample - loss: 0.0010\n",
            "Epoch 16/50\n",
            "986/986 [==============================] - 1s 681us/sample - loss: 0.0010\n",
            "Epoch 17/50\n",
            "986/986 [==============================] - 1s 563us/sample - loss: 0.0010\n",
            "Epoch 18/50\n",
            "986/986 [==============================] - 1s 736us/sample - loss: 0.0011A: 0s - loss: 9.930\n",
            "Epoch 19/50\n",
            "986/986 [==============================] - 1s 649us/sample - loss: 9.3723e-04\n",
            "Epoch 20/50\n",
            "986/986 [==============================] - 1s 552us/sample - loss: 9.3937e-04\n",
            "Epoch 21/50\n",
            "986/986 [==============================] - 1s 601us/sample - loss: 0.0010\n",
            "Epoch 22/50\n",
            "986/986 [==============================] - 1s 512us/sample - loss: 9.7709e-04\n",
            "Epoch 23/50\n",
            "986/986 [==============================] - 1s 739us/sample - loss: 8.4898e-04\n",
            "Epoch 24/50\n",
            "986/986 [==============================] - 1s 653us/sample - loss: 9.0822e-04\n",
            "Epoch 25/50\n",
            "986/986 [==============================] - 1s 573us/sample - loss: 9.6816e-04\n",
            "Epoch 26/50\n",
            "986/986 [==============================] - 1s 660us/sample - loss: 8.7818e-04\n",
            "Epoch 27/50\n",
            "986/986 [==============================] - 1s 621us/sample - loss: 8.2359e-04\n",
            "Epoch 28/50\n",
            "986/986 [==============================] - 1s 567us/sample - loss: 8.9903e-04\n",
            "Epoch 29/50\n",
            "986/986 [==============================] - 1s 511us/sample - loss: 7.9955e-04\n",
            "Epoch 30/50\n",
            "986/986 [==============================] - 1s 566us/sample - loss: 9.3239e-04\n",
            "Epoch 31/50\n",
            "986/986 [==============================] - 1s 549us/sample - loss: 9.2118e-04\n",
            "Epoch 32/50\n",
            "986/986 [==============================] - 1s 658us/sample - loss: 8.2481e-04\n",
            "Epoch 33/50\n",
            "986/986 [==============================] - 1s 570us/sample - loss: 8.6458e-04\n",
            "Epoch 34/50\n",
            "986/986 [==============================] - 0s 497us/sample - loss: 0.0011\n",
            "Epoch 00034: early stopping\n",
            "Program is running for 30 neurons and 1 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "986/986 [==============================] - 4s 4ms/sample - loss: 0.0406\n",
            "Epoch 2/50\n",
            "986/986 [==============================] - 1s 562us/sample - loss: 0.0018\n",
            "Epoch 3/50\n",
            "986/986 [==============================] - 1s 527us/sample - loss: 0.0015\n",
            "Epoch 4/50\n",
            "986/986 [==============================] - 1s 654us/sample - loss: 0.0015s - lo\n",
            "Epoch 5/50\n",
            "986/986 [==============================] - 1s 579us/sample - loss: 0.0015\n",
            "Epoch 6/50\n",
            "986/986 [==============================] - 1s 749us/sample - loss: 0.0014\n",
            "Epoch 7/50\n",
            "986/986 [==============================] - 1s 580us/sample - loss: 0.0014\n",
            "Epoch 8/50\n",
            "986/986 [==============================] - 1s 575us/sample - loss: 0.0013\n",
            "Epoch 9/50\n",
            "986/986 [==============================] - 1s 575us/sample - loss: 0.0015\n",
            "Epoch 10/50\n",
            "986/986 [==============================] - 1s 783us/sample - loss: 0.0012\n",
            "Epoch 11/50\n",
            "986/986 [==============================] - 1s 596us/sample - loss: 0.0012\n",
            "Epoch 12/50\n",
            "986/986 [==============================] - 1s 572us/sample - loss: 0.0012\n",
            "Epoch 13/50\n",
            "986/986 [==============================] - 1s 570us/sample - loss: 0.0014\n",
            "Epoch 14/50\n",
            "986/986 [==============================] - 1s 693us/sample - loss: 0.0011\n",
            "Epoch 15/50\n",
            "986/986 [==============================] - 1s 592us/sample - loss: 0.0012\n",
            "Epoch 16/50\n",
            "986/986 [==============================] - 1s 553us/sample - loss: 0.0011\n",
            "Epoch 17/50\n",
            "986/986 [==============================] - 1s 564us/sample - loss: 9.7699e-04loss\n",
            "Epoch 18/50\n",
            "986/986 [==============================] - 1s 747us/sample - loss: 9.3406e-04\n",
            "Epoch 19/50\n",
            "986/986 [==============================] - 1s 565us/sample - loss: 9.8728e-04\n",
            "Epoch 20/50\n",
            "986/986 [==============================] - 1s 567us/sample - loss: 9.0509e-04\n",
            "Epoch 21/50\n",
            "986/986 [==============================] - 1s 567us/sample - loss: 8.6014e-04\n",
            "Epoch 22/50\n",
            "986/986 [==============================] - 1s 582us/sample - loss: 8.6266e-04\n",
            "Epoch 23/50\n",
            "986/986 [==============================] - 1s 795us/sample - loss: 0.0010\n",
            "Epoch 24/50\n",
            "986/986 [==============================] - 1s 599us/sample - loss: 8.6437e-04\n",
            "Epoch 25/50\n",
            "986/986 [==============================] - 1s 598us/sample - loss: 8.1862e-04\n",
            "Epoch 26/50\n",
            "986/986 [==============================] - 1s 604us/sample - loss: 8.6398e-04\n",
            "Epoch 27/50\n",
            "986/986 [==============================] - 1s 698us/sample - loss: 8.3815e-04\n",
            "Epoch 28/50\n",
            "986/986 [==============================] - 1s 579us/sample - loss: 8.5464e-04\n",
            "Epoch 29/50\n",
            "986/986 [==============================] - 1s 583us/sample - loss: 8.5540e-04\n",
            "Epoch 30/50\n",
            "986/986 [==============================] - 1s 592us/sample - loss: 9.1352e-04\n",
            "Epoch 00030: early stopping\n",
            "Program is running for 30 neurons and 2 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "986/986 [==============================] - 4s 4ms/sample - loss: 0.0207\n",
            "Epoch 2/50\n",
            "986/986 [==============================] - 1s 566us/sample - loss: 0.0022\n",
            "Epoch 3/50\n",
            "986/986 [==============================] - 1s 529us/sample - loss: 0.0018\n",
            "Epoch 4/50\n",
            "986/986 [==============================] - 1s 609us/sample - loss: 0.0019\n",
            "Epoch 5/50\n",
            "986/986 [==============================] - 1s 586us/sample - loss: 0.0016\n",
            "Epoch 6/50\n",
            "986/986 [==============================] - 1s 765us/sample - loss: 0.0020\n",
            "Epoch 7/50\n",
            "986/986 [==============================] - 1s 576us/sample - loss: 0.0015\n",
            "Epoch 8/50\n",
            "986/986 [==============================] - ETA: 0s - loss: 0.001 - 1s 585us/sample - loss: 0.0013\n",
            "Epoch 9/50\n",
            "986/986 [==============================] - ETA: 0s - loss: 0.001 - 1s 596us/sample - loss: 0.0013\n",
            "Epoch 10/50\n",
            "986/986 [==============================] - 1s 739us/sample - loss: 0.0012\n",
            "Epoch 11/50\n",
            "986/986 [==============================] - 1s 557us/sample - loss: 0.0012\n",
            "Epoch 12/50\n",
            "986/986 [==============================] - 1s 591us/sample - loss: 0.0012\n",
            "Epoch 13/50\n",
            "986/986 [==============================] - 1s 711us/sample - loss: 0.0010\n",
            "Epoch 14/50\n",
            "986/986 [==============================] - 1s 588us/sample - loss: 0.0010\n",
            "Epoch 15/50\n",
            "986/986 [==============================] - 1s 604us/sample - loss: 0.0011\n",
            "Epoch 16/50\n",
            "986/986 [==============================] - 1s 556us/sample - loss: 9.7848e-04\n",
            "Epoch 17/50\n",
            "986/986 [==============================] - 1s 719us/sample - loss: 0.0010A: 0s - loss: \n",
            "Epoch 18/50\n",
            "986/986 [==============================] - 1s 695us/sample - loss: 9.1448e-04\n",
            "Epoch 19/50\n",
            "986/986 [==============================] - 1s 626us/sample - loss: 9.0576e-04\n",
            "Epoch 20/50\n",
            "986/986 [==============================] - 1s 631us/sample - loss: 0.0011\n",
            "Epoch 21/50\n",
            "986/986 [==============================] - 1s 640us/sample - loss: 9.8301e-04\n",
            "Epoch 22/50\n",
            "986/986 [==============================] - 1s 619us/sample - loss: 8.9326e-04\n",
            "Epoch 23/50\n",
            "986/986 [==============================] - 1s 642us/sample - loss: 8.6552e-04\n",
            "Epoch 24/50\n",
            "986/986 [==============================] - 1s 583us/sample - loss: 8.7448e-04\n",
            "Epoch 25/50\n",
            "986/986 [==============================] - 1s 633us/sample - loss: 7.9031e-04\n",
            "Epoch 26/50\n",
            "986/986 [==============================] - 1s 688us/sample - loss: 9.7949e-04\n",
            "Epoch 27/50\n",
            "986/986 [==============================] - 1s 532us/sample - loss: 8.8229e-04\n",
            "Epoch 28/50\n",
            "986/986 [==============================] - 1s 592us/sample - loss: 9.2878e-04\n",
            "Epoch 29/50\n",
            "986/986 [==============================] - 1s 643us/sample - loss: 8.6101e-04\n",
            "Epoch 30/50\n",
            "986/986 [==============================] - 1s 659us/sample - loss: 8.6862e-04\n",
            "Epoch 00030: early stopping\n",
            "Program is running for 30 neurons and 3 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "986/986 [==============================] - 5s 5ms/sample - loss: 0.0211\n",
            "Epoch 2/50\n",
            "986/986 [==============================] - 1s 572us/sample - loss: 0.0019\n",
            "Epoch 3/50\n",
            "986/986 [==============================] - 1s 605us/sample - loss: 0.0020\n",
            "Epoch 4/50\n",
            "986/986 [==============================] - 1s 577us/sample - loss: 0.0019\n",
            "Epoch 5/50\n",
            "986/986 [==============================] - 1s 582us/sample - loss: 0.0017\n",
            "Epoch 6/50\n",
            "986/986 [==============================] - 1s 818us/sample - loss: 0.0017\n",
            "Epoch 7/50\n",
            "986/986 [==============================] - 1s 528us/sample - loss: 0.0015\n",
            "Epoch 8/50\n",
            "986/986 [==============================] - 1s 605us/sample - loss: 0.0013\n",
            "Epoch 9/50\n",
            "986/986 [==============================] - 1s 653us/sample - loss: 0.0015\n",
            "Epoch 10/50\n",
            "986/986 [==============================] - 1s 591us/sample - loss: 0.0015\n",
            "Epoch 11/50\n",
            "986/986 [==============================] - 1s 601us/sample - loss: 0.0012\n",
            "Epoch 12/50\n",
            "986/986 [==============================] - 1s 621us/sample - loss: 0.0013\n",
            "Epoch 13/50\n",
            "986/986 [==============================] - 1s 659us/sample - loss: 0.0014\n",
            "Epoch 14/50\n",
            "986/986 [==============================] - 1s 616us/sample - loss: 0.0012\n",
            "Epoch 15/50\n",
            "986/986 [==============================] - 1s 577us/sample - loss: 0.0013\n",
            "Epoch 16/50\n",
            "986/986 [==============================] - 1s 624us/sample - loss: 0.0011\n",
            "Epoch 17/50\n",
            "986/986 [==============================] - 1s 706us/sample - loss: 9.9526e-04\n",
            "Epoch 18/50\n",
            "986/986 [==============================] - 1s 587us/sample - loss: 9.3248e-04\n",
            "Epoch 19/50\n",
            "986/986 [==============================] - 1s 702us/sample - loss: 9.5994e-04\n",
            "Epoch 20/50\n",
            "986/986 [==============================] - 1s 756us/sample - loss: 9.3042e-04s - loss: 9\n",
            "Epoch 21/50\n",
            "986/986 [==============================] - 1s 610us/sample - loss: 8.5649e-04\n",
            "Epoch 22/50\n",
            "986/986 [==============================] - 1s 589us/sample - loss: 9.5570e-04\n",
            "Epoch 23/50\n",
            "986/986 [==============================] - 1s 527us/sample - loss: 8.0777e-04\n",
            "Epoch 24/50\n",
            "986/986 [==============================] - 1s 738us/sample - loss: 8.4429e-04\n",
            "Epoch 25/50\n",
            "986/986 [==============================] - 1s 624us/sample - loss: 9.5240e-04\n",
            "Epoch 26/50\n",
            "986/986 [==============================] - 1s 576us/sample - loss: 8.5083e-04\n",
            "Epoch 27/50\n",
            "986/986 [==============================] - 1s 767us/sample - loss: 7.9002e-04\n",
            "Epoch 28/50\n",
            "986/986 [==============================] - 1s 577us/sample - loss: 8.5750e-04\n",
            "Epoch 29/50\n",
            "986/986 [==============================] - 1s 614us/sample - loss: 8.5838e-04\n",
            "Epoch 30/50\n",
            "986/986 [==============================] - 1s 610us/sample - loss: 9.2744e-04loss\n",
            "Epoch 31/50\n",
            "986/986 [==============================] - 1s 540us/sample - loss: 7.8613e-04\n",
            "Epoch 32/50\n",
            "986/986 [==============================] - 1s 720us/sample - loss: 8.4119e-04\n",
            "Epoch 33/50\n",
            "986/986 [==============================] - 1s 568us/sample - loss: 9.2323e-04\n",
            "Epoch 34/50\n",
            "986/986 [==============================] - 1s 649us/sample - loss: 9.7488e-04\n",
            "Epoch 35/50\n",
            "986/986 [==============================] - 1s 598us/sample - loss: 9.0478e-04\n",
            "Epoch 36/50\n",
            "986/986 [==============================] - 1s 695us/sample - loss: 8.3053e-04\n",
            "Epoch 00036: early stopping\n",
            "Program is running for 30 neurons and 4 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "986/986 [==============================] - 5s 5ms/sample - loss: 0.0189\n",
            "Epoch 2/50\n",
            "986/986 [==============================] - 1s 617us/sample - loss: 0.0018\n",
            "Epoch 3/50\n",
            "986/986 [==============================] - 1s 622us/sample - loss: 0.0018\n",
            "Epoch 4/50\n",
            "986/986 [==============================] - 1s 582us/sample - loss: 0.0017\n",
            "Epoch 5/50\n",
            "986/986 [==============================] - 1s 739us/sample - loss: 0.0017\n",
            "Epoch 6/50\n",
            "986/986 [==============================] - 1s 590us/sample - loss: 0.0016\n",
            "Epoch 7/50\n",
            "986/986 [==============================] - 1s 628us/sample - loss: 0.0015\n",
            "Epoch 8/50\n",
            "986/986 [==============================] - 1s 531us/sample - loss: 0.0014\n",
            "Epoch 9/50\n",
            "986/986 [==============================] - 1s 758us/sample - loss: 0.0014\n",
            "Epoch 10/50\n",
            "986/986 [==============================] - 1s 682us/sample - loss: 0.0012\n",
            "Epoch 11/50\n",
            "986/986 [==============================] - 1s 651us/sample - loss: 0.0014\n",
            "Epoch 12/50\n",
            "986/986 [==============================] - 1s 779us/sample - loss: 0.0011\n",
            "Epoch 13/50\n",
            "986/986 [==============================] - 1s 603us/sample - loss: 0.0011\n",
            "Epoch 14/50\n",
            "986/986 [==============================] - 1s 585us/sample - loss: 0.0010\n",
            "Epoch 15/50\n",
            "986/986 [==============================] - 1s 823us/sample - loss: 9.0082e-04\n",
            "Epoch 16/50\n",
            "986/986 [==============================] - 1s 728us/sample - loss: 9.9394e-04\n",
            "Epoch 17/50\n",
            "986/986 [==============================] - 1s 667us/sample - loss: 0.0011\n",
            "Epoch 18/50\n",
            "986/986 [==============================] - 1s 759us/sample - loss: 0.0010\n",
            "Epoch 19/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 9.6625e-04\n",
            "Epoch 20/50\n",
            "986/986 [==============================] - 1s 873us/sample - loss: 8.7911e-04\n",
            "Epoch 21/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0010\n",
            "Epoch 22/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 9.0267e-04\n",
            "Epoch 23/50\n",
            "986/986 [==============================] - 2s 2ms/sample - loss: 0.0011\n",
            "Epoch 24/50\n",
            "986/986 [==============================] - 2s 2ms/sample - loss: 0.0010\n",
            "Epoch 25/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.4531e-04 0s - lo\n",
            "Epoch 26/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 9.3847e-04 0s - los\n",
            "Epoch 27/50\n",
            "986/986 [==============================] - 1s 799us/sample - loss: 8.0283e-04\n",
            "Epoch 28/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.6849e-04\n",
            "Epoch 29/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.8306e-04\n",
            "Epoch 30/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 9.6031e-04\n",
            "Epoch 31/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.4949e-04\n",
            "Epoch 32/50\n",
            "986/986 [==============================] - 1s 894us/sample - loss: 8.6290e-04\n",
            "Epoch 00032: early stopping\n",
            "Program is running for 30 neurons and 5 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "986/986 [==============================] - 6s 6ms/sample - loss: 0.0261 0s - loss: 0.0\n",
            "Epoch 2/50\n",
            "986/986 [==============================] - 1s 827us/sample - loss: 0.0020\n",
            "Epoch 3/50\n",
            "986/986 [==============================] - 1s 879us/sample - loss: 0.0019\n",
            "Epoch 4/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0019 1\n",
            "Epoch 5/50\n",
            "986/986 [==============================] - 1s 919us/sample - loss: 0.0017\n",
            "Epoch 6/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0015\n",
            "Epoch 7/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0014\n",
            "Epoch 8/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0014\n",
            "Epoch 9/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0015\n",
            "Epoch 10/50\n",
            "986/986 [==============================] - 1s 777us/sample - loss: 0.0014s - loss: \n",
            "Epoch 11/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0014\n",
            "Epoch 12/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0013\n",
            "Epoch 13/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0011ETA: 0s -\n",
            "Epoch 14/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0012\n",
            "Epoch 15/50\n",
            "986/986 [==============================] - 1s 950us/sample - loss: 0.0010\n",
            "Epoch 16/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0011 0s - lo\n",
            "Epoch 17/50\n",
            "986/986 [==============================] - 1s 916us/sample - loss: 9.2549e-04\n",
            "Epoch 18/50\n",
            "986/986 [==============================] - 1s 775us/sample - loss: 0.0010\n",
            "Epoch 19/50\n",
            "986/986 [==============================] - 1s 984us/sample - loss: 0.0010\n",
            "Epoch 20/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 9.7700e-04- \n",
            "Epoch 21/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.9972e-04\n",
            "Epoch 22/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.9806e-04\n",
            "Epoch 23/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 9.6363e-04\n",
            "Epoch 24/50\n",
            "986/986 [==============================] - 1s 957us/sample - loss: 8.2658e-04\n",
            "Epoch 25/50\n",
            "986/986 [==============================] - 1s 996us/sample - loss: 8.9305e-04\n",
            "Epoch 26/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 9.6696e-04\n",
            "Epoch 27/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 9.4204e-04 0s - loss: 8.\n",
            "Epoch 28/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 9.4429e-04\n",
            "Epoch 29/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.5550e-04\n",
            "Epoch 00029: early stopping\n",
            "Program is running for 30 neurons and 6 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "986/986 [==============================] - 8s 8ms/sample - loss: 0.0160\n",
            "Epoch 2/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0017\n",
            "Epoch 3/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0016\n",
            "Epoch 4/50\n",
            "986/986 [==============================] - 1s 884us/sample - loss: 0.0016s - los\n",
            "Epoch 5/50\n",
            "986/986 [==============================] - 1s 906us/sample - loss: 0.0015\n",
            "Epoch 6/50\n",
            "986/986 [==============================] - 1s 904us/sample - loss: 0.0014\n",
            "Epoch 7/50\n",
            "986/986 [==============================] - 1s 908us/sample - loss: 0.0013\n",
            "Epoch 8/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0015\n",
            "Epoch 9/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0012 0s - loss: 0\n",
            "Epoch 10/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0013\n",
            "Epoch 11/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 9.6700e-04\n",
            "Epoch 12/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 9.8792e-04\n",
            "Epoch 13/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0010\n",
            "Epoch 14/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0011\n",
            "Epoch 15/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.5728e-04\n",
            "Epoch 16/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 9.5348e-04\n",
            "Epoch 17/50\n",
            "986/986 [==============================] - 1s 884us/sample - loss: 8.8707e-04\n",
            "Epoch 18/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.0775e-040s - l - ETA: 0s - loss: 8.4062\n",
            "Epoch 19/50\n",
            "986/986 [==============================] - 1s 959us/sample - loss: 8.8663e-04\n",
            "Epoch 20/50\n",
            "986/986 [==============================] - ETA: 0s - loss: 9.0333e-0 - 1s 1ms/sample - loss: 8.9988e-04\n",
            "Epoch 21/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.4898e-04\n",
            "Epoch 22/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 7.9259e-04\n",
            "Epoch 23/50\n",
            "986/986 [==============================] - 2s 2ms/sample - loss: 8.4327e-04\n",
            "Epoch 24/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.6280e-04\n",
            "Epoch 25/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.7001e-04\n",
            "Epoch 26/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 9.2769e-04\n",
            "Epoch 27/50\n",
            "986/986 [==============================] - 2s 2ms/sample - loss: 9.2154e-04\n",
            "Epoch 00027: early stopping\n",
            "Program is running for 30 neurons and 7 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "986/986 [==============================] - 8s 8ms/sample - loss: 0.0284\n",
            "Epoch 2/50\n",
            "986/986 [==============================] - 1s 884us/sample - loss: 0.0018\n",
            "Epoch 3/50\n",
            "986/986 [==============================] - 1s 832us/sample - loss: 0.0017\n",
            "Epoch 4/50\n",
            "986/986 [==============================] - 1s 919us/sample - loss: 0.0016\n",
            "Epoch 5/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0017\n",
            "Epoch 6/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0015\n",
            "Epoch 7/50\n",
            "986/986 [==============================] - 1s 906us/sample - loss: 0.0014\n",
            "Epoch 8/50\n",
            "986/986 [==============================] - 1s 892us/sample - loss: 0.0014\n",
            "Epoch 9/50\n",
            "986/986 [==============================] - 1s 739us/sample - loss: 0.0014s - loss: 0\n",
            "Epoch 10/50\n",
            "986/986 [==============================] - 1s 947us/sample - loss: 0.0013\n",
            "Epoch 11/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0012\n",
            "Epoch 12/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0011ETA: 0s - loss\n",
            "Epoch 13/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0013\n",
            "Epoch 14/50\n",
            "986/986 [==============================] - 1s 879us/sample - loss: 0.0011\n",
            "Epoch 15/50\n",
            "986/986 [==============================] - 1s 722us/sample - loss: 0.0010\n",
            "Epoch 16/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0010\n",
            "Epoch 17/50\n",
            "986/986 [==============================] - 1s 726us/sample - loss: 8.6839e-04\n",
            "Epoch 18/50\n",
            "986/986 [==============================] - 1s 988us/sample - loss: 9.5276e-04s - lo\n",
            "Epoch 19/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.5295e-04\n",
            "Epoch 20/50\n",
            "986/986 [==============================] - 1s 881us/sample - loss: 9.1702e-04\n",
            "Epoch 21/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.1771e-04\n",
            "Epoch 22/50\n",
            "986/986 [==============================] - 1s 710us/sample - loss: 8.4315e-04\n",
            "Epoch 23/50\n",
            "986/986 [==============================] - 1s 991us/sample - loss: 9.5565e-04\n",
            "Epoch 24/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.6961e-04\n",
            "Epoch 25/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0011\n",
            "Epoch 26/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.2119e-04 0s - loss: 8.3126e-\n",
            "Epoch 00026: early stopping\n",
            "Program is running for 30 neurons and 8 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "986/986 [==============================] - 7s 7ms/sample - loss: 0.0083\n",
            "Epoch 2/50\n",
            "986/986 [==============================] - 1s 797us/sample - loss: 0.0016\n",
            "Epoch 3/50\n",
            "986/986 [==============================] - 1s 759us/sample - loss: 0.0017\n",
            "Epoch 4/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0016\n",
            "Epoch 5/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0014 0s - loss: - ETA: 0s - loss: 0.001\n",
            "Epoch 6/50\n",
            "986/986 [==============================] - 2s 2ms/sample - loss: 0.0014\n",
            "Epoch 7/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0015\n",
            "Epoch 8/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0012\n",
            "Epoch 9/50\n",
            "986/986 [==============================] - 1s 927us/sample - loss: 0.0011\n",
            "Epoch 10/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 0.0013\n",
            "Epoch 11/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 9.6243e-04\n",
            "Epoch 12/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 9.7718e-04\n",
            "Epoch 13/50\n",
            "986/986 [==============================] - 1s 904us/sample - loss: 0.0010\n",
            "Epoch 14/50\n",
            "986/986 [==============================] - 1s 770us/sample - loss: 9.2811e-04\n",
            "Epoch 15/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.3691e-04\n",
            "Epoch 16/50\n",
            "986/986 [==============================] - 1s 990us/sample - loss: 9.2383e-04\n",
            "Epoch 17/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.7047e-04\n",
            "Epoch 18/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 9.0204e-04\n",
            "Epoch 19/50\n",
            "986/986 [==============================] - 1s 1ms/sample - loss: 8.5856e-04 1s -\n",
            "Epoch 20/50\n",
            "986/986 [==============================] - 1s 957us/sample - loss: 8.6129e-04\n",
            "Epoch 00020: early stopping\n",
            "Program is running for 30 neurons and 9 replicate ----->\n",
            "\n",
            "Epoch 1/50\n",
            "986/986 [==============================] - 7s 7ms/sample - loss: 0.0285\n",
            "Epoch 2/50\n",
            "986/986 [==============================] - 1s 746us/sample - loss: 0.0019\n",
            "Epoch 3/50\n",
            "986/986 [==============================] - 1s 723us/sample - loss: 0.0019\n",
            "Epoch 4/50\n",
            "986/986 [==============================] - 1s 809us/sample - loss: 0.0016\n",
            "Epoch 5/50\n",
            "986/986 [==============================] - 1s 779us/sample - loss: 0.0016s - loss\n",
            "Epoch 6/50\n",
            "986/986 [==============================] - 1s 879us/sample - loss: 0.0015\n",
            "Epoch 7/50\n",
            "986/986 [==============================] - 1s 724us/sample - loss: 0.0014\n",
            "Epoch 8/50\n",
            "986/986 [==============================] - 1s 705us/sample - loss: 0.0016\n",
            "Epoch 9/50\n",
            "986/986 [==============================] - 1s 899us/sample - loss: 0.0014\n",
            "Epoch 10/50\n",
            "986/986 [==============================] - 1s 789us/sample - loss: 0.0014\n",
            "Epoch 11/50\n",
            "986/986 [==============================] - 1s 981us/sample - loss: 0.0014\n",
            "Epoch 12/50\n",
            "986/986 [==============================] - 1s 778us/sample - loss: 0.0012\n",
            "Epoch 13/50\n",
            "986/986 [==============================] - 1s 976us/sample - loss: 0.0011\n",
            "Epoch 14/50\n",
            "986/986 [==============================] - 1s 770us/sample - loss: 0.0013s - loss: 0.0\n",
            "Epoch 15/50\n",
            "986/986 [==============================] - 1s 696us/sample - loss: 0.0010\n",
            "Epoch 16/50\n",
            "986/986 [==============================] - ETA: 0s - loss: 9.0695e-0 - 1s 744us/sample - loss: 9.0798e-04\n",
            "Epoch 17/50\n",
            "986/986 [==============================] - 1s 723us/sample - loss: 9.1828e-04s - loss: 9\n",
            "Epoch 18/50\n",
            "986/986 [==============================] - 1s 657us/sample - loss: 9.7087e-04\n",
            "Epoch 19/50\n",
            "986/986 [==============================] - 1s 789us/sample - loss: 0.0011\n",
            "Epoch 20/50\n",
            "986/986 [==============================] - 1s 636us/sample - loss: 8.6883e-04\n",
            "Epoch 21/50\n",
            "986/986 [==============================] - 1s 931us/sample - loss: 9.2004e-04\n",
            "Epoch 22/50\n",
            "986/986 [==============================] - 1s 792us/sample - loss: 9.1227e-04s - loss: 9.\n",
            "Epoch 23/50\n",
            "986/986 [==============================] - 1s 906us/sample - loss: 9.4543e-04\n",
            "Epoch 24/50\n",
            "986/986 [==============================] - 1s 893us/sample - loss: 8.4186e-04\n",
            "Epoch 25/50\n",
            "986/986 [==============================] - 1s 779us/sample - loss: 8.3695e-04\n",
            "Epoch 26/50\n",
            "986/986 [==============================] - 1s 920us/sample - loss: 9.4573e-04\n",
            "Epoch 27/50\n",
            "986/986 [==============================] - 1s 850us/sample - loss: 8.5590e-04\n",
            "Epoch 28/50\n",
            "986/986 [==============================] - 1s 812us/sample - loss: 0.0010\n",
            "Epoch 29/50\n",
            "986/986 [==============================] - 1s 786us/sample - loss: 8.6949e-04\n",
            "Epoch 30/50\n",
            "986/986 [==============================] - 1s 779us/sample - loss: 9.3426e-04\n",
            "Epoch 00030: early stopping\n",
            "Progress: Collecting outputs.......\n",
            "\n",
            "\n",
            "Best model (neurons, replicate, rmse):  30 7 14.160316814251377\n",
            "\n",
            "Average scores:\n",
            "    neurons       rmse      mape         R  elapsed_time\n",
            "0       30  14.552509  0.967438  0.960794     31.385315\n",
            "\n",
            "Standard_deviations:\n",
            "    neurons     rmse      mape         R  elapsed_time\n",
            "0       30  0.31564  0.022036  0.000885      6.361975\n",
            "\n",
            "Minimums:\n",
            "    neurons       rmse     mape         R  elapsed_time\n",
            "0       30  14.160317  0.94941  0.959166     23.182876\n",
            "\n",
            "Maximums:\n",
            "    neurons       rmse      mape         R  elapsed_time\n",
            "0       30  15.193289  1.009665  0.962488      43.66566\n",
            "\n",
            "Progress: All works are done successfully, congratulations!!\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1008x360 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1296x288 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "IndexError",
          "evalue": "index 1 is out of bounds for axis 0 with size 1",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-24-cc10b67e62d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m                           epochs = 50, num_replicates = 10)\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mcreate_visualization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msl_model_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-20-1fddac661ea7>\u001b[0m in \u001b[0;36mcreate_visualization\u001b[1;34m(model_output)\u001b[0m\n\u001b[0;32m    358\u001b[0m   \u001b[0mtrue_pred_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m   \u001b[0mtest_scores_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m   \u001b[0mprediction_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m   \u001b[0mbest_model_prediction_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m   \u001b[0mrmse_boxplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-20-1fddac661ea7>\u001b[0m in \u001b[0;36mprediction_plot\u001b[1;34m(model_output)\u001b[0m\n\u001b[0;32m     99\u001b[0m   \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m232\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m   \u001b[0mtrain_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_predictions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest_replicate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m   \u001b[0mtest_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_predictions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest_replicate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "view limit minimum -36920.700000000004 is less than 1 and is an invalid Matplotlib date value. This often happens if you pass a non-datetime value to an axis that has datetime units",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    339\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m             \u001b[1;31m# Finally look for special method names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(fig)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'png'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mpng_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'retina'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'png2x'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'svg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[0;32m   2098\u001b[0m                            else suppress())\n\u001b[0;32m   2099\u001b[0m                     \u001b[1;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2100\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2101\u001b[0m                     \u001b[0mbbox_artists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bbox_extra_artists\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2102\u001b[0m                     bbox_inches = self.figure.get_tightbbox(renderer,\n",
            "\u001b[1;32mc:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   1734\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1735\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[1;32m-> 1736\u001b[1;33m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[0;32m   1737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1738\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'figure'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;31m# Composite any adjacent images together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer, inframe)\u001b[0m\n\u001b[0;32m   2588\u001b[0m                 \u001b[0martists\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_title_position\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxison\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minframe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_update_title_position\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   2531\u001b[0m                 if (ax.xaxis.get_ticks_position() in ['top', 'unknown']\n\u001b[0;32m   2532\u001b[0m                         or ax.xaxis.get_label_position() == 'top'):\n\u001b[1;32m-> 2533\u001b[1;33m                     \u001b[0mbb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2534\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2535\u001b[0m                     \u001b[0mbb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_window_extent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   1184\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m         \u001b[0mticks_to_draw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_label_position\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m_update_ticks\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mticks\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdrawn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m         \"\"\"\n\u001b[1;32m-> 1103\u001b[1;33m         \u001b[0mmajor_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_majorticklocs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1104\u001b[0m         \u001b[0mmajor_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_ticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmajor_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m         \u001b[0mmajor_ticks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_major_ticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmajor_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mget_majorticklocs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1346\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_majorticklocs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m         \u001b[1;34m\"\"\"Get the array of major tick locations in data coordinates.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_minorticklocs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\matplotlib\\dates.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[1;34m'Return the locations of the ticks'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1338\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1339\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_locator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\matplotlib\\dates.py\u001b[0m in \u001b[0;36mrefresh\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[1;31m# docstring inherited\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1364\u001b[1;33m         \u001b[0mdmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewlim_to_dt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1365\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_locator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_locator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Dimas\\anaconda3\\lib\\site-packages\\matplotlib\\dates.py\u001b[0m in \u001b[0;36mviewlim_to_dt\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                              \u001b[1;34m'often happens if you pass a non-datetime '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m                              \u001b[1;34m'value to an axis that has datetime units'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m                              .format(vmin))\n\u001b[0m\u001b[0;32m   1099\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnum2date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum2date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: view limit minimum -36920.700000000004 is less than 1 and is an invalid Matplotlib date value. This often happens if you pass a non-datetime value to an axis that has datetime units"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1296x864 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# neurons = np.array([10, 30, 50, 100, 150,200])\n",
        "neurons = np.array([30])\n",
        "\n",
        "# best_hyper_parameters = [['Adam', 0.01, 16],# 10N model\n",
        "#                       ['Adagrad', 0.01, 8], # 30N model\n",
        "#                       ['Adagrad', 0.01, 4], # 50N model\n",
        "#                       ['Adagrad', 0.01, 16], # 1000N model\n",
        "#                       ['Adagrad', 0.01, 16], # 150N model\n",
        "#                       ['Adagrad', 0.001, 4] # 200N model\n",
        "#                    ]\n",
        "\n",
        "# best_hyper_parameters = [['Adam', 0.01, 16],# 10N model\n",
        "#                       ['Adam', 0.01, 16], # 30N model\n",
        "#                       ['Adam', 0.001, 8], # 50N model\n",
        "#                       ['Adam', 0.001, 16], # 1000N model\n",
        "#                       ['Adam', 0.001, 16], # 150N model\n",
        "#                       ['Adam', 0.001, 16] # 200N model\n",
        "#                    ]\n",
        "\n",
        "best_hyper_parameters = [['Adam', 0.01, 16], # 30N model\n",
        "                         ]\n",
        "\n",
        "\n",
        "sl_model_output = LSTM_model(neurons, best_hyper_parameters, data, time_step = 5, test_split = 0.2,\n",
        "                          epochs = 50, num_replicates = 10)\n",
        "\n",
        "create_visualization(sl_model_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatetimeIndex(['2016-12-23', '2016-12-27', '2016-12-28', '2016-12-29',\n",
              "               '2016-12-30', '2017-01-02', '2017-01-03', '2017-01-11',\n",
              "               '2017-01-13', '2017-01-16',\n",
              "               ...\n",
              "               '2022-12-19', '2022-12-20', '2022-12-21', '2022-12-22',\n",
              "               '2022-12-23', '2022-12-26', '2022-12-27', '2022-12-28',\n",
              "               '2022-12-29', '2022-12-30'],\n",
              "              dtype='datetime64[ns]', name='Tanggal', length=1241, freq=None)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7j-WjM1UmJuQ"
      },
      "source": [
        "### **Case II: Executing Multi-Layer Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pblzmIsbmN1v"
      },
      "outputs": [],
      "source": [
        "hidden_layers = [[10, 5], [20, 10], [50, 20], [100, 50], [150, 100], [100, 50, 20]]\n",
        "\n",
        "best_hyper_parameters_multilayers  = [['Adagrad', 0.1, 4],#10-5N model\n",
        "                                     ['Adagrad', 0.01, 16],#20-10N model\n",
        "                                    ['Adagrad', 0.01, 16], #50-20N model\n",
        "                                   ['Adagrad', 0.01, 16], #100-50N model\n",
        "                                   ['Adagrad', 0.01, 16], #150-100N model\n",
        "                                   ['Adagrad', 0.001, 8] #100-50-20N model\n",
        "                                      ]\n",
        "\n",
        "ml_model_output = run_multi_layer_LSTM_Model(hidden_layers, best_hyper_parameters_multilayers, data, time_step = 5, test_split = 0.2, epochs = 100,  num_replicates = 30) \n",
        "\n",
        "multi_layers_all_scores_boxplots(ml_model_output)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PjSnbNidmiXy"
      },
      "source": [
        "\n",
        "\n",
        "### **Case III: Performing Statistical Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nrkDXcn8mtI6",
        "outputId": "22f3574c-300c-4847-a339-72627103ee57"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1008x360 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performaing Normality Tests\n",
            "\n",
            "Data1: \n",
            "NormaltestResult(statistic=1.4905288066235225, pvalue=0.4746087952114656)\n",
            "Data 2:\n",
            "NormaltestResult(statistic=2.1222308489898967, pvalue=0.3460695804779618)\n",
            "\n",
            "Tests of Equality of Variances\n",
            "F_statistic: 0.5751259056258581 F_pvalue: 0.9288785434286714\n",
            "Levene_statistic: 1.9685925804783602 Levene_pvalue: 0.16592816550823955\n",
            "Bartlett_statistic: 2.153597251460697 Bartlett_pvalue: 0.14223629186622289\n",
            "\n",
            "Two-sample ttest\n",
            "Ttest_indResult(statistic=-22.238765455511963, pvalue=7.084799222677942e-29)\n"
          ]
        }
      ],
      "source": [
        "sl_index = 4 # index for the best single layers model(0: 10N,1:30N,2:50N,3:100N,4:150N,5:200N)\n",
        "\n",
        "ml_index = 4  # index for the best multilayer models\n",
        "\n",
        "sl_all_rmse = read_df_from_file(output_dir_path+'sl-lstm-all-rmse.csv')\n",
        "ml_all_rmse = read_df_from_file(output_dir_path+'ml-lstm-all-rmse.csv')\n",
        "\n",
        "data1 = sl_all_rmse.iloc[sl_index,1:]\n",
        "data2 = ml_all_rmse.iloc[ml_index,1:]\n",
        "\n",
        "perform_statistical_analysis(data1, data2)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Predicting_stock_market_index_using_LSTM_Code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
